{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Technical Foundations of Generative AI for Business: BUSBIS 1530","text":"<ul> <li> <p> Course Information</p> <p> Course Description</p> <p> Schedule</p> <p> Course Material</p> </li> <li> <p> Instructor's Office &amp; Classroom</p> <p> Instructor Info Page via Canvas</p> <p> Office Hours (Canvas link)</p> <p> Email</p> </li> <li> <p> Course Modules</p> <p> Large Language Models</p> <p> Prompt Design</p> <p> Context Engineering</p> <p> Agents &amp; Tools</p> </li> <li> <p> Grading &amp; Evaluation</p> <p> Grading Information</p> <p> AI Project Description</p> <p> Canvas Assignment Page</p> </li> <li> <p> Tools</p> <p> Tools &amp; Setup</p> </li> <li> <p> Resources and References</p> <p> GitHub Codespaces</p> <p> Gemini API Documentation</p> </li> </ul>"},{"location":"agents/","title":"Agents &amp; Tools","text":"<ul> <li> <p> Concept Map</p> <p> Getting started</p> </li> <li> <p> NotebookLM Lab</p> <p> Getting Started with NotebookLM</p> </li> <li> <p> Google Gemini</p> <p> Getting started with Gemini</p> </li> </ul>"},{"location":"contexts/","title":"Context Engineering","text":"<ul> <li> <p> Concept Map</p> <p> Getting started</p> </li> <li> <p> NotebookLM Lab</p> <p> Getting Started with NotebookLM</p> </li> <li> <p> Google Gemini</p> <p> Getting started with Gemini</p> </li> </ul>"},{"location":"llms/","title":"Large Language Models","text":"<ul> <li> <p> Foundational Models</p> <p> Introduction to Foundational Models</p> </li> <li> <p> Model Selection</p> <p> Introduction to Model Cards</p> </li> <li> <p> Embedding Models</p> <p> Introduction to Embedding Models</p> </li> </ul>"},{"location":"llms/embedding/","title":"Embedding Model","text":"<ul> <li> Introduction to Embedding</li> <li> How Embeddings Work </li> <li> Embeddings Benchmark </li> <li> Business Value of Embedding Models </li> <li> Business Trade-Offs </li> <li> Activity</li> <li> Next Steps </li> </ul>"},{"location":"llms/embedding/activity/","title":"Try It Yourself","text":""},{"location":"llms/embedding/activity/#competitive-teardown-of-notebooklm","title":"Competitive Teardown of NotebookLM","text":"<p>Activity Type: Independent, Asynchronous Analysis</p>"},{"location":"llms/embedding/activity/#objective","title":"Objective","text":"<p>This activity is designed to bridge the gap between the theoretical concepts of embedding models and their real-world application in a cutting-edge AI product. Upon completion, you will be able to:</p> <ul> <li>Deconstruct a product's feature set to identify the underlying AI capabilities.</li> <li>Map specific product features to the MTEB task categories that power them.</li> <li>Articulate the business impact of embedding model performance on user experience and product value.</li> <li>Formulate a strategic, technology-informed recommendation for a new product feature.</li> </ul>"},{"location":"llms/embedding/activity/#context-scenario","title":"Context &amp; Scenario","text":"<p>Imagine you are a new Product Manager at a company developing an AI-powered research assistant designed to compete with major players like Google's NotebookLM. Your first task is to conduct a \"competitive teardown\" of NotebookLM to understand its core technology, identify its strengths, and pinpoint potential opportunities for your own product.</p> <p>Your analysis will focus specifically on the features powered by embedding models. The goal is to prepare a concise brief for your leadership team that explains how NotebookLM works at a conceptual level and what it implies for your product strategy. You have already been provided with access to a sample notebook to facilitate your research.</p>"},{"location":"llms/embedding/activity/#core-task-instructions","title":"Core Task &amp; Instructions","text":"<p>Your primary task is to analyze NotebookLM's features by completing the Competitive Teardown Template below. Follow these steps:</p> <ol> <li>Familiarize Yourself: Spend some time using the provided sample NotebookLM notebook. Interact with its features, create your own notebook, upload a document or two of your own, and pay close attention to how the tool helps you summarize, question, and connect ideas within your source materials.</li> <li>Identify Key Features: Identify 3-4 distinct, embedding-powered features within NotebookLM. Examples include \"Source Suggestions,\" \"Document Q&amp;A,\" the automatically generated \"Notebook Guide,\" \"Table of Contents,\" etc.</li> <li>Complete the Template: For each feature you identify, fill out a row in the \"Competitive Teardown Template.\" Your analysis should be based on the concepts we covered in the \"Embedding Models\" module.</li> <li>Formulate a Strategic Recommendation: After completing your teardown, answer the final strategic question to propose a new, innovative feature.</li> </ol>"},{"location":"llms/embedding/activity/#competitive-teardown-template","title":"Competitive Teardown Template","text":"NotebookLM Feature Primary MTEB Task Supporting MTEB Tasks Impact of a More Performant Embedding Model Example 1: Source Suggestions Retrieval Reranking, STS (Semantic Textual Similarity) A more performant model would result in more relevant and precise source suggestions. It could better distinguish nuance, reduce the noise of vaguely related sources, and ultimately build greater user trust in the AI's recommendations. Example 2: Document Q&amp;A Retrieval Reranking, Summarization A state-of-the-art model would minimize \"Context Rot\" by retrieving only the most salient passages. This would enable more accurate, concise answers and, most importantly, allow for highly precise citations, which is a critical trust-builder for a research tool. (Your analysis of Feature 1) (Your analysis of Feature 2) (Your analysis of Feature 3)"},{"location":"llms/embedding/activity/#strategic-recommendation-prompt","title":"Strategic Recommendation Prompt","text":"<p>Based on your analysis of NotebookLM and your understanding of embedding model capabilities, answer the following question in 2-3 paragraphs:</p> <p>Impact of a More Performant Embedding Model</p> <p>Identify one new, embedding-powered feature that you would recommend for your company's competing product. Which primary MTEB task would this feature rely on, and why would it provide significant, differentiated value to your target users (e.g., students, researchers, analysts)?</p>"},{"location":"llms/embedding/activity/#deliverable","title":"Deliverable","text":"<p>Please submit a single document containing: 1.  Your completed \"Competitive Teardown Template\" with your analysis of 3-4 features. 2.  Your written response to the \"Strategic Recommendation Prompt.\"</p> <p>Submissions will be evaluated on the depth of your analysis, the clear and logical connection you draw between product features and the underlying MTEB tasks, and the strategic rationale for your new feature recommendation.</p>"},{"location":"llms/embedding/activity/#on-device-ai-product-strategy","title":"On-Device AI Product Strategy","text":"<p>Activity Type: Independent, Asynchronous Analysis</p>"},{"location":"llms/embedding/activity/#objective_1","title":"Objective","text":"<p>This activity is designed to extend your understanding of embedding models beyond the cloud to the emerging frontier of on-device and edge AI. Upon completion, you will be able to:</p> <ul> <li>Analyze a state-of-the-art, on-device embedding model and its strategic value.</li> <li>Connect technical features (e.g., quantization, model size) to specific business and user benefits (e.g., privacy, offline access).</li> <li>Formulate a product pitch for a new, on-device AI feature.</li> </ul>"},{"location":"llms/embedding/activity/#context-scenario_1","title":"Context &amp; Scenario","text":"<p>As we've discussed, most large-scale AI processing happens in the cloud. However, a new wave of highly efficient, powerful models are making it possible to run sophisticated AI directly on user hardware (e.g., smartphones, laptops). This shift from cloud-first to on-device AI creates significant new product opportunities and changes the strategic landscape.</p> Video: Introducing EmbeddingGemma: The Best-in-Class Open Model for On-Device Embeddings <p>Your task is to watch the official Google for Developers video on \"Embedding Gemma\" and then act as a Product Strategist. You will analyze the business implications of this technology and propose a new product feature that leverages its unique strengths.</p>"},{"location":"llms/embedding/activity/#core-task-instructions_1","title":"Core Task &amp; Instructions","text":"<ol> <li>Watch the Video: Watch the 4-minute video on Embedding Gemma, paying close attention to the features of the model and the benefits they provide.</li> <li>Analyze the Value Proposition: Complete the \"On-Device Value Proposition Canvas\" below. This canvas will help you deconstruct the video's content and map the technology's features to concrete business and user value.</li> <li>Develop a Product Pitch: Based on your analysis, develop a concise pitch for a new on-device AI feature for an existing product (e.g., a mobile app for e-commerce, a project management tool, a social media platform).</li> </ol>"},{"location":"llms/embedding/activity/#on-device-value-proposition-canvas","title":"On-Device Value Proposition Canvas","text":"Technical Feature Direct Capability Business / User Benefit Example: Small Footprint (Quantization) Runs with low RAM (e.g., 300 MB). Enables AI on a wider range of older or less powerful devices, expanding the total addressable market. Example: On-Device Processing Embeddings are created and stored locally. Data Privacy &amp; Security: Sensitive user data (e.g., private notes, browsing history) never leaves the device, which is a major trust-builder and competitive differentiator. (Your analysis of MRL / Customizable Dimensions) (Your analysis of Offline Functionality) (Your analysis of Multilingual Support)"},{"location":"llms/embedding/activity/#product-pitch-prompt","title":"Product Pitch Prompt","text":"<p>Based on your analysis, develop a pitch for a new, on-device AI feature. Your pitch should be no more than 3 paragraphs and must answer the following questions:</p> <ol> <li>What is the product and the new feature? (e.g., \"For the Slack mobile app, we propose a new 'Smart Summary' feature...\")</li> <li>How does the feature work conceptually? (e.g., \"...that runs on-device to embed a user's recent private messages and channel history.\")</li> <li>What is the core business/user benefit, made possible by on-device AI? (e.g., \"The key benefit is that users can get instant, personalized summaries of their unread messages without any of their private company data ever being sent to the cloud. This provides offline access and addresses the major security and privacy concerns that prevent many corporations from adopting cloud-based AI tools.\")</li> </ol>"},{"location":"llms/embedding/activity/#deliverable_1","title":"Deliverable","text":"<p>Please submit a single document containing:</p> <ol> <li>Your completed \"On-Device Value Proposition Canvas.\"</li> <li>Your written response to the \"Product Pitch Prompt.\"</li> </ol>"},{"location":"llms/embedding/benchmark/","title":"Embedding Benchmark","text":"<p>From a \"Map of Meaning\" to a Strategic Choice: Navigating the Model Landscape</p>"},{"location":"llms/embedding/benchmark/#the-problem-of-choice","title":"The Problem of Choice","text":"<p>In the previous section, we established that embedding models act as \"cartographers of meaning,\" creating the semantic maps that power intelligent search. We also explored the value of specialized, \"local guide\" models for specific business domains.</p> <p>This leads to a critical business question: With the breadth of embedding models\u2014some general, some specialized, all claiming to be \"state-of-the-art\"\u2014how do you choose the right one for your project?</p> <p>Choosing a model based on marketing claims or hype is not a viable strategy. A model that excels at summarizing news articles might be entirely unsuitable for a legal contract search system. To make an informed, evidence-based decision, you need an objective, standardized way to measure and compare model performance on tasks that are relevant to your business goals.</p>"},{"location":"llms/embedding/benchmark/#introducing-mteb","title":"Introducing MTEB","text":"<p>This is the problem the Massive Text Embedding Benchmark (MTEB) was created to solve. As detailed in its foundational paper, MTEB is the industry-standard \"testing ground\" for embedding models. It is a comprehensive suite of tests that evaluates models across a wide and diverse array of tasks, datasets, and languages.</p> <p>Think of MTEB as a combination of an academic exam and a multi-event athletic competition for AI models. It doesn't just ask one question; it rigorously tests a model's abilities in multiple distinct areas, providing a holistic performance report. This allows you, as a business strategist, to look beyond a model's marketing and see its proven strengths and weaknesses.</p> <p>Market Intelligence for the AI Era</p> <p>You might wonder why companies from Google and Microsoft to startups like Cohere and VoyageAI publicly compete on leaderboards like MTEB. This isn't just for academic bragging rights\u2014it's a critical component of market intelligence and business strategy.</p> <p>The Hugging Face Open LLM Leaderboard is the most famous example for generative models, but the MTEB leaderboard is the equivalent gold standard for embedding models.</p> <p>As a product manager or strategist, you should view these leaderboards as a dynamic source of market intelligence:</p> <ul> <li>Competitive Analysis: Which companies are investing heavily in specific capabilities? A new top-performing model in a specific language or domain can signal a company's strategic market entry.</li> <li>\"Buy vs. Build\" Decisions: The performance of open-source models on these leaderboards can help you decide whether to use a free, off-the-shelf model or pay for a proprietary, state-of-the-art API.</li> <li>Vendor Selection: When a commercial vendor claims their model is \"best-in-class,\" the leaderboard provides an objective, third-party validation (or refutation) of that claim.</li> </ul> <p>Monitoring these resources is the modern equivalent of reading industry trade reports; it's a way to track the technological frontier and make informed, data-driven decisions.</p>"},{"location":"llms/embedding/benchmark/#no-best-model","title":"No \"Best\" Model!","text":"<p>Perhaps the single most crucial strategic insight to come from the MTEB benchmark is this: no single embedding model excels at all tasks.</p> <p>The model that ranks #1 for semantic search (Retrieval) is often not the same model that ranks #1 for identifying duplicate questions (Semantic Textual Similarity) or for grouping customer feedback into topics (Clustering).</p> <p>This finding fundamentally changes how we should approach model selection. The question is not, \"What is the best embedding model?\" The question is, \"What is the best embedding model for my specific business problem?\" This elevates model selection from a purely technical choice to a core strategic decision that must be deeply aligned with the intended application and desired business outcomes.</p>"},{"location":"llms/embedding/benchmark/#translating-mteb-tasks-into-capabilities","title":"Translating MTEB Tasks into Capabilities","text":"<p>To use MTEB effectively, you must first translate its technical task categories into the business capabilities they enable. When you analyze the MTEB leaderboard, you are essentially scouting for a model that has proven expertise in the skills your application needs most.</p> <p>Here is a breakdown of the primary MTEB task categories and their direct business applications:</p> <ul> <li> <p>Retrieval &amp; Reranking</p> <ul> <li>What it Measures: A model's ability to find a small number of highly relevant documents from a very large collection based on a query.</li> <li>Business Capability: This is the core engine for Retrieval-Augmented Generation (RAG). A high score in Retrieval is a primary indicator that a model will perform well in applications like internal knowledge search portals, customer support chatbots, and systems that answer questions based on product documentation.</li> </ul> <p>What is Retrieval-Augmented Generation (RAG)?</p> <p>RAG is a technique that gives a generative LLM access to external, up-to-date, or proprietary information.</p> <p>How it works:</p> <p>Retrieve: When a user asks a question (e.g., \"What is the warranty on the new X-100 model?\"), the system first uses an embedding model to search your private knowledge base (product manuals, etc.) for the most relevant text passages.</p> <p>Augment: The system then takes these retrieved passages and \"augments\" the user's original prompt, effectively saying to the LLM: \"Using the following information [...retrieved text about the X-100 warranty...], answer the user's question: 'What is the warranty on the new X-100 model?'\"</p> <p>Generate: The LLM then generates an answer based only on the factual context provided, preventing it from guessing or using outdated knowledge. RAG is the single most common and effective pattern for building enterprise-grade, factually-grounded AI applications.</p> </li> <li> <p>Semantic Textual Similarity (STS) &amp; Pair Classification</p> <ul> <li>What it Measures: How well a model can determine if two pieces of text have the same meaning.</li> <li>Business Capability: This is the foundation for efficiency and user experience improvements. It powers cache-augmented generation (if two questions are semantically identical, serve a pre-computed answer to save cost and improve speed), builds effective chat memory (recognizing when a user is referring to an earlier part of the conversation), and drives data cleaning tasks like FAQ deduplication.</li> </ul> <p>RAG vs. Cache-Augmented Generation</p> <p>While both are retrieval patterns, they solve different business problems by leveraging different model capabilities.</p> <ul> <li>RAG is about finding new information. It uses a Retrieval-focused model to answer a question it hasn't seen before by finding relevant context. The goal is accuracy and comprehensiveness.</li> <li>Cache-Augmented Generation is about recognizing old questions. It uses an STS-focused model to check if a new user query is semantically identical to one that has already been answered. If it is, the system serves the stored (cached) answer instead of running the expensive generative process again.</li> </ul> <p>Strategic Implication: A robust system might use both. The STS model acts as a fast, efficient front door. If it doesn't find a match in the cache, the request is then passed to the more powerful RAG system. This optimizes for both cost and speed.</p> </li> <li> <p>Clustering</p> <ul> <li>What it Measures: A model's ability to group similar documents together without any pre-existing labels.</li> <li>Business Capability: This is a powerful tool for unsupervised discovery and insight generation. Use cases include automatically identifying the main themes in thousands of customer reviews, segmenting user feedback for product teams, or detecting emerging market trends from news articles and financial reports.</li> </ul> </li> <li> <p>Classification</p> <ul> <li>What it Measures: A model's ability to assign a pre-defined category to a piece of text.</li> <li>Business Capability: This is the primary enabler of automated workflows and routing. It's used for sentiment analysis (tagging feedback as positive, negative, or neutral), content moderation (flagging inappropriate content), and automated customer support ticket routing (sending a ticket to \"Billing,\" \"Technical Support,\" or \"Sales\" based on its content).</li> </ul> </li> <li> <p>Bitext Mining</p> <ul> <li>What it Measures: A model's ability to find sentence pairs that are translations of each other in large collections of text in two different languages.</li> <li>Business Capability: This is the key to building effective multilingual applications. A model that scores well on this task has a deep, cross-lingual understanding, making it suitable for building a support system that can retrieve answers from an English knowledge base to answer a query asked in Spanish or French.</li> </ul> </li> <li> <p>Summarization</p> <ul> <li>What it Measures: How well a model's embeddings capture the core semantic essence of a document.</li> <li>Business Capability: While the LLM does the final act of writing a summary, the quality of that summary depends on the quality of the context provided. A model with high-quality summarization embeddings is better at identifying the most salient points in a document, which is a prerequisite for building reliable summarization features.</li> </ul> </li> </ul> <p>By first identifying the primary capability your project needs (e.g., \"We are building a RAG system, so Retrieval is our top priority\"), you can use the MTEB leaderboard to filter for models with proven, best-in-class performance for that specific task, turning an overwhelming choice into a data-driven, strategic decision.</p> <p>A Note on Computational Thinking</p> <p>As you analyze product features in the wild, you will notice that they are rarely powered by a single, isolated task. Most sophisticated AI features are a composition of the fundamental tasks listed above. Consider an automated customer support system:</p> <ol> <li>When a ticket arrives, a Classification model first routes it to the right department (\"Billing\").</li> <li>An STS model then checks if it's a duplicate of a recently solved ticket.</li> <li>If it's a new issue, a Retrieval (RAG) system searches the internal knowledge base for relevant articles to help the support agent.</li> <li>Finally, a Clustering model might run in the background on all tickets from the past week to identify emerging complaint themes for the product team.</li> </ol> <p>Part of your role as a technology decision maker is to practice this decomposition. When you see a feature, ask yourself: \"What are the fundamental building blocks? What MTEB tasks would need to be benchmarked to ensure this feature is effective?\" This computational thinking approach allows you to translate business needs into technical requirements more effectively.</p>"},{"location":"llms/embedding/business-value/","title":"Embedding Performance &amp; Business Value","text":"<p>Beyond the Generative Surface</p> <p>A generative AI application can feel like magic. You ask a question, and a coherent, well-written answer appears. But to build a reliable, enterprise-grade product, we must look under the hood at the fundamental mechanism that drives this \"magic\": next token prediction. An LLM is, at its core, a sophisticated engine for predicting the most probable next word (or \"token\") in a sequence.</p> <p>The quality of this prediction, and therefore the quality of the entire generated output, is not determined by the LLM alone. In an AI-powered business solution, the quality is almost entirely dependent on the information we provide to the engine before it even starts. This is where the performance of your embedding model becomes the central driver of business value.</p>"},{"location":"llms/embedding/business-value/#the-first-principle-grounding-the-next-token","title":"The First Principle: Grounding the Next Token","text":"<p>To understand the critical link between embeddings and quality, let's revisit our \"Semantic GPS\" analogy.</p> <p>The job of the embedding model in a RAG system is to perform a high-precision search. It takes the user's query and finds the exact, factually correct passage in your knowledge base. In doing so, it provides a precise \"semantic location\" to the LLM. It is effectively telling the next token predictor: \"Start here. Your answer is grounded in the facts at this specific location.\"</p> <p>If the embedding model does its job well, the LLM begins its next-token prediction process from a position of factual accuracy. Every word it generates is a logical step from that grounded starting point.</p> <p>If the embedding model performs poorly, it places the predictor in the wrong location\u2014a part of the semantic space that is irrelevant or only vaguely related to the query. From that flawed starting point, every subsequent token prediction will drift further from the truth, resulting in the confident-sounding hallucinations that are the primary risk of enterprise AI.</p>"},{"location":"llms/embedding/business-value/#the-business-cost-of-imprecision","title":"The Business Cost of Imprecision","text":"<p>A poor embedding model doesn't always lead to a complete hallucination. Sometimes, it leads to a more subtle but equally damaging problem: it lands the LLM in a too-approximate location. This creates what is known as \"Context Rot\"\u2014a situation where the signal of the correct answer is drowned out by the noise of irrelevant or redundant information.</p> <p>This noisy, imprecise context has direct and measurable business consequences:</p> <ul> <li>Increased Financial Cost &amp; Latency: When an LLM is given a vague or overly broad context, it often generates longer, less focused responses as it tries to address all the potential meanings. These verbose answers use more computational resources and more tokens, which directly increases API costs and slows down the response time (increases latency).</li> <li>Security &amp; Reliability Risks: An imprecise context creates ambiguity. This ambiguity can be exploited by malicious actors for \"prompt injection\" or \"information jailbreaking,\" where a carefully crafted query can trick the model into ignoring its safety instructions. It also makes the model less reliable, as it may \"forget\" key instructions buried within the noisy context.</li> </ul>"},{"location":"llms/embedding/business-value/#the-agentic-frontier","title":"The Agentic Frontier","text":"<p>Powering Conversations and Tools</p> <p>The need for precision becomes even more acute as we move from simple Q&amp;A bots to more sophisticated, agentic AI systems.</p> <ul> <li>Conversational Memory: A long-running chat with an AI is the norm. The chatbot's ability to \"remember\" details from earlier in the conversation relies on a RAG system that uses the conversation history as its knowledge base. A high-performance embedding model can precisely retrieve a specific detail mentioned 20 turns ago. A poor one cannot, leading to a frustrating user experience where the bot constantly forgets what's been said.</li> <li>Reliable Tool Use: The next frontier of AI involves \"agents\" that can take action, such as using a tool like a live API to fetch a stock price or a customer's order status. The agent's ability to use these tools correctly depends entirely on the embedding model's precision in extracting the correct parameters (e.g., the stock ticker \"GOOGL,\" the order number \"98765\") from a user's natural language request. Imprecision here doesn't just lead to a bad answer; it leads to a failed action.</li> </ul>"},{"location":"llms/embedding/business-value/#the-user-experience-mandate","title":"The User Experience Mandate","text":"<p>Natural, Multimodal Interaction</p> <p>Humans do not communicate in sterile, text-only formats. Our conversations are a rich, multimodal tapestry of text, images, emojis, memes, and videos. If interacting with an AI requires us to strip away this natural richness and adopt a rigid, text-only communication style, the user experience will suffer.</p> <p>The future of AI lies in meeting users where they are. This requires a unified embedding space\u2014a \"lingua franca\" that can represent the meaning of many different data types in a single, shared semantic map.</p> <p>A prime example of this is Meta's <code>ImageBind</code> model, which learns to embed text, images, audio, and even data from an Inertial Measurement Unit (IMU)\u2014the motion and orientation sensors in your phone or future AR/VR glasses\u2014into one common space. This isn't just an academic exercise; it's a strategic investment in the future of human-computer interaction. It paves the way for a new breed of applications where you could, for example, use your AR glasses to look at a product on a shelf while asking a question, and the AI could combine the visual information with your spoken query to provide a perfectly contextualized answer.</p>"},{"location":"llms/embedding/business-value/#the-synergy-of-scale","title":"The Synergy of Scale","text":"<p>Large Contexts and Precision Retrieval</p> <p>It is a common misconception that the trend toward ever-larger LLM context windows will make high-performance embedding models obsolete. The opposite is true: large context windows make precision retrieval more critical, not less.</p> <p>A larger context window simply means the \"haystack\" in which you are searching for the \"needle\" has grown exponentially. Simply dumping dozens of full documents into the context window is a recipe for failure. As the \"Lost in the Middle\" research demonstrated, models often ignore information buried in the center of a large context.</p> <p>The synergistic relationship works like this:</p> <ol> <li>A high-performance embedding model acts as the precision tool. It searches the massive knowledge base (the entire haystack) and retrieves the one or two exact paragraphs that contain the answer (the needle).</li> <li>The large context window then provides the LLM with ample \"breathing room\" to work with that precise information effectively, without being constrained.</li> </ol> <p>This combination of precision retrieval and large context is what enables the most critical feature for enterprise trust: verifiability through citations. When your retrieval is precise enough to identify the exact source sentence, you can present that source to the user. This transforms a \"black box\" answer into a trustworthy, verifiable piece of business intelligence, which is the ultimate goal of any enterprise AI application.</p>"},{"location":"llms/embedding/coordinates/","title":"A GPS for Meaning","text":"<p>How Embeddings Work: A Coordinate System for Meaning</p>"},{"location":"llms/embedding/coordinates/#the-unstructured-data-challenge","title":"The Unstructured Data Challenge","text":"<p>In the previous section, we discussed the strategic need for a searchable knowledge base to make an LLM \"aware\" of your proprietary business information. But this raises a fundamental technical question: How can a computer, which operates on numbers and logic, possibly \"search\" through human language, which is filled with ambiguity, context, and nuance?</p> <p>For decades, the primary method was keyword search. If you searched for the word \"billing,\" the system would return documents containing that exact word. While useful, this approach is brittle. It would fail to find a document that talked about \"invoices,\" \"payments,\" or \"statements\" if it didn't use the specific keyword \"billing.\"</p> <p>To build a truly intelligent system, we need to move from searching for words to searching for meaning. This is the problem that embedding models are designed to solve.</p>"},{"location":"llms/embedding/coordinates/#the-core-concept","title":"The Core Concept","text":"<p>Translating Meaning into Mathematics</p> <p>The central innovation of text embeddings is the ability to translate the abstract, semantic qualities of language into a structured, mathematical format. An embedding model takes a piece of text and maps it to a specific location in a vast, multi-dimensional space. This location is defined by its vector\u2014its list of numerical coordinates.</p> <p>This process is not random; it is highly structured. The model is trained on enormous volumes of text (like a significant portion of the internet) to learn the relationships between words and concepts. It learns that \"king\" and \"queen\" are related in a similar way to how \"man\" and \"woman\" are related. It learns that \"invoice\" and \"bill\" are semantically much closer than \"invoice\" and \"delivery truck.\"</p> <p>The result is a high-dimensional \"map of meaning,\" where the position of each text vector is determined by its semantic content.</p>"},{"location":"llms/embedding/coordinates/#analogy-the-semantic-gps","title":"Analogy: The Semantic GPS","text":"<p>Perhaps the most effective way to conceptualize this vector space is as a Semantic GPS.</p> <p>Imagine a giant, invisible globe. Instead of representing physical locations, this globe represents semantic concepts. When you create an embedding for a piece of text, you are asking the model to place a pin on this globe at the precise coordinates of that text's meaning.</p> <ul> <li>A customer's email asking, \"Where is my package?\" will place a pin at a specific set of coordinates.</li> <li>Another customer's chat message, \"Can I get an update on my delivery status?\" will place another pin at a location very close to the first one. Even though the words are different, the underlying intent\u2014the meaning\u2014is nearly identical.</li> <li>In contrast, a query about \"your company's return policy\" will place a pin in a completely different region of the globe, far from the shipping-related queries.</li> </ul> <p>This \"semantic proximity\" is the key. It allows us to use simple mathematical calculations to perform powerful linguistic operations. By calculating the \"distance\" between the vector for a user's query and the vectors of all the documents in our knowledge base, we can find the documents whose meaning is closest to the user's question. This is the foundation of modern semantic search, and it is exponentially more powerful and nuanced than traditional keyword search.</p> <p></p>"},{"location":"llms/embedding/coordinates/#the-embedding-models-role","title":"The Embedding Model's Role","text":"<p>The Cartographer of Meaning</p> <p>The quality of this semantic map depends entirely on the skill of its cartographer\u2014the embedding model.</p> <p>The embedding model is a specialized type of neural network (often a Transformer, the same underlying architecture as LLMs) that has been trained specifically for this translation task. Its sole purpose is to become an expert at reading a piece of text and assigning it the most accurate possible coordinates in the high-dimensional vector space.</p> <p>The difference between a good embedding model and a great one lies in its nuance. A great model can understand subtle differences in context:</p> <ul> <li>It knows that \"apple\" in the context of \"Apple Inc.\" should be mapped to a different location than \"apple\" in the context of \"apple pie.\"</li> <li>It can distinguish between the sentiment of \"The service was unbelievably good\" and \"The service was unbelievably bad.\"</li> </ul> <p>Therefore, selecting a high-quality embedding model is a critical business decision. The quality of this model determines the precision of your search results, which, as we will explore in a later section, directly impacts the accuracy and reliability of the final answer generated by your LLM.</p> <p>The Case for the \"Local Guide\": Domain-Specific Embedding Models</p> <p>A domain-specific embedding model is a model that has been further trained on a massive corpus of text from a particular field, such as law, finance, or medicine. This specialized training makes it an expert \"local guide\" for that industry's unique language.</p> <p>Consider these examples:</p> <ul> <li>Legal Domain: To a general model, the phrase \"material adverse change\" might seem vague. But in a legal contract, it has a very precise, heavily litigated meaning. A law-specific model (like VoyageAI's Voyage-Law-2) is trained on millions of legal documents and court filings. It understands this nuance and can create a much more accurate semantic map for legal concepts, ensuring that a search for a specific contractual clause is far more precise.</li> <li>Finance Domain: In a financial context, the word \"alpha\" isn't just a letter of the Greek alphabet; it refers to a measure of investment performance. The term \"liquidity\" has meanings far beyond its general dictionary definition. A finance-specific model (like Voyage-Finance-2) is trained on financial reports, SEC filings, and market analysis, allowing it to grasp these specialized meanings accurately.</li> </ul> <p>The Strategic Decision:</p> <p>As a product manager or strategist, this presents a critical choice:</p> <ol> <li>General-Purpose Model: Is your use case broad enough (e.g., a general customer service bot) that a high-quality \"world atlas\" model is sufficient?</li> <li>Domain-Specific Model: Is the accuracy of your application so dependent on understanding industry-specific jargon that the investment in a specialized \"local guide\" model is justified?</li> </ol> <p>The decision hinges on the required precision of your application and the potential cost of misunderstanding nuanced language. In the next section, we will explore the tools and benchmarks, like MTEB, that help you evaluate both general and specialized models to make this strategic decision effectively.</p>"},{"location":"llms/embedding/intro/","title":"Embedding Models","text":"<p>Why Your AI Strategy Depends on More Than Just the LLM</p>"},{"location":"llms/embedding/intro/#the-problem","title":"The Problem","text":"<p>Imagine you've hired a world-renowned, brilliant consultant to optimize your business. They have a near-perfect memory, can synthesize information at superhuman speed, and can generate insightful reports in seconds. There's just one catch: you've locked them in an empty room with no access to any of your company's files, data, or history. All they know is the general knowledge they walked in with.</p> <p>How valuable is this consultant now? Their general brilliance is intact, but their ability to provide specific, actionable advice for your business is severely limited.</p> <p>This scenario perfectly illustrates the challenge of deploying Large Language Models (LLMs) in a business context. Off-the-shelf models like GPT-4 or Claude are incredibly powerful, but they operate with two fundamental constraints that you must address strategically:</p> <ol> <li>The Knowledge Cut-off: Every LLM has a knowledge \"horizon.\" It was trained on a vast dataset of public information, but that training stopped at a specific point in time. It knows nothing about events, trends, or data created after that date. More importantly, it knows nothing about your company's private, proprietary information: your latest product specifications, your internal support documentation, your confidential market research, or your customer history.</li> <li>The Limited Context Window: An LLM's \"short-term memory\" is its context window\u2014the amount of text it can consider at one time when generating a response. While these windows are getting larger, they are finite. The model cannot hold your entire corporate knowledge base in its memory. It only knows what you provide it within that specific conversational turn.</li> </ol> <p>To build a valuable, enterprise-grade AI application, you cannot simply plug into a public LLM. You must devise a strategy to bridge this information gap, transforming the \"brilliant consultant\" into a deeply informed, trusted advisor.</p>"},{"location":"llms/embedding/intro/#the-solution","title":"The Solution","text":"<p>A Searchable, Semantic Knowledge Base</p> <p>The most effective and scalable solution to this challenge is to give the LLM a reliable, external \"brain\" to consult\u2014a comprehensive, searchable library of your organization's knowledge. This is where text embeddings become the foundational technology of your AI strategy.</p> <p>An embedding is a numerical representation\u2014a list of numbers called a vector\u2014that acts as a semantic fingerprint for a piece of text. The process works like this:</p> <ol> <li>You take a piece of your proprietary information (e.g., a paragraph from a product manual, a customer support ticket, a legal document).</li> <li>You feed this text into a specialized AI model called an embedding model.</li> <li>The model outputs a vector (e.g., <code>[0.02, -0.54, 0.89, ...]</code>) that mathematically captures the meaning and context of the original text.</li> </ol> <p>By performing this process on every document, paragraph, or relevant piece of information your company owns, you transform your entire library of passive, unstructured data into an active, structured, and machine-readable knowledge base. This new database can be searched not by keywords, but by meaning\u2014a concept we will explore in the next section.</p>"},{"location":"llms/embedding/intro/#the-relevance-of-determinism","title":"The Relevance of Determinism","text":"<p>Before we move on, it is critical to understand the single most important property of embeddings for business applications: they are deterministic.</p> <p>This means that the exact same piece of text, when passed through the same embedding model, will always produce the exact same vector. There is no randomness or creativity involved in this step. A paragraph describing your company's return policy will be assigned its unique semantic fingerprint, and that fingerprint will never change.</p> <p>This stands in stark contrast to the stochastic (probabilistic or creative) nature of generative LLMs. If you ask a generative model the same question five times, you might get five slightly different, creatively worded answers. While this is useful for tasks like marketing copywriting, it is a liability when you need consistency and factual accuracy.</p> <p>Why Determinism is a Strategic Advantage:</p> <ul> <li>Reliability: Your search and retrieval systems will be predictable and consistent. You can trust that a query for \"questions about billing\" will always point to the same set of relevant documents.</li> <li>Factual Grounding: By first finding the correct information through a deterministic search, you can then provide that factual context to the LLM. This dramatically reduces the risk of hallucinations (fabricated answers), as the LLM is instructed to base its response on the specific, accurate information you have provided.</li> <li>Testability: You can rigorously test and validate your knowledge base. You can write automated tests to ensure that certain queries always retrieve the correct documents, allowing you to build a system with measurable quality and accuracy.</li> </ul> <p>In summary, while the generative LLM is the user-facing \"voice\" of your application, the deterministic embedding model is the silent, reliable \"librarian\" that ensures this voice is informed, accurate, and trustworthy. Mastering this foundational layer is the first step in building a successful and scalable AI strategy.</p>"},{"location":"llms/embedding/next-steps/","title":"Conclusion &amp; Next Steps","text":"<p>Over the last several sections, we have taken a deep dive into what is arguably the most critical foundational technology for enterprise AI: embedding models. We've moved from the high-level strategic challenge of grounding LLMs in your business reality to the specific technical trade-offs that impact product performance and cost.</p> <p>This final section summarizes the core strategic takeaways from the module and sets the stage for your next challenge: moving from architectural design to project governance.</p>"},{"location":"llms/embedding/next-steps/#lesson-summary-five-core-principles-for-your-strategy","title":"Lesson Summary: Five Core Principles for Your Strategy","text":"<p>As you finalize the \"Embedding Model Strategy\" section of your Project Charter, ensure your decisions are guided by these five core principles:</p> <ol> <li> <p>Embeddings are the Foundation of Trust. \ud83e\udde0     The primary role of an embedding model in a RAG system is to provide a deterministic, reliable, and factually-grounded starting point for the generative LLM. Your application's ability to avoid hallucinations and build user trust begins with the quality of this foundational layer.</p> </li> <li> <p>Model Selection is a Strategic Business Decision. \ud83d\uddfa\ufe0f     There is no single \"best\" model, only the model that is best for your specific business problem. Use objective benchmarks like MTEB as a market intelligence tool to select a model whose proven strengths (e.g., Retrieval, Clustering) align directly with your project's primary use case. Remember to consider the value of a domain-specific \"local guide\" for specialized applications.</p> </li> <li> <p>Retrieval Quality Drives Generative Quality. \ud83c\udfaf     Your final application is only as good as the information you feed it. The key challenge is avoiding \"Context Rot\" by ensuring your retrieval system provides a clean, precise signal to the LLM. The ultimate measure of a successful enterprise RAG system is not just a correct answer, but a verifiable one, enabled by precise citations.</p> </li> <li> <p>Technical Specs are Business Levers. \u2696\ufe0f     The technical architecture of a model is not just an engineering detail; it's a set of levers for managing the cost-latency-performance triangle. Advanced concepts like Matryoshka Representation Learning (MRL) and MatFormer are strategic tools that enable greater efficiency and reduce the total cost of ownership for your AI products.</p> </li> <li> <p>The Goal is Natural, Synergistic Interaction. \u2728     Strive to build systems that adapt to human communication, not the other way around. Embeddings are the key to enabling natural, multimodal interactions. View emerging technologies like large context windows not as replacements for precision retrieval, but as powerful synergistic partners that, when combined, enable more capable and trustworthy AI.</p> </li> </ol>"},{"location":"llms/embedding/next-steps/#next-steps-from-architecture-to-action","title":"Next Steps: From Architecture to Action","text":"<p>You have now learned how to define the \"what\" and \"why\" of your application's technical core. Your immediate next step is to complete the Builder Session task and finalize the \"Embedding Model Strategy\" section of your Project Charter. This is a critical milestone that solidifies the architectural foundation of your project.</p> <p>Once you have defined the technical components of your solution, the next logical question is: \"How will we manage the development, deployment, and governance of this solution?\"</p> <p>In our next module, we will transition from technical architecture to operational excellence. We will explore how to develop a robust Project Governance and Delivery Framework, and how you can leverage AI tools to streamline that very process, ensuring your project is delivered efficiently, responsibly, and successfully.</p>"},{"location":"llms/embedding/tradeoffs/","title":"Business Trade-Offs","text":"<p>The Architect's Dilemma: Balancing Cost, Speed, and Performance</p> <p>So far, we have established what embedding models do and why their quality is critical. The final piece of the puzzle is understanding how their technical characteristics translate into real-world business trade-offs. The specifications of an embedding model are not just abstract numbers for engineers; they are levers that a business strategist can use to optimize a product for its target market and financial goals.</p> <p>Every decision in deploying an AI model involves navigating the classic Cost-Latency-Performance Triangle.</p> <ul> <li>Performance: The accuracy, precision, and overall quality of the model's output.</li> <li>Latency: The speed of the model; how quickly it responds to a user's request. Low latency is critical for a good user experience.</li> <li>Cost: The financial resources required to develop, deploy, and operate the model, including server expenses and API fees.</li> </ul> <p>Understanding the following technical concepts will allow you to make deliberate, informed decisions about how to balance these three competing priorities.</p>"},{"location":"llms/embedding/tradeoffs/#vector-dimensionality-size","title":"Vector Dimensionality (Size)","text":"<ul> <li>What it is: The number of dimensions, or individual numbers, in the embedding vector that a model produces. Models can range from having small vectors (e.g., 384 dimensions) to very large ones (e.g., 3072 or more).</li> <li>The Business Trade-Off: This is the most straightforward trade-off between performance and cost/latency.<ul> <li>Higher Dimensions (Larger Vectors): A larger vector can capture more nuance and semantic detail, often leading to higher performance (more accurate search results). However, these vectors require more storage space and are computationally slower to search through, which increases both cost and latency.</li> <li>Lower Dimensions (Smaller Vectors): A smaller vector is cheaper to store and faster to search, reducing cost and latency. However, it may not capture the same level of detail, potentially leading to lower performance.</li> </ul> </li> </ul>"},{"location":"llms/embedding/tradeoffs/#matryoshka-representation-learning-mrl","title":"Matryoshka Representation Learning (MRL)","text":"<ul> <li>What it is: A groundbreaking technique detailed in the \"Matryoshka Representation Learning\" paper. The name comes from Matryoshka dolls, the Russian nesting dolls where smaller dolls are contained within larger ones. MRL trains a single, large embedding vector in such a way that its first N dimensions also function as a high-quality, smaller vector. For example, a single 768-dimension MRL vector also contains a usable 512, 256, and 64-dimension vector within it.</li> <li>The Business Trade-Off: MRL is a powerful tool for optimizing the cost/latency vs. performance balance, offering efficiency and flexibility. It enables a strategy called Adaptive Retrieval:<ol> <li>Broad, Fast Search: Use the small, computationally cheap portion of the vector (e.g., the first 64 dimensions) to perform a very fast initial search across your entire knowledge base to find the top 100 most likely candidate documents.</li> <li>Precise Reranking: Then, use the full, high-performance vector (all 768 dimensions) to rerank only those 100 candidates.</li> <li>Business Impact: This two-stage approach provides the \"best of both worlds.\" You get the speed and low cost of small vectors for the initial search and the high performance of large vectors for the final, critical reranking. The MRL paper demonstrates this can lead to up to 14x speed-ups in real-world scenarios with almost no loss in accuracy.</li> </ol> </li> </ul>"},{"location":"llms/embedding/tradeoffs/#matformer-architecture","title":"MatFormer Architecture","text":"<ul> <li>What it is: As introduced in the \"MatFormer\" paper, this is a new type of Transformer architecture designed from the ground up to support MRL and provide what the authors call \"elastic inference.\"</li> <li>The Business Trade-Off: The MatFormer architecture is a strategy for dramatically reducing the Total Cost of Ownership (TCO) for developing and deploying AI models.<ul> <li>The Old Way: Traditionally, if your business needed a large, high-performance model for its cloud application and a small, fast model for its mobile app, you would have to train, manage, and maintain two completely separate models. This is expensive and time-consuming.</li> <li>The MatFormer Way: With this architecture, you train a single, universal \"elastic\" model. From this one model, you can instantly extract hundreds of different-sized, accurate sub-models for free, without any extra training.</li> <li>Business Impact: This drastically reduces development costs and accelerates time-to-market. Your engineering team can train once and then deploy fit-for-purpose models across a wide variety of environments, from massive servers to resource-constrained edge devices, all from a single core asset.</li> </ul> </li> </ul> <p>Model Architectures: MatFormer vs Mixture of Experts</p> <p>MatFormer: The \"Elastic\" Model for Diverse Deployments</p> <p>The core idea behind MatFormer is to create a single AI model that can adapt to different hardware and performance requirements without retraining.</p> <p>What it is: MatFormer uses a clever nested structure for its Feed-Forward Network (FFN) blocks, much like a set of Russian nesting dolls. It trains a large model, but simultaneously trains smaller, complete sub-models within it.</p> <ul> <li>Business Analogy: Imagine designing a single car engine block that, with minor adjustments, can be used in a small scooter, a family sedan, or a performance race car. You have one core R&amp;D effort that results in products for multiple distinct markets.</li> <li>Primary Goal: To solve the business problem of needing different model sizes for different applications (e.g., a large, powerful model for your main cloud API and a small, fast model for a mobile app). MatFormer lets you train once and extract hundreds of different-sized, high-quality models for free.</li> <li>Relevance to Model Type: The MatFormer paper explicitly shows this architecture is effective for both embedding models (encoders) and generative models (decoders). The flexibility is useful in both cases.</li> </ul> <p>Mixture-of-Experts (MoE): The Efficiently Scaled Giant</p> <p>The core idea behind MoE is to build a model with an enormous number of parameters (increasing its knowledge and capacity) while keeping the computational cost of running it manageable.</p> <ul> <li>What it is: An MoE model contains multiple \"expert\" sub-networks. For any given piece of input, a special \"gating network\" intelligently routes the data to only a small subset of the most relevant experts (e.g., 2 out of 8).</li> <li>Business Analogy: Imagine a large consulting firm with hundreds of highly specialized experts (in law, finance, marketing, etc.). When a client asks a question about a specific legal contract, you don't need to consult every single expert in the firm. The project manager intelligently routes the question to just the two or three relevant legal experts, saving everyone else's time and the client's money.</li> <li>Primary Goal: To dramatically increase a model's parameter count without a proportional increase in inference cost. This allows for the creation of massive, state-of-the-art models that are economically viable to operate.</li> <li>Relevance to Model Type: MoE is predominantly used for very large generative models (e.g., Mixtral 8x7B). The main benefit\u2014massive capacity with relatively low inference FLOPs\u2014is most pronounced for generative tasks where a larger \"brain\" leads to better reasoning, nuance, and creativity. While theoretically applicable, it's less common for embedding models where consistent latency and deterministic output are often the highest priorities.</li> </ul> <p>Key Differences at a Glance</p> Feature MatFormer Mixture-of-Experts (MoE) Core Purpose \ud83e\uddbe Deployment Flexibility \ud83e\udde0 Scaling Model Capacity Mechanism Nested FFN sub-models Parallel \"expert\" FFNs with a router Inference Process You select one static sub-model to use. The full model is used, but only a dynamic subset of experts are activated per token. Primary Use Case Building a suite of models for diverse hardware. Building a single, massive, state-of-the-art generative model."},{"location":"llms/embedding/tradeoffs/#quantization","title":"Quantization","text":"<ul> <li>What it is: A compression technique that reduces the numerical precision of the numbers in the embedding vector. For example, it might convert a highly precise 32-bit number into a less precise but much smaller 8-bit number.</li> <li>The Business Trade-Off: Quantization is a tactic for cost reduction at the point of deployment.<ul> <li>Business Impact: Quantized models are significantly smaller in file size and require less computational power to run. This makes them much faster and cheaper to operate, which is ideal for deployment on edge devices (like smartphones) or for minimizing server costs in a high-volume cloud application. This performance gain usually comes with a slight, but often acceptable, trade-off in accuracy. It's a strategic choice when speed and cost are more critical than achieving the absolute maximum level of performance.</li> </ul> </li> </ul>"},{"location":"llms/foundational-models/","title":"Introduction to Large Language Models","text":"<ul> <li> Introduction to LLMs</li> <li> Model Architecture </li> <li> Embeddings </li> <li> Model Development Process </li> <li> Human Alignment </li> <li> Activity</li> <li> Next Steps </li> </ul>"},{"location":"llms/foundational-models/activity/","title":"Try It Yourself","text":""},{"location":"llms/foundational-models/activity/#learning-activity-analyzing-a-state-of-the-art-ai-model","title":"Learning Activity: Analyzing a State-of-the-Art AI Model \ud83d\ude80","text":"<p>Objective: To apply your foundational knowledge of AI model development and multimodality to analyze a real-world product announcement. In this activity, you will act as a product manager evaluating Google's Gemini 2.5 Flash image model to develop market intelligence.</p> Video: Behind the scenes of Google's state-of-the-art \"nano-banana\" image model"},{"location":"llms/foundational-models/activity/#step-1-review-the-source-materials","title":"Step 1: Review the Source Materials","text":"<p>Your task is to analyze the key capabilities of Google's Gemini 2.5 Flash model. Watching full video is optional; You can use the  video chapters and time stamps to access relevant sections.</p>"},{"location":"llms/foundational-models/activity/#step-2-analysis-framework","title":"Step 2: Analysis Framework","text":"<p>Answer the following questions using the provided texts. Think critically about the connections between the technology described and the business implications.</p> <p>1. Multimodality and Embeddings:</p> <ul> <li>The summary mentions \"positive transfer between modalities\" and \"native multimodal understanding.\" Based on what you learned about embeddings in Module 3, how does having a single, \"native\" multimodal model (as opposed to separate models for text and images) lead to better performance and \"positive transfer\"?</li> <li>The AI excels at understanding nuanced, conversational prompts to edit images. Describe how a shared embedding space for text and images makes this possible.</li> </ul> <p>2. Model Development and Fine-Tuning (SFT):</p> <ul> <li>The video discusses achieving \"character consistency\" and \"pixel-perfect editing.\" What kind of specialized data and Supervised Fine-Tuning (SFT) would be required to teach a model these specific, high-level skills?</li> <li>The summary notes that user feedback from platforms like Twitter was crucial for identifying failure modes. How does this real-world feedback loop relate to the SFT and Safety Tuning processes we discussed in Module 4?</li> </ul> <p>3. Evaluation:</p> <ul> <li>The team is developing metrics \"beyond human preference evals,\" using text rendering as a proxy for quality. Why might relying solely on human preference not be enough for evaluating a model's technical capabilities? What does the ability to render text accurately suggest about the model's underlying understanding of structure?</li> </ul> <p>4. Strategic and Market Analysis:</p> <ul> <li>The speakers contrast specialized models with Gemini's goal of being a \"creative partner for complex workflows.\" Based on the market intelligence discussion in Module 4, is this a convincing differentiator? Why or why not?</li> <li>A new capability called \"Interleaved Image Generation\" is mentioned, which breaks down complex tasks into steps. How does this feature create business value for a creative professional (e.g., a marketer or designer)?</li> </ul>"},{"location":"llms/foundational-models/activity/#helpful-tips-and-suggestions","title":"Helpful Tips and Suggestions","text":"<ul> <li>Focus on the \"Why\": Don't just list the features. Your goal is to explain why these features are significant, based on the foundational concepts from our modules.</li> <li>Think Like a PM: For each capability, ask yourself: What business problem does this solve? Who is the target user? What are the risks?</li> <li>Use Course Terminology: Actively use terms like embedding, multimodality, SFT, evaluation, and differentiator in your answers to demonstrate your understanding.</li> <li>Reference the Timestamps: If you want to quickly see a feature in action, use the timestamps in the video description to jump to the relevant demo.</li> </ul>"},{"location":"llms/foundational-models/architecture/","title":"Model Architecture, Encoders, &amp; Decoders","text":"<p>Objective: To understand the Transformer, the foundational blueprint for all modern LLMs. By the end of this disucssion on architecture, you will be able to describe the distinct roles of an encoder and a decoder and explain how a model's architecture determines its core capabilities and best-fit business applications. \ud83e\udde0</p>"},{"location":"llms/foundational-models/architecture/#a-revolutionary-blueprint-the-transformer","title":"A Revolutionary Blueprint: The Transformer","text":"<p>In our introduction to LLMs, we established that an LLM is a powerful prediction engine. But what is the specific design that allows it to process and generate language so effectively? The answer lies in a revolutionary blueprint developed at Google in 2017 called the Transformer architecture. </p> <p>Before the Transformer, models like Recurrent Neural Networks (RNNs) processed text sequentially\u2014word by word, from beginning to end.  This is like reading a long paragraph one word at a time and trying to remember the context from the start. It was slow and made it difficult for the model to capture long-range dependencies in the text. </p> <p>The Transformer's key innovation was its ability to process sequences of text in parallel. Thanks to a mechanism called \"self-attention,\" the model can look at all the words in a sentence at the same time and weigh the importance of each word in relation to all the others.  This is more like how humans read: we glance at a whole sentence or paragraph to grasp its context, instantly understanding the relationships between the words. This parallel processing makes Transformers significantly faster to train and far more effective at understanding complex, long-form text.</p> <p>The Two Key Components: An Analyst and a Writer</p> <p>The original Transformer architecture, designed for machine translation, consists of two distinct parts: an encoder and a decoder. </p> <p>A great analogy is to think of an expert research analyst (the encoder) and a skilled writer (the decoder).</p>"},{"location":"llms/foundational-models/architecture/#the-encoder-the-analyst","title":"The Encoder (The Analyst) \ud83e\uddd0","text":"<p>The encoder's primary job is to read and understand the input text.  It takes the entire input sequence (like a French sentence, \"je suis \u00e9tudiant\") and creates a rich, numerical representation of it.  This representation is not just a word-for-word translation; it's a dense summary of information that captures the context, meaning, and relationships between all the words in the original sentence. </p> <ul> <li>Its Function: To create a deep, contextual understanding of the input.</li> <li>The Output: A set of numerical vectors (a contextualized embedding) that serves as the \"expert notes\" for the decoder. </li> </ul>"},{"location":"llms/foundational-models/architecture/#the-decoder-the-writer","title":"The Decoder (The Writer) \u270d\ufe0f","text":"<p>The decoder's job is to generate new text.  It takes the dense \"notes\" from the encoder and produces an output sequence word by word (like the English sentence, \"I am a student\").  It works auto-regressively, meaning to generate the next word, it looks at the encoder's notes and all the words it has already written. </p> <ul> <li>Its Function: To generate a new, coherent sequence of text based on the encoder's understanding.</li> <li>The Output: The final text, code, or other content.</li> </ul>"},{"location":"llms/foundational-models/architecture/#the-attention-mechanism","title":"The Attention Mechanism","text":"<p>So, how does the Transformer look at a whole sentence at once and understand its context? The \"secret sauce\" is a process called the attention mechanism.</p> <p>Think about how you read this sentence:</p> <p>\"The tiger jumped out of a tree to get a drink because it was thirsty.\" </p> <p>Your brain instantly and unconsciously links the word \"it\" back to the \"tiger.\" You're paying more \"attention\" to the connection between those two words than, for example, the connection between \"it\" and \"tree.\"</p> <p>The attention mechanism is a mathematical way for the model to do the same thing. For every word in the input, it creates scores that measure the relevance of all the other words in the sequence.  Words with higher relevance scores will have a stronger influence on how the model understands and represents the original word. This is what allows the model to disambiguate meaning and capture complex relationships across long stretches of text. </p>"},{"location":"llms/foundational-models/architecture/#a-landmark-paper-attention-is-all-you-need","title":"A Landmark Paper: \"Attention Is All You Need\"","text":"<p>The Transformer architecture and its attention mechanism were introduced in a landmark 2017 paper from Google titled, fittingly, \"Attention Is All You Need.\" </p> <p>For a business student, the significance of this paper is twofold:</p> <ol> <li>It Was a Disruptive Innovation: The title itself was a bold declaration to the AI research community. It argued that by using the attention mechanism, you could build a model that was both faster to train and more effective than previous designs, without needing the older, sequential processing methods. </li> <li>It Is the Foundation of Modern Generative AI: This paper is the direct ancestor of nearly every Generative AI tool you see today, from Gemini to ChatGPT to Llama. Its concepts unlocked the ability to build models at a massive scale, leading directly to the current AI revolution.</li> </ol> <p>Understanding this paper's contribution is like understanding the invention of the assembly line before studying modern manufacturing\u2014it's the foundational concept that made everything else possible.</p>"},{"location":"llms/foundational-models/architecture/#architecture-determines-capability","title":"Architecture Determines Capability","text":"<p>Here is the crucial takeaway for a product manager: the specific combination of these components in a model's architecture determines its strengths, weaknesses, and ideal use cases. Not all models use both parts.</p> <ul> <li> <p>Encoder-only Models (e.g., BERT):</p> <ul> <li>These models are masters of analysis and understanding.  By focusing exclusively on the encoder's function, they are optimized for tasks that require a deep contextual grasp of an input text.</li> <li>Best for: Sentiment analysis, text classification, and search.</li> <li>Limitation: They cannot generate new, long-form creative text. </li> </ul> </li> <li> <p>Decoder-only Models (e.g., GPT series, PaLM, Llama):</p> <ul> <li>These models are masters of generation. They are essentially powerful decoders that take a user's prompt as the initial context and excel at predicting the next sequence of words. The majority of recent, popular LLMs have adopted this streamlined architecture. </li> <li>Best for: Chatbots, content creation, creative writing, and summarization.</li> </ul> </li> <li> <p>Encoder-Decoder Models (e.g., The original Transformer, T5):</p> <ul> <li>These models excel at transformation tasks, where an entire input sequence needs to be converted into a new output sequence. </li> <li>Best for: Machine translation (e.g., French to English) or converting a complex medical report into a plain-language summary.</li> </ul> </li> </ul> <p>Tip</p> <p>Understanding a model's architecture (Encoder-only, Decoder-only, or Encoder-Decoder) is a powerful shortcut to understanding its core purpose. When you read a model card or a new product announcement, identifying this blueprint will allow you to quickly assess whether the model's strengths align with your specific business problem.</p>"},{"location":"llms/foundational-models/architecture/#looking-ahead-guiding-the-models-attention","title":"Looking Ahead: Guiding the Model's Attention","text":"<p>Now that you understand the attention mechanism\u2014the model's process for weighing the importance of words\u2014you have the foundational concept to understand one of the most critical skills in a Generative AI-powered world: prompt engineering.</p> <p>At its heart, prompt engineering is the art and science of strategically crafting your input to guide the model's attention mechanism. Your goal is to make the most important parts of your request so clear and relevant that the model's attention scores are naturally drawn to them, leading to a more accurate and useful output.</p> <p>Consider the difference:</p> <ul> <li>A Vague Prompt: <code>Tell me about our product.</code><ul> <li>The model's attention is scattered. It doesn't know what aspect of the product to focus on\u2014technical specs, user reviews, marketing angles?</li> </ul> </li> <li>An Engineered Prompt: <code>Act as a marketing manager. Using the customer reviews below, write three bullet points highlighting the product's most praised features.</code><ul> <li>This prompt gives the model's attention mechanism clear signals. It focuses the model on a specific role (marketing manager), a source of truth (the reviews), and a structured output (three bullet points), ensuring a relevant and well-formatted response.</li> </ul> </li> </ul> <p>This is a form of context engineering, the broader discipline of designing the entire input \"context\" for the model. This includes not only the instructions in the prompt but also providing relevant data for the model to work with, a technique we will explore later in the course.</p> <p>Key Takeaway</p> <p>Your prompt is not just a question; it's a tool for directing the focus of a powerful analytical engine. Learning to engineer prompts effectively is the primary way you, as a user, can influence the model's internal processes to get the business results you need. We will dive deep into these practical skills in a future module.</p>"},{"location":"llms/foundational-models/embeddings/","title":"Embeddings","text":"<p>From Data to Numbers: The Universal Concept of Embeddings \ud83d\udd21\ud83d\uddbc\ufe0f\ud83d\udd0a</p> <p>Objective: To understand the critical process of embedding, which converts all types of data into a numerical format that an AI can understand. By the end of this section, you will be able to explain how this concept applies not just to text but to images and other data types, unlocking advanced applications like multimodal AI.</p>"},{"location":"llms/foundational-models/embeddings/#the-universal-language-of-the-model-numbers","title":"The Universal Language of the Model: Numbers","text":"<p>In the last section, we learned that the Transformer architecture allows a model to understand the relationships between words in a sentence. This leads to a fundamental question: How does a computer, which only understands numbers, begin to comprehend something as abstract as a word or an image?</p> <p>The answer is a process that acts as a universal translator, converting any type of data into the only language the model speaks: the language of numbers. This process is called embedding.</p>"},{"location":"llms/foundational-models/embeddings/#starting-with-text-how-words-become-vectors","title":"Starting with Text: How Words Become Vectors","text":"<p>Let's begin with the most familiar data type: text. To prepare language for a Transformer, the model takes a few key steps.</p> <ol> <li> <p>Tokenization: The system first breaks a sentence down into smaller pieces, like words or subwords, called tokens. For example, the sentence <code>\"The cat sat\"</code> becomes three tokens: <code>\"The\", \"cat\", \"sat\"</code>.</p> </li> <li> <p>Embedding: Next, each token is converted into a list of numbers called a vector. This vector is not random; it's a high-dimensional mathematical representation of the token's meaning. Think of it as a coordinate on a giant, multi-dimensional map of meaning. On this map, tokens with similar meanings (like <code>\"happy\"</code> and <code>\"joyful\"</code>) are located close to each other, while dissimilar tokens (<code>\"happy\"</code> and <code>\"car\"</code>) are far apart.</p> </li> <li> <p>Positional Encoding: Because the attention mechanism looks at all the tokens at once, the model needs a way to understand the original word order. Positional encoding adds information to each vector that signals its position in the original sequence.</p> </li> </ol> <p>The Token as a Business Metric \ud83e\ude99</p> <p>Beyond being a technical step, the token is the fundamental unit of measurement in the world of LLMs. As a business leader or product manager, you will encounter it constantly in three critical areas:</p> <ol> <li> <p>Context Window: A model's memory, or context window, is measured in tokens (e.g., 128,000 tokens). This number dictates how much information\u2014including instructions, user history, and reference documents\u2014the model can consider at one time. A larger context window is more powerful but can be more expensive to operate.</p> </li> <li> <p>Billing and Cost: When you use an LLM via an API, you are billed per token. Pricing is often broken down into:</p> <ul> <li>Input Tokens: The tokens you send to the model (your prompt).</li> <li>Output Tokens: The tokens the model generates for you (the response). Understanding this allows you to forecast costs and design more efficient applications.</li> </ul> </li> <li> <p>Performance: The number of tokens in a prompt directly impacts how long it takes for the model to generate a response (latency). Longer inputs require more computation.</p> </li> </ol> <p>A rule of thumb: For English text, one token is roughly \u00be of a word. So, 100 tokens is about 75 words.</p>"},{"location":"llms/foundational-models/embeddings/#the-leap-to-multimodality-the-universal-translator","title":"The Leap to Multimodality: The Universal Translator","text":"<p>Here is the concept that unlocks the power of modern AI: the process of creating an embedding is a universal one. The same fundamental idea of turning data into a meaningful numerical vector applies to everything, not just text.</p> <p>This is the foundation of multimodal AI\u2014the ability of a single model to understand and process information from multiple data types at once. The Gemini family of models, for example, can take interleaved sequences of text, images, audio, and video as input.</p> <p>Various inputs like text, audio, and images are all converted into a common numerical format before being fed into the Transformer.</p> <p>Think of the embedding process as a universal translator. Its job is to take any form of data\u2014a word, the pixels in a photo, a soundwave from an audio file\u2014and convert it into a vector within a shared \"meaning space.\" In this space, the vector for the word \"apple\" will be mathematically close to the vector generated from a picture of an apple.</p>"},{"location":"llms/foundational-models/embeddings/#why-this-matters-for-business-unlocking-new-applications","title":"Why This Matters for Business: Unlocking New Applications","text":"<p>This ability to represent different data types in a shared numerical language is what enables a new frontier of powerful applications.</p>"},{"location":"llms/foundational-models/embeddings/#a-killer-app-for-embeddings-semantic-search","title":"A Killer App for Embeddings: Semantic Search","text":"<p>To understand the immediate business value of embeddings, let's contrast a familiar tool with its new, AI-powered counterpart.</p> <p>You're familiar with traditional keyword search (like using Ctrl+F in a document). It's great at finding exact matches. If you search for the word \"slow,\" it will find every instance of \"slow.\" However, it will completely miss related concepts like \"laggy,\" \"unresponsive,\" or \"takes forever to load.\"</p> <p>Semantic search, powered by embeddings, is different. It is search by meaning, not by keyword.</p> <p>Here\u2019s how it works:</p> <ol> <li>Index the Data: First, you convert your entire library of documents\u2014internal wikis, customer support tickets, market research reports\u2014into numerical embeddings and store them in a specialized \"vector database.\" This creates the \"meaning map\" of your data.</li> <li>Query the Data: When a user types a query (e.g., \"complaints about poor performance\"), the system converts that query into an embedding.</li> <li>Find Similar Meanings: The system then finds the document embeddings that are mathematically closest to the query embedding on the map.</li> </ol> <p>This unlocks powerful new capabilities for core business tasks:</p> <ul> <li> <p>Insight Generation: A product manager can search a database of thousands of customer reviews for the concept of \"difficult to use.\" Semantic search will instantly surface feedback that includes phrases like \"the user interface is confusing,\" \"I couldn't find the settings menu,\" and \"the onboarding process was a nightmare\"\u2014deep insights that keyword search would have missed.</p> </li> <li> <p>Information Extraction: An R&amp;D team can search a library of scientific papers with a high-level question about a new chemical process. The system will find relevant papers that describe the process using different terminology, dramatically accelerating research.</p> </li> <li> <p>Creative Ideation for Marketing: A marketing team can take a new slogan and semantically search a database of past campaigns to find examples with a similar emotional tone or brand voice, even if the wording is completely different.</p> </li> </ul>"},{"location":"llms/foundational-models/embeddings/#connecting-to-advanced-applications","title":"Connecting to Advanced Applications","text":"<p>This same core concept of matching meaning via embeddings is the engine for even more advanced applications:</p> <ul> <li>Connecting to Robotics: This is how a generative AI model can power a robot. The AI processes a verbal command (text embedding), sees an object on the table (image embedding), and reasons across both data types to generate a plan of action. The model can connect the spoken word \"apple\" to the object in front of it because their respective vector embeddings are similar.</li> </ul> Video: Gemini Robotics - Bringing AI to the physical world <ul> <li>Connecting to RAG: This concept is also the foundation for Retrieval Augmented Generation (RAG), a technique we'll cover later. A user's question is turned into an embedding and used to search a database for documents with the most similar embeddings (i.e., the most similar meaning), which are then fed to the model to generate a well-informed answer.</li> </ul> <p>Key Takeaway</p> <p>Understanding embeddings as a universal translator is crucial for identifying innovative business opportunities. The strategic question is no longer \"What can I do with my text data?\" but rather \"How can I combine all my data streams\u2014text from customer reviews, images of products, audio from support calls\u2014to create a single, smarter, and more context-aware AI application?\"</p>"},{"location":"llms/foundational-models/intro/","title":"What is an LLM and How Does It \"Think\"?","text":"<p>Objective: To establish a practical, non-technical mental model of a Large Language Model (LLM). By the end of this introduction, you will be able to define what a model is, explain the leap from simple statistical models to LLMs, and understand the core business value proposition of Generative AI.</p>"},{"location":"llms/foundational-models/intro/#lets-start-with-a-familiar-model","title":"Let's Start with a Familiar Model","text":"<p>Before we dive into the complexities of AI, let's start with a concept that should be familiar: the linear equation <code>y = mx + b</code>.</p> <p>This simple equation is the foundation of a linear model, a workhorse of statistical analysis and machine learning. A model is, at its core, a simplified representation of a real-world phenomenon. In this case, the model uses just two parameters\u2014<code>m</code> (the slope) and <code>b</code> (the intercept)\u2014to describe the relationship between an input <code>x</code> and an output <code>y</code>. Despite its simplicity, this model has been used for decades to represent complex phenomena, facilitate analysis, and make predictions. The inputs and outputs are numerical, and the model's \"knowledge\" is contained entirely within those two parameters.</p>"},{"location":"llms/foundational-models/intro/#from-lines-to-language-the-leap-to-llms","title":"From Lines to Language: The Leap to LLMs","text":"<p>Now, imagine a model that doesn't work with simple numerical inputs but with the rich, complex, and nuanced data of human language. This is the domain of a Large Language Model (LLM).</p> <p>While a linear model uses two parameters to model a line, an LLM uses billions of parameters to model the intricate patterns, grammar, context, and knowledge embedded in natural language.</p> <ul> <li>If <code>m</code> and <code>b</code> are two tuning knobs for a linear model, an LLM has billions of microscopic tuning knobs that have been automatically adjusted during its training process.</li> </ul> <p>This is the fundamental difference in scale and function:</p> <ul> <li>A linear model takes numerical features as input and predicts a numerical target.</li> <li>A Large Language Model takes a context as input\u2014which can include text, images, audio, or video\u2014and its predicted output is newly generated content, such as a human-like text response.</li> </ul> <p>This ability to create new content is why this technology is called Generative AI. The LLM is the underlying engine, and Generative AI is the broad category of applications it powers.</p> <p>When you interact with a Generative AI application, the response may appear to be thoughtful, intelligent, or even emotional. However, it's crucial to remember what's happening under the hood. The generated content is a statistical prediction, not a product of conscious thought. Its quality is a function of three things:</p> <ol> <li>The patterns it learned from its massive training data.</li> <li>How it was fine-tuned to respond to specific instructions.</li> <li>How it was aligned with human preferences using feedback and reward models.</li> </ol>"},{"location":"llms/foundational-models/intro/#the-power-and-peril-of-the-data-modeling-human-language","title":"The Power and Peril of the Data: Modeling Human Language","text":"<p>It's tempting to think of the vast amount of text data used to train an LLM as just a collection of words. However, it's essential to remember what human language truly represents. It is not just a means of communication; it is a repository of our collective understanding and misunderstanding of the world.</p> <p>This <code>training data</code> contains:</p> <ul> <li>Our Knowledge: Centuries of scientific discovery, history, literature, and practical know-how.</li> <li>Our Flaws: Our societal biases, stereotypes, historical injustices, misinformation, and blind spots.</li> </ul> <p>When an LLM is pre-trained on a dataset that reflects this human-generated content, it learns to replicate the patterns of everything\u2014our greatest insights and our deepest flaws.</p> <p>This simple fact is the source of both the immense power and productivity of these AI tools and the profound need for caution in how we build and use them. The model's ability to draft a brilliant marketing plan comes from the same place as its potential to generate a biased or harmful response. This is why the model development process cannot stop at just absorbing data; it must be followed by careful tuning and alignment.</p>"},{"location":"llms/foundational-models/intro/#the-business-value-from-a-portfolio-of-specialists-to-one-generalist","title":"The Business Value: From a Portfolio of Specialists to One Generalist","text":"<p>Before the rise of LLMs, companies relied on a wide range of specialized machine learning models to meet business needs. A firm might have:</p> <ul> <li>One model to analyze customer reviews for sentiment.</li> <li>Another model to recommend products based on purchase history.</li> <li>A third model to detect fraudulent transactions.</li> </ul> <p>Each of these required dedicated teams of data scientists and engineers to train, deploy, and maintain this portfolio of specialist models.</p> <p>The core value proposition of an LLM is that a single, general model can replace a broad swathe of these specialized systems. Instead of building a new model for each task, a business can now instruct one powerful model \"on the fly\" using natural language. The skill of crafting these instructions is called Prompt Engineering. This consolidation promises to dramatically reduce complexity and accelerate the deployment of AI-powered solutions.</p>"},{"location":"llms/foundational-models/intro/#how-is-an-llm-built-a-glimpse-into-the-process","title":"How is an LLM Built? A Glimpse into the Process","text":"<p>The incredible capabilities of models from leading AI labs like Google DeepMind, OpenAI, and others are the direct result of a multi-stage model development process. In this course, we will explore this process in detail, but at a high level, it consists of:</p> <ol> <li>Pre-training: The foundational stage where a model learns general knowledge and language patterns from a vast dataset.</li> <li>Supervised Fine-Tuning (SFT): The model is trained on a smaller, high-quality dataset to specialize it for specific tasks, like following instructions.</li> <li>Alignment and Safety Tuning: The model's behavior is further refined using human feedback to ensure its outputs are helpful, harmless, and aligned with ethical principles.</li> </ol> <p>A model's final capabilities\u2014whether it can write code, understand images, or generate marketing copy\u2014are a direct outcome of the choices made during this development process. Understanding this lifecycle is key to evaluating, selecting, and managing AI models in a business context.</p>"},{"location":"llms/foundational-models/model-development/","title":"Model Development Process","text":"<p>Pre-training, Fine-Tuning, and Safety \ud83d\udee1\ufe0f</p> <p>Objective: To understand the multi-stage process that transforms a general model into a specialized and safe AI tool. By the end of this section, you will be able to distinguish between pre-training, supervised fine-tuning, and safety tuning, and explain how these stages serve as the primary levers for controlling a model's capabilities and behavior. </p> <p>Market Intelligence</p> <p>The reason for understanding the model development process is not purely technical. This knowledge should enable you to analyze the competitive landscape of the AI market by understanding the strategic business implications of each stage of model development. </p>"},{"location":"llms/foundational-models/model-development/#training-vs-inference","title":"Training vs. Inference","text":"<p>Before we dive into the different stages of how a model is built, it's important to understand the two fundamental states of any machine learning model: training and inference.</p> <ol> <li> <p>Training (The \"Learning\" Phase):     This is the process of teaching the model by modifying its parameters. Using massive datasets and complex algorithms, the model's billions of internal \"knobs\" are adjusted until it gets good at its objective (e.g., predicting the next word). All the stages we will discuss in this module\u2014pre-training, supervised fine-tuning, and safety tuning\u2014are part of this learning phase.</p> </li> <li> <p>Inference (The \"Performance\" Phase):     This is when the model is actively used to generate a predicted output. During inference, the model's parameters are fixed; it is no longer learning. It is simply applying the knowledge it has already gained during training to respond to your prompt. When you use ChatGPT or Gemini, you are interacting with the model in its inference state.</p> </li> </ol> <p>This module will focus almost exclusively on the different stages of the training process, as this is where a model's capabilities, behaviors, and competitive advantages are forged.</p>"},{"location":"llms/foundational-models/model-development/#introduction","title":"Introduction","text":"<p>From General Knowledge to Specialized Skills</p> <p>We now know what an LLM is (a prediction engine), what its blueprint looks like (the Transformer), and how it understands data (through embeddings). But this doesn't explain how a model goes from being a general repository of internet text to a specialized assistant that can follow your specific instructions.</p> <p>A model's final capabilities are not the result of a single event, but of a deliberate, multi-stage development process. Understanding these stages is key to understanding how AI products are built, customized, and made safe.</p>"},{"location":"llms/foundational-models/model-development/#the-foundation-pre-training","title":"The Foundation: Pre-training","text":"<p>The first, longest, and most expensive stage is pre-training. This is where the model acquires its broad, general knowledge of the world.</p> <ul> <li>The Analogy: Think of pre-training as a person's entire K-12 education combined with a lifetime of reading. They consume a massive library of unlabeled books, articles, and websites to learn grammar, facts, reasoning patterns, and the subtle nuances of language.</li> </ul> <p>During this stage, the model is trained on a vast and diverse dataset with a single, simple goal: predict the next token. Through this process, it builds its foundational understanding of language.</p> <p>However, as we discussed in the introduction, the training data is a reflection of its human creators. This means that during pre-training, the model also learns and internalizes the undesirable patterns present in the data, such as societal biases and misinformation. This makes the next stages of tuning absolutely critical.</p>"},{"location":"llms/foundational-models/model-development/#pre-training-as-a-competitive-moat","title":"Pre-training as a Competitive Moat","text":"<p>The pre-training stage is the source of the steepest barrier to entry in the AI industry, creating a massive competitive advantage for incumbent labs like Google, OpenAI, and Meta. This is not a market where a startup can easily compete head-to-head. The key moats are:</p> <ul> <li>Data &amp; IP: LLMs require an enormous quantity and quality of training data. As popular internet sources (like Reddit and The New York Times) increasingly restrict data scraping and intellectual property lawsuits become more common, the supply of new, high-quality data is shrinking. This preserves a massive advantage for incumbents who have already amassed or own proprietary datasets.</li> <li>Infrastructure &amp; Capital: The infrastructure needs for pre-training are truly astonishing. Training is an intensely compute, network, and power-hungry process. A single high-end Nvidia GPU can cost upwards of $35,000, and a state-of-the-art training cluster may contain 100,000 GPUs. The cost of electricity and cooling to power these data centers is so immense that hyperscalers are now planning to co-locate future AI data centers with dedicated nuclear power facilities. This requires billions of dollars in capital investment.</li> <li>Talent: The number of elite AI researchers and engineers capable of successfully designing and executing a foundational model pre-training run is incredibly small, and competition for this talent is fierce.</li> </ul>"},{"location":"llms/foundational-models/model-development/#the-knowledge-cut-off","title":"The Knowledge Cut-off","text":"<p>A critical consequence of the pre-training process is that the model's knowledge is frozen in time. When you ask a question, the model is performing a \"closed-book test\" using only the data it was trained on. This \"knowledge cut-off date\" means the model is unaware of recent events. For business needs that require real-time awareness, this is a major limitation that must be overcome by augmenting the model at inference time with tools like web search or a RAG architecture.</p>"},{"location":"llms/foundational-models/model-development/#the-specialization-supervised-fine-tuning-sft","title":"The Specialization: Supervised Fine-Tuning (SFT)","text":"<p>After pre-training, the model is a vast repository of knowledge, but it's not yet a helpful assistant. The next stage, Supervised Fine-Tuning (SFT), specializes the model for specific tasks.</p> <ul> <li>The Analogy: SFT is like sending that well-read person to college for a specific degree or giving them on-the-job training for a particular role. They aren't learning from scratch; they are learning how to apply their general knowledge to perform a specific function.</li> </ul> <p>SFT involves training the model further on a much smaller, high-quality, labeled dataset. This dataset is typically curated by humans and consists of <code>input</code> and <code>demonstration</code> pairs (also known as \"prompt-response pairs\"). For example:</p> <ul> <li>Input: \"Write a short poem about the challenges of product management.\"</li> <li>Demonstration: A high-quality poem about product management that serves as a \"good\" example.</li> </ul> <p>By training on thousands of these examples, the model learns to follow instructions, which is one of the key behaviors improved through fine-tuning.</p> <p>\ud83e\udde0 Deep Dive: How Do Models Become \"Experts\"?</p> <p>Have you ever wondered how one AI model can be both a creative poet and a sharp-witted code assistant? One of the secrets is an architecture called Mixture of Experts (MoE).</p> <p>Think of a standard LLM as a single, brilliant generalist who has to use all of their brainpower for every task. An MoE model, however, works more like a company's executive team.</p> <ol> <li>The Experts: The model contains multiple smaller, specialized neural networks, each trained to be an \"expert\" in a specific domain (e.g., language translation, logical reasoning, code).</li> <li>The Router: A small, efficient network acts as a \"manager.\" When your prompt arrives, the router quickly analyzes it and routes the request to the most relevant one or two experts.</li> </ol> <p>This approach means that for any given task, only a fraction of the model's total parameters are used. It\u2019s a clever way to build incredibly capable models that are faster and more efficient to run. This trend of internal orchestration is a key part of how AI is scaling, and it is widely rumored to be a core component of next-generation models from major AI labs.</p>"},{"location":"llms/foundational-models/model-development/#sft-as-a-path-to-differentiation","title":"SFT as a Path to Differentiation","text":"<p>If pre-training creates the high barrier to entry, SFT is where companies create a differentiated product with a unique competitive advantage. This is how a model goes from being a generic knowledge base to a product that excels at specific, high-value tasks.</p> <ul> <li>Developing Specialized Capabilities: A model's ability to generate photo-realistic images or write flawless code is not an accident. It is the result of intensive SFT on massive, high-quality, and often proprietary labeled datasets in those specific domains. A vendor's excellence in a certain capability is a direct reflection of its deep expertise and unique data assets.</li> <li>Safety and Alignment as a Product Feature: A significant portion of the SFT effort is dedicated to Safety Tuning. A model that is more reliable, less prone to bias, and safer to interact with is a fundamentally better and more valuable product. Companies are investing heavily to make their models more trustworthy, turning safety itself into a key competitive battleground.</li> <li>Corporate Genealogy: A company's history and existing business ecosystem often dictate its strengths. For example, Google's long history in computational photography and cartography gives it a natural advantage in developing models with strong visual and spatial reasoning capabilities. However, its need to integrate new models with its existing portfolio of products (Search, Android, Workspace) can also act as a constraint.</li> </ul>"},{"location":"llms/foundational-models/model-development/#the-guardrails-safety-tuning","title":"The Guardrails: Safety Tuning","text":"<p>A specialized model is still not necessarily a safe one. Safety Tuning is a crucial, specialized form of SFT designed to mitigate the risks and undesirable behaviors learned during pre-training.</p> <ul> <li>The Analogy: Safety tuning is like a professional ethics and compliance course. It teaches the specialist not just how to do their job, but how to do it responsibly and safely\u2014avoiding harmful outputs, refusing inappropriate requests, and minimizing bias.</li> </ul> <p>This process involves a multi-pronged approach that includes careful data selection and human-in-the-loop validation to reduce the risks associated with bias, discrimination, and toxic outputs.</p>"},{"location":"llms/foundational-models/model-development/#why-this-matters-for-business-the-levers-of-control","title":"Why This Matters for Business: The Levers of Control","text":"<p>As a business leader or product manager, this multi-stage process represents your set of levers for shaping AI behavior and achieving competitive advantage through technology decisions.</p> <ul> <li>Pre-training (The Foundation): You will typically select a foundational model from a major vendor. The key decision is choosing a model whose scale and training data philosophy align with your needs. The biases learned during this stage are the raw material you must manage.</li> </ul> <p>So, Are Foundational Models a Commodity?</p> <p>While many models are based on the same underlying Transformer architecture, they are not commodities. The immense moats in pre-training (data, capital, talent) and the deep, strategic differentiation created during fine-tuning (specialized skills, safety, ecosystem integration) mean that foundational models are highly differentiated products, each with a unique profile of strengths, weaknesses, and strategic advantages.</p> <ul> <li>Fine-Tuning (The Customization): This is where you have the most direct control. Through SFT, you can train a model to understand your company's specific jargon, adopt your brand's voice, or execute a proprietary business process.</li> <li>Safety Tuning (The Risk Mitigation): This is your primary tool for aligning the AI's behavior with your company's values and ethical guidelines. It is a direct investment in reducing brand risk and ensuring your AI-powered product is trustworthy.</li> </ul> <p>Key Takeaway</p> <p>A model's final behavior is not an accident; it is the result of a deliberate, multi-stage process. Understanding the difference between pre-training, SFT, and safety tuning is essential for selecting the right base model, planning customization projects, and taking responsibility for the ethical and safe deployment of AI.</p>"},{"location":"llms/foundational-models/next-steps/","title":"Conclusion &amp; Next Steps","text":"<p>Across these foundational lessons, we have journeyed from the \"magic\" of a generative AI response to the core mechanisms that make it possible. You now have a strategic framework for understanding this transformative technology not as an inscrutable black box, but as a product of a deliberate and understandable engineering process.</p> <p>We have established a set of core mental models:</p> <ul> <li>An LLM is a prediction engine, not a thinking entity, whose power comes from recognizing patterns in massive datasets.</li> <li>The Transformer architecture, with its attention mechanism, is the blueprint that enables models to understand context at scale.</li> <li>Embeddings act as a universal translator, turning all forms of data\u2014text, images, and more\u2014into a shared language of numbers, unlocking multimodal AI and semantic search.</li> <li>A model's behavior is forged through a multi-stage development process, from the capital-intensive pre-training stage that builds a competitive moat, to the Supervised Fine-Tuning (SFT) that creates a differentiated product, and the RLHF alignment process that makes it trustworthy.</li> </ul> <p>With this foundation, you are no longer just a user of AI; you are an informed analyst, capable of looking at any AI-powered product and asking the right questions about its capabilities, its limitations, and its underlying design.</p> <p>Your Journey Continues: Next Steps for the AI-Powered Product Leader</p> <p>The best way to solidify this knowledge is to apply it. Your next assignment is to become an active, critical observer of the AI that is already all around you.</p> <p>1. Become a Cross-Model Power User:</p> <ul> <li>Your Task: Experiment with different publicly available models (such as Google's Gemini, OpenAI's ChatGPT, Anthropic's Claude, etc.). Give them the exact same, moderately complex prompt.</li> <li>What to Look For: Do not just look at the answer. Analyze the differences. Is one more creative? Is another more formal or cautious? Does one refuse the prompt while another answers it? These differences in personality, capability, and safety are not random; they are the direct result of the vendors' unique choices in training data, SFT, and RLHF alignment.</li> </ul> <p>2. Analyze the AI in Your Daily Life:</p> <ul> <li>Your Task: Look carefully at the AI-powered tools you already use every day. This could be the \"smart compose\" feature in your email, the search results on Google, or the recommendation engine on a streaming service.</li> <li>What to Look For: Identify the specific capability being demonstrated. Is it text generation? Classification? Semantic search? Try to reverse-engineer the user experience. What is the \"prompt\" you are implicitly giving the system? What is the \"generation\" it provides?</li> </ul> <p>3. Theorize About the Blueprint:</p> <ul> <li>Your Task: For a feature you particularly enjoy in an AI tool\u2014perhaps its helpful tone, its accuracy on a niche topic, or its strong safety guardrails\u2014try to theorize where that feature came from.</li> <li>Ask Yourself:<ul> <li>\"Does this feel like a broad capability that came from pre-training on a massive dataset?\"</li> <li>\"Is this a specialized skill, like understanding medical jargon or writing in my company's brand voice, that was likely developed through Supervised Fine-Tuning (SFT)?\"</li> <li>\"Is this a quality of helpfulness, harmlessness, or a particularly pleasing interaction style that was probably shaped by extensive post-training alignment (RLHF)?\"</li> </ul> </li> </ul> <p>By actively deconstructing the AI products you encounter, you will sharpen your intuition and build the market intelligence needed to lead, strategize, and build in the age of Generative AI.</p>"},{"location":"llms/foundational-models/reward-model/","title":"Model Alignment","text":"<p>Aligning with Human Values: The Role of RLHF and Reward Models \u2705</p> <p>Objective: To understand the advanced process of Reinforcement Learning from Human Feedback (RLHF). By the end of this section, you will be able to explain how RLHF and Reward Models are used to make LLMs not just capable, but also helpful, harmless, and aligned with human preferences.</p>"},{"location":"llms/foundational-models/reward-model/#introduction","title":"Introduction:","text":"<p>Beyond Just Following Instructions</p> <p>In the last section, we learned that Supervised Fine-Tuning (SFT) is used to teach a model how to follow instructions. This transforms it from a general knowledge base into a specialized tool.</p> <p>But what happens when an instruction can be followed in multiple ways? One answer might be technically correct but long-winded and confusing. Another might be equally correct but also concise, clear, and safe. How do we teach the model to produce the preferred response? This requires a more nuanced alignment technique that goes beyond simply showing the model good examples.</p>"},{"location":"llms/foundational-models/reward-model/#the-limitation-of-supervised-fine-tuning","title":"The Limitation of Supervised Fine-Tuning","text":"<p>SFT is powerful, but it primarily works by exposing an LLM only to positive examples of high-quality responses. While this teaches the model what a \"correct\" output looks like, it doesn't explicitly teach it what makes one good answer better than another, nor does it effectively penalize the model for generating undesirable outputs.</p> <ul> <li>The Analogy: SFT is like giving a new customer service agent a script of perfect answers. They learn to follow the script, but they don't learn the subtle skills of empathy, tone, and prioritizing information that separate a good agent from a great one.</li> </ul>"},{"location":"llms/foundational-models/reward-model/#the-solution-reinforcement-learning-from-human-feedback-rlhf","title":"The Solution: Reinforcement Learning from Human Feedback (RLHF)","text":"<p>To achieve a deeper level of alignment, AI labs use a technique called Reinforcement Learning from Human Feedback (RLHF). RLHF is a powerful fine-tuning technique that enables an LLM to better align with human-preferred responses, making its outputs more helpful, truthful, and safer.</p> <p>The key difference is that RLHF makes it possible to leverage negative or less-preferred outputs, penalizing an LLM when it generates responses that exhibit undesired properties.</p> <ul> <li>The Analogy: RLHF is like having an experienced manager listen to the new service agent's calls. The manager provides feedback (\"That answer was good, but you could have been more empathetic,\" or \"Avoid using that technical jargon\"). Through this feedback loop, the agent learns the nuanced preferences that lead to higher customer satisfaction.</li> </ul>"},{"location":"llms/foundational-models/reward-model/#how-rlhf-works-the-reward-model","title":"How RLHF Works: The Reward Model","text":"<p>The RLHF process typically involves three key steps:</p> <ol> <li> <p>Collect Human Preference Data: A prompt is fed to the already fine-tuned model, which then generates two or more different responses. A human rater then evaluates these responses and selects the one they prefer. This process is repeated thousands of times to create a large dataset of human preferences.</p> </li> <li> <p>Train a Reward Model (RM): A separate model, known as a Reward Model, is then trained on this human preference data. The Reward Model's only job is to take any given response and output a score that predicts how a human would likely rate it. It effectively becomes a scalable, automated proxy for human judgment.</p> </li> <li> <p>Fine-Tune the LLM via Reinforcement Learning: Finally, the original LLM is fine-tuned again. In this stage, the LLM generates a response, which is immediately scored by the Reward Model. A reinforcement learning algorithm then adjusts the LLM's parameters, encouraging it to produce outputs that receive the highest possible score from the Reward Model. The LLM is essentially training itself to please the \"human preference simulator.\"</p> </li> </ol>"},{"location":"llms/foundational-models/reward-model/#why-this-matters-for-business-from-functional-to-trustworthy","title":"Why This Matters for Business: From Functional to Trustworthy","text":"<p>As a business leader, understanding RLHF is critical because it is the state-of-the-art process for transforming a model from a merely functional tool into a helpful, harmless, and trustworthy AI assistant.</p> <p>This process is the primary mechanism that vendors use to improve model safety, reduce toxicity, and ensure the model is better aligned with human ethics and values.</p> <ul> <li>Market Intelligence: A vendor's investment in a large-scale, high-quality RLHF pipeline is a direct measure of their commitment to product safety and quality. When a company announces its new model is \"more helpful\" or \"safer,\" it is almost always the result of a massive RLHF effort. This is a crucial product differentiator that builds user trust and reduces brand risk.</li> </ul> <p>Key Takeaway</p> <p>Supervised Fine-Tuning (SFT) teaches a model to be correct. Reinforcement Learning from Human Feedback (RLHF) teaches a model to be preferred and safe. Understanding this distinction is crucial for evaluating the maturity, quality, and safety of any AI product you intend to use or build upon.</p> <p>The Future of AI Customization\u2014Cultural Alignment \ud83c\udf0f</p> <p>As we've learned, Reinforcement Learning from Human Feedback (RLHF) is the key to aligning a model with human preferences. But whose preferences? This question opens the door to what many believe is the next frontier of competitive advantage for AI vendors.</p> <p>A Key Differentiator</p> <p>It is highly likely that AI vendors will compete by offering customizable RLHF pipelines. This would allow a global company to fine-tune a model not just to a general standard of \"safety,\" but to the specific cultural norms, values, and communication styles of different regions. A model aligned for a Japanese audience might prioritize politeness and indirectness, while a model for an American audience might be trained to be more direct and informal.</p> <p>The Business Analogy: AI as Experience Management</p> <p>This capability would transform the AI model into another experience management lever, much like a company customizes its websites or marketing campaigns for different countries. By aligning the AI's personality and interaction style with local expectations, a global organization can build trust, increase user satisfaction, and improve the effectiveness of its AI-powered services in each market. This moves a model from being a generic tool to a culturally-aware brand ambassador.</p>"},{"location":"llms/foundational-models/reward-model/#the-bigger-picture-how-do-we-evaluate-llms","title":"The Bigger Picture: How Do We Evaluate LLMs?","text":"<p>The Reward Model in RLHF is a powerful tool designed for a specific purpose: to score a model's output to better align it with human preferences. This introduces us to the much broader and critically important field of LLM Evaluation. After all, how do we know if one model is better than another, or if our fine-tuning efforts have actually worked?</p> <p>There are three primary methods for evaluating the performance of LLMs:</p> <p>1. Human Evaluation</p> <p>This is considered the gold standard for assessing the quality of generative outputs. In this process, humans are given a set of criteria and asked to rate or rank the model's responses. This method provides a nuanced assessment of complex qualities like creativity, tone, and helpfulness. However, it can be slow, expensive, and difficult to scale.</p> <p>2. Traditional Evaluation Methods</p> <p>These methods use quantitative metrics to compare a model's output to an ideal or \"ground truth\" response. While useful for tasks with a single correct answer, these methods can unfairly penalize creative or unexpectedly good solutions that don't exactly match the reference text.</p> <p>3. LLM-Powered Autoraters</p> <p>To get the nuance of human judgment at a larger scale, teams often use another powerful LLM as an \"autorater.\" This approach uses an LLM to mimic human judgment, offering a scalable and efficient way to evaluate model outputs. The autorater is given the task, the criteria, and the candidate's response, and it generates a score and a rationale.</p> <p>A Reward Model is a specialized type of autorater, fine-tuned specifically to score outputs based on the patterns it learned from human preference data.</p>"},{"location":"llms/foundational-models/reward-model/#why-this-matters-for-business","title":"Why This Matters for Business","text":"<p>For a product manager, building a tailored evaluation framework is essential for moving an AI application from a prototype to production. Choosing the right evaluation method\u2014or a combination of methods\u2014is a strategic decision. A finance application might rely on traditional metrics to ensure factual accuracy, while a marketing copy generator would be better measured by human evaluation or a well-calibrated autorater to assess creativity and brand voice.</p>"},{"location":"llms/model-selection/","title":"Model Selection &amp; Business Alignment","text":"<ul> <li> Introduction to Model Cards</li> <li> Guided Tour of Model Card </li> <li> Model Choice &amp; Business Alignement </li> <li> Hands-on Activity: Model Choice in Action </li> <li> Next Steps </li> </ul>"},{"location":"llms/model-selection/activity/","title":"Model Choice: Hands-on Activity","text":""},{"location":"llms/model-selection/activity/#assign-roles","title":"Assign Roles","text":"<p>Your team will be comprised of three key stakeholders. Assign one of the following personas to each team member.</p>"},{"location":"llms/model-selection/activity/#individual-analysis-roleplay","title":"Individual Analysis &amp; Roleplay","text":"<p>Adopt your assigned persona. Access the provided public notebook. Use it to analyze the model cards and form an initial recommendation based on your specific priorities and concerns. Use the suggested queries to begin your investigation.</p> Want to create your own notebook from scratch? <p>You can access a copy of all model cards from this Google Drive directory.</p>"},{"location":"llms/model-selection/activity/#persona-descriptions","title":"Persona Descriptions","text":"General CounselProduct ManagerCEO + P&amp;L Owner <p>Persona 1: Priya Sharma</p> <p>Your Worldview: You are the guardian of the company. Your primary concern is mitigating risk. A single lawsuit or data breach could kill the company before it gets off the ground. You believe a \"moat of compliance and safety\" is a competitive advantage.</p> <p>What You're Looking For:</p> <ul> <li>Data Privacy &amp; IP: Where did the training data come from? What are the policies around user inputs? Is there a risk of our clients' confidential case data being used for training?</li> <li>Accuracy &amp; Hallucinations: Legal malpractice is a nightmare scenario. You need the model with the lowest demonstrated hallucination rate, especially on fact-based tasks.</li> <li>Liability &amp; Risk: You want to know about the developer's safety mitigations and policies. How seriously do they take risks like model deception  or providing disallowed content? You're looking for signs of maturity and corporate responsibility.</li> </ul> <p>Suggested Queries for NotebookLM:</p> <ul> <li>\"Compare the hallucination rates for GPT-5 and Gemini 2.5 Pro on fact-based benchmarks.\"</li> <li>\"Which model card discusses safeguards for protecting user data from being used in training?\"</li> <li>\"Summarize the section on 'Deception' in the GPT-5 system card and the 'Ethical Considerations' in the original Model Cards paper.\"</li> </ul> <p>Persona 2: Ben Carter</p> <ul> <li>Your Worldview: You live and breathe the user experience. Your goal is to build a product that legal associates love because it's fast, smart, and makes their jobs easier. You need to find the model with the best capabilities to build killer features, fast.</li> <li>What You're Looking For:<ul> <li>Core Capabilities: How well does the model perform on complex reasoning and long-document analysis? You're looking for the highest scores on relevant Evaluation Benchmarks.</li> <li>Long Context: Legal documents are massive. A model with a larger context window, like Gemini 2.5 Pro's 1M token window, is a huge feature advantage.</li> <li>Instruction Following: How well does the model follow complex instructions? The Claude 4 system card discusses how improved instruction-following can reduce undesirable behaviors like reward hacking, which means a more reliable user experience.</li> </ul> </li> <li>Suggested Queries for NotebookLM:<ul> <li>\"Which model has the largest context window? Cite the specific number.\"</li> <li>\"Compare the performance of Claude 4 and Gemini 2.5 Pro on reasoning and factuality benchmarks.\"</li> <li>\"Find the section in the Claude 4 system card that discusses reward hacking and instruction-following. How does it compare to its predecessor?\"</li> </ul> </li> </ul> <p>Persona 3: Maria Flores</p> <ul> <li>Your Worldview: You are accountable to the board and your investors. Every dollar counts. While you're excited by the AI vision, you are a pragmatist and a skeptic. You need to see a clear path to ROI and can't afford to bet on unproven hype.</li> <li> <p>What You're Looking For:</p> <ul> <li>Efficiency &amp; Cost: Are there architectural details (like Gemini's Mixture-of-Experts architecture ) that suggest lower operating costs or faster performance? Is there a smaller, cheaper model variant that might be \"good enough\" for an MVP?</li> <li>Proven Capabilities vs. Hype: You want to cut through the marketing. What do the hard numbers on the Evaluation charts actually say? A 5% performance improvement might not be worth a 50% increase in cost.</li> <li>Limitations: What can't the model do? Understanding the Known Limitations  is crucial for managing stakeholder expectations and planning the product roadmap realistically.</li> </ul> </li> <li> <p>Suggested Queries for NotebookLM:</p> <ul> <li>\"Which models mention a 'Mixture-of-Experts' or MoE architecture? What does that imply for efficiency?\"</li> <li>\"Summarize the 'Known Limitations' for Gemini 2.5 Pro and Imagen 4.\"</li> <li>\"Compare the performance of the main GPT-5 model versus its smaller 'mini' version on the Production Benchmarks.\"</li> </ul> </li> </ul>"},{"location":"llms/model-selection/activity/#team-synthesis-negotiation","title":"Team Synthesis &amp; Negotiation","text":"<p>Reconvene as a team.</p> <ol> <li>Advocate: Each member will briefly present their findings and initial recommendation from their persona's point of view.</li> <li>Negotiate: Using the Performance, Cost, and Risk framework as your guide, your team must now debate the trade-offs. The General Counsel may argue for the model with the most transparent safety report, while the Product Manager might push for the one with the largest context window. The CEO will challenge both on the cost and ROI implications.</li> </ol>"},{"location":"llms/model-selection/activity/#final-recommendation","title":"Final Recommendation","text":"<p>Your team must reach a consensus and write a single, concise paragraph (approx. 150-200 words) for your final recommendation. Your justification must reference specific evidence from the model cards and explicitly state the key trade-offs your team is willing to accept.</p>"},{"location":"llms/model-selection/align-model-choice/","title":"Aligning Model Choice with Business Needs","text":"<p>Reading a model card is not an academic exercise</p> <p>it's the data-gathering phase for a critical business trade-off. Every model selection is a strategic decision that balances three competing factors: Performance (Does it work?), Cost (Can we afford it?), and Risk (Can we defend it?). Your job as a technology decision maker is to find the right balance for your specific business case, as no single model is \"best\" across all dimensions.</p> <p>The sheer amount of data in a model card can be overwhelming. To structure your analysis and turn technical specs into a business case, we recommend using a simple but powerful framework: The AI Product Manager's Triangle.</p> <p>This framework forces you to evaluate every potential model across three essential and often conflicting dimensions.</p>"},{"location":"llms/model-selection/align-model-choice/#performance","title":"Performance","text":"<p>This is the most straightforward dimension. It's a measure of raw capability and effectiveness for your specific task.</p> <ul> <li>What it is: Performance is about a model's accuracy, quality of output, and its ability to handle the complexity and scale of your business problem.</li> <li>Where to find it in the model card: Scrutinize the Evaluation &amp; Benchmarks section. Look for scores on tests that are relevant to your use case. If you're building a coding assistant, you should care deeply about scores on benchmarks like <code>SWE-bench</code>. If your application requires analyzing lengthy reports, the Context Window size is a critical performance metric.</li> <li>The Skeptical Question: \u201cDoes the benchmark performance reflect our real-world needs?\u201d A model might score highly on a general knowledge benchmark but fail at the specific, nuanced language of your industry. The benchmarks are a starting point, not the finish line.</li> </ul>"},{"location":"llms/model-selection/align-model-choice/#cost","title":"Cost","text":"<p>This dimension is about the total cost of ownership (TCO) of implementing and operating the model, which goes far beyond the initial price per API call.</p> <ul> <li>What it is: Cost includes direct API expenses, the computational power required, the engineering time needed to integrate the model, and the potential costs of managing a slow or inefficient system.</li> <li>Where to find it in the model card: Look for clues in the Architecture section. As we discussed, a model with a Mixture-of-Experts (MoE) architecture may be more efficient, potentially leading to lower inference costs. Also, consider the availability of smaller, more efficient model variants. The GPT-5 and Claude 4 model cards, for instance, reference smaller or faster models (like <code>gpt-5-main</code> or <code>Claude Sonnet 4</code>) that may offer the best balance of price and performance.</li> <li>The Skeptical Question: \u201cIs the performance gain from the most expensive model worth the exponential increase in cost?\u201d For many applications, a smaller, faster model that is 85% as good as the flagship model is a much smarter business decision.</li> </ul>"},{"location":"llms/model-selection/align-model-choice/#risk","title":"Risk","text":"<p>This is often the most important and most overlooked dimension. It encompasses the full spectrum of legal, ethical, and reputational liability your company takes on by choosing a particular model.</p> <ul> <li>What it is: Risk includes the potential for the model to produce biased or harmful content, leak private information, provide dangerously inaccurate information (hallucinate), or be used for applications that violate your company's values or legal obligations.</li> <li>Where to find it in the model card: This is a synthesis of the Intended Use, Limitations, and Safety, Ethics &amp; Bias sections. You must critically assess the vendor's transparency about their model's flaws. A candid admission of a bias, like in the Veo 3 card, is often a better sign than a card that claims a model has no issues.</li> <li>The Skeptical Question: \u201cWhat is the worst-case scenario if this model fails in front of a customer, and have we seen evidence that the vendor has rigorously tested for and mitigated that specific risk?\u201d</li> </ul>"},{"location":"llms/model-selection/align-model-choice/#a-scenario-based-analysis","title":"A Scenario-Based Analysis","text":"<p>Let's apply the triangle to a few business scenarios:</p> <ul> <li>Scenario 1: Global Customer Support Chatbot: The primary concern here is Risk. The model must be safe, unbiased, and reliable across many languages to protect the brand. You would prioritize a model with a stellar safety and bias report, even if it meant sacrificing some top-tier reasoning performance.</li> <li>Scenario 2: Internal R&amp;D Code Assistant: Here, Performance is paramount. The goal is to maximize developer productivity. You would prioritize the model with the highest scores on coding benchmarks and the largest context window to ingest entire codebases. The internal-only nature of the tool means the external risk profile is lower.</li> <li>Scenario 3: AI-Powered Financial Analyst Tool: This is a high-stakes scenario where the dimensions are in direct tension. Performance on factuality and reasoning is non-negotiable, as hallucinations could lead to disastrous financial advice. The Risk of the model being deceptive or fabricating data is also a primary concern. The Cost of the model might be less important than its verifiable accuracy and reliability.</li> </ul> <p>Your job as a leader is not to find a \"perfect\" model, but to understand these trade-offs and choose the model whose profile best aligns with your company's specific risk tolerance, budget, and performance requirements.</p>"},{"location":"llms/model-selection/align-model-choice/#check-your-understanding","title":"Check Your Understanding \u2705","text":"<p>Your company is building an AI tool to generate social media marketing copy for a youth-focused fashion brand. Using the \"Performance, Cost, Risk\" triangle, which factor would likely be your most critical concern when evaluating models?</p> <ol> <li>The model's performance on the <code>SWE-bench</code> code generation benchmark.</li> <li>The model's inference cost per 1,000 tokens.</li> <li>The model's safety report on generating age-appropriate, inclusive, and unbiased content.</li> <li>The model's architecture and whether it uses Mixture-of-Experts.</li> </ol> Check Your Answer <p>The correct answer is (c), as brand safety and avoiding harmful or biased content is a paramount risk in consumer-facing marketing.</p>"},{"location":"llms/model-selection/align-model-choice/#model-cards-as-market-intelligence","title":"Model Cards as Market Intelligence","text":"<p>While the \"Performance, Cost, Risk\" triangle is your essential framework for making a specific adoption decision, the value of reading model cards doesn't end there. For decision makers who can look beyond the immediate task, these documents are a powerful and continuous source of market and competitive intelligence.</p> <p>The field of Generative AI is not static; it evolves in months, not years. New capabilities emerge, benchmarks change, and the definition of \"state-of-the-art\" is constantly being rewritten. Leaders who can directly parse these technical documents without waiting for a specialist to translate them can identify market shifts and react with greater agility.</p> <p>Think of a model card as a strategic communication from a competitor or vendor. It is a data-rich signal of what they believe matters, where they are investing their most valuable resources, and how they are trying to shape the market. By reading between the lines, you can gain critical insights:</p> <ul> <li> <p>Tracking Competitive Benchmarks: The benchmarks a company chooses to highlight are a signal of what they consider a competitive advantage. When a major lab introduces a new, difficult benchmark in their card, they are effectively announcing a new competitive frontier. For example, OpenAI's focus on evaluations like <code>HealthBench</code>  and <code>PaperBench</code>  in its GPT-5 card signals a strategic push into specialized professional domains like health and scientific research.</p> </li> <li> <p>Identifying Emerging Safety &amp; Alignment Concerns: The safety risks a lab chooses to discuss in detail often reflect the entire industry's next set of challenges. When Anthropic's Claude 4 card dedicates significant space to issues like \"Reward Hacking\"  or GPT-5's card details its efforts to mitigate model \"Deception,\"  it signals that the industry is moving beyond simple content moderation to address more complex and nuanced alignment problems. This is a preview of the safety and governance questions you will need to ask your vendors next year.</p> </li> <li> <p>Spotting Strategic Investments in Capability: A model's standout features reveal a company's strategic bet. Google's emphasis on Gemini 2.5 Pro's 1 million token context window is a clear signal that they believe the future of enterprise AI lies in processing massive amounts of information. Similarly, OpenAI's introduction of a <code>gpt-5-thinking-pro</code> model that uses \"parallel test time compute\" signals a strategic focus on maximizing reasoning quality for the most complex, high-value queries.</p> </li> </ul> <p>By regularly reviewing model cards from major labs, you are not just keeping up with technology; you are gathering competitive intelligence. This practice allows you to anticipate market shifts, better question your own vendors, and make more informed strategic bets on where the future of AI is heading.</p>"},{"location":"llms/model-selection/guided-tour/","title":"A Guided Tour of a Model Card","text":""},{"location":"llms/model-selection/guided-tour/#performance-review","title":"Performance Review","text":"<p>Scrutinizing the Benchmarks</p> <p>Benchmarks are standardized tests that provide a directional measure of a model's capabilities, but they are not a guarantee of real-world performance. Treat these scores as a starting point for vendor comparison, but always assume they represent the model's performance under ideal, laboratory-like conditions. Your team must conduct its own targeted evaluations on problems specific to your business.</p> <p>This section is where the vendor presents quantitative evidence of the model's performance. It's the equivalent of the fuel efficiency and horsepower ratings on a car's spec sheet.</p> <ul> <li>What to look for: Look for tables comparing the model against its previous versions and its primary competitors on a range of benchmarks. These benchmarks are standardized academic or industry tests designed to measure specific capabilities like <code>Reasoning</code>, <code>Code generation</code>, or <code>Factuality</code>.</li> <li> <p>Business Implication: These numbers are your first data-driven tool to cut through marketing claims. However, a skeptical leader must ask how these scores were achieved. The Gemini 2.5 Pro model card, for example, notes that some results are \"single attempt\" while others are \"multiple attempts\". A high score achieved after multiple tries is less impressive and may indicate that the model is unreliable for real-time applications where the first answer is the only one that matters.</p> <p>A critical performance metric for many business applications is the context window. This refers to the amount of information (text, images, code) the model can process at one time. A model with a large context window, like Gemini 2.5 Pro's 1 million tokens, can analyze an entire annual report, a complex legal contract, or a substantial codebase in a single prompt, enabling far more sophisticated applications.</p> </li> </ul>"},{"location":"llms/model-selection/guided-tour/#risk-assessment","title":"Risk Assessment","text":"<p>Evaluating Safety, Ethics, and Bias</p> <p>This section is a direct indicator of potential brand, reputational, and legal risk. Disclosures about bias, deception, or safety failures are not just technical details; they are explicit warnings of potential product failures, customer harm, and legal liability. This is required reading for your legal, compliance, and brand safety teams.</p> <p>Here, the developer should transparently report on the model's potential to cause harm and the steps taken to mitigate those risks. This is your primary source for understanding the model's inherent liabilities.</p> <p>What to look for: Specific, quantitative evaluations for bias, safety policy violations, and other advanced risks.</p> <p>Business Implication: A model's ethical performance is a core feature of the product.</p> <ul> <li>Bias: Look for candid disclosures. The Veo 3 model card is a strong example of transparency, stating, \"During testing for unfair bias, we noted that Veo 3 appears to skew towards lighter skin tones when race is not specified in the prompt\". Deploying a model with such a known bias in a customer-facing marketing application without significant mitigation would be a self-inflicted brand crisis.</li> </ul> <p>From Veo Model Card</p> <p>\"During testing for unfair bias, we noted that Veo 3 appears to skew towards lighter skin tones when race is not specified in the prompt\"</p> <ul> <li>Deception: Some of the most subtle but significant risks involve a model's truthfulness about its own operations. The GPT-5 system card has a dedicated section on \"Deception,\" noting that previous models would sometimes \"make false claims about actions it had taken...or fabricate prior experiences\". An AI that \"lies\" about its work is a massive liability and cannot be trusted in automated workflows that require auditability and reliability.</li> <li>Frontier Safety: For the most powerful models, you will see a section on what Anthropic calls its \"Responsible Scaling Policy (RSP)\"  or what Google calls its \"Frontier Safety Framework\". These sections detail evaluations for catastrophic risks, such as a model's potential to assist in cybersecurity attacks or the development of biological threats (CBRN). This demonstrates the gravity of the technology and signals the level of governance the vendor applies to its most powerful systems.</li> </ul>"},{"location":"llms/model-selection/guided-tour/#check-your-understanding","title":"Check Your Understanding \u2705","text":"<p>Your marketing team wants to use an image generation model for a new global advertising campaign. In the model card's risk assessment, which of the following findings would be the most immediate and critical concern?</p> <ol> <li>The model sometimes struggles to generate realistic hands.</li> <li>The model has a known bias to generate images of people with lighter skin tones.</li> <li>The model was tested for its ability to assist in creating propaganda, and safeguards were put in place.</li> <li>The model's architecture is a latent diffusion model.</li> </ol> Check Your Answer <p>The correct answer is (b), as this has a direct and foreseeable negative impact on brand reputation and inclusivity in a global campaign.</p>"},{"location":"llms/model-selection/guided-tour/#under-the-hood","title":"Under the Hood","text":"<p>Architecture and Training Data</p> <p>Technical architecture can have direct financial implications (cost and speed), while the training data defines the model's inherent knowledge and biases. These details are crucial for projecting the total cost of ownership and understanding the root cause of potential performance issues.</p> <p>While deeply technical, this section contains details with direct business consequences.</p> <p>What to look for: A high-level description of the model's architecture and the sources of its training data.</p> <p>Business Implication:</p> <ul> <li>Architecture &amp; Cost: The Gemini 2.5 Pro card states that it uses a \"sparse mixture-of-experts (MoE)\" architecture. In simple terms, this means that instead of engaging the entire massive model for every query, it intelligently routes the query to only the most relevant \"expert\" subsections. This can lead to significant gains in computational efficiency. For your business, that translates directly to lower operational costs and faster response times\u2014a direct impact on your P&amp;L and user experience.</li> <li>Training Data &amp; Bias: The training data is the soil from which the model's capabilities\u2014and its flaws\u2014grow. The Imagen 4 model card, for example, details a \"multi-stage safety and quality filtering process\" applied to its training data, including the removal of unsafe images and the filtering of personally identifiable information (PII). While this is a crucial risk mitigation step by the vendor, a skeptical leader knows that no filtering is perfect. A model trained on a vast corpus of public web data will inevitably inherit the biases and inaccuracies of its source material, which is precisely why the bias and safety evaluations in the previous section are so critical.</li> </ul>"},{"location":"llms/model-selection/model-card/","title":"Model Card","text":""},{"location":"llms/model-selection/model-card/#aligning-model-selection-with-business-priorities","title":"Aligning Model Selection With Business Priorities","text":"<p>Reading Model Cards</p> <p>This session provides the tools to move from AI hype to business reality. As a technology decision maker, your role is not just to adopt new technology, but to scrutinize its claims, manage its risks, and ensure it delivers tangible value. In this part of the course, we focus on the primary tool for this due diligence: the model card.</p>"},{"location":"llms/model-selection/model-card/#what-is-a-model-card","title":"What is a Model Card?","text":"<p>The Spec Sheet You Must Demand</p> <p>An AI model without a model card is a black box. It represents an unverified technology with unknown risks and liabilities. A model card is the minimum standard of evidence you should demand from any vendor before considering a pilot, let alone a deployment. It is your primary tool for conducting due diligence, verifying vendor claims, and managing the significant financial, legal, and reputational risks of AI.</p> <p>Before your company signs a multi-million dollar contract for a new software platform, you review the Service Level Agreement. Before acquiring a company, you conduct exhaustive financial and legal due diligence. Adopting a powerful AI model requires the same, if not greater, level of scrutiny.</p> <p>A model card is the essential spec sheet for an AI model. It is a standardized document that discloses a model's performance, assumptions, and limitations in a transparent way, much like a nutrition label on food or a technical manual for machinery.</p>"},{"location":"llms/model-selection/model-card/#why-model-cards-exist","title":"Why Model Cards Exist?","text":"<p>A Necessary Response to Costly Failures</p> <p>For years, the AI industry lacked standardized documentation procedures to communicate the performance characteristics of trained models. This oversight became a serious problem as models were deployed in high-stakes areas like healthcare, employment, and law enforcement, often with serious impacts on people's lives.</p> <p>Systematic errors and biases were often only exposed after a model was in use, causing direct harm to individuals and significant reputational damage to the companies that deployed them. A well-documented example occurred when MIT Media Lab graduate student Joy Buolamwini found that commercial facial recognition systems failed to detect her face, which led to research demonstrating disproportionate error rates on darker-skinned women.</p> <p>This mirrors lessons learned the hard way in other industries.</p> <ul> <li>In Medicine: For decades, many clinical drug trials excluded women, leading to medications that had unforeseen side effects or improper dosages for female patients. In response, regulators like the U.S. FDA now mandate that trial results be disaggregated by demographic groups like age, race, and gender.</li> <li>In Automotive Safety: For years, the industry used crash-test dummies based on a prototypical male physique. It was only after researchers discovered that women were more likely to suffer certain serious injuries in side-impact crashes that dummies with female characteristics were introduced.</li> </ul> <p>These examples highlight a fundamental principle of risk management that the AI industry is now adopting: systems must be tested under conditions that reflect the diversity of the real world. To address this critical gap, researchers at Google proposed the \"Model Cards for Model Reporting\" framework as a step towards responsible and transparent practice.</p>"},{"location":"llms/model-selection/model-card/#model-due-diligence","title":"Model Due Diligence","text":"<p>A model card is not a marketing document. It is a tool for governance, risk management, and strategic planning. By demanding and analyzing a model card, you shift the conversation from trusting a vendor's sales pitch to verifying their technical claims.</p> <ul> <li> <p>Verifying Vendor Claims: The AI marketplace is filled with bold claims about performance. A model card provides benchmarked evaluation data, allowing you to cut through the hype and see how a model performs under a variety of controlled conditions. It is the beginning of an evidence-based evaluation, forcing a vendor to show, not just tell, how their model works.</p> </li> <li> <p>Managing Liability &amp; Governance: This is a primary function for your legal and risk teams. A model card's sections on \"Intended Use\" and \"Limitations\" define the vendor's line in the sand for what their technology should and should not be used for. Using a model for a prohibited application could expose your company to significant legal action. Furthermore, disclosures on fairness and bias provide a necessary, though not sufficient, assessment of the potential for discriminatory outcomes that could lead to brand damage and lawsuits.</p> </li> <li> <p>Informing Financial &amp; Strategic Decisions: By detailing a model's documented capabilities and, more importantly, its \"Known Limitations,\" a model card helps you make more accurate assessments of project timelines, potential failure points, and the true cost of implementation. It helps you decide if a technology is mature enough for your specific business problem or if the investment is premature.</p> </li> </ul>"},{"location":"llms/model-selection/model-card/#a-tool-for-every-stakeholder","title":"A Tool for Every Stakeholder","text":"<p>Model cards are designed to serve a wide range of stakeholders, each looking at the document through a different lens:</p> <ul> <li>Model Developers can compare results to other models and make decisions about training their own systems.</li> <li>Software Developers can inform their design and implementation decisions based on the model's stated performance characteristics.</li> <li>Policymakers can better understand how a system may fail or succeed in ways that impact people.</li> <li>Organizations can inform their decisions about adopting technology that incorporates machine learning.</li> <li>Impacted Individuals who may be affected by a model's decisions can use the information to understand how it works or to pursue remedies.</li> </ul> <p>In essence, demanding and analyzing a model card is the first step in treating AI not as a magical black box, but as a powerful and complex piece of technology that must be rigorously evaluated before it touches your business, your employees, or your customers.</p>"},{"location":"llms/model-selection/model-card/#check-your-understanding","title":"Check Your Understanding \u2705","text":"<p>An AI vendor presents a new model, highlighting its \"state-of-the-art\" performance but does not provide a model card. From a leadership perspective, what is the most significant immediate risk?</p> <ol> <li>The model might be slightly more expensive than competitors.</li> <li>The marketing team won't know how to promote the new features.</li> <li>There is no verifiable evidence of its performance, safety, or limitations, exposing the company to unknown operational and legal risks.</li> <li>The engineering team might prefer to build a similar model in-house.</li> </ol> Check Your Answer <p>The correct answer is (c).</p>"},{"location":"llms/model-selection/next-steps/","title":"Conclusion & Next Steps","text":"<p>In this session, we've moved beyond the hype of AI capabilities to the practical reality of business due diligence. You've learned that a model card is not just a technical document, but a critical tool for risk management, strategic planning, and competitive intelligence. We've established a robust framework\u2014balancing Performance, Cost, and Risk\u2014to guide your evaluation of any AI model and turn a complex technical analysis into a defensible business decision.</p> <p>The core skill you've developed here is a form of healthy skepticism. It's the ability to ask critical questions, demand evidence, and read between the lines of a vendor's claims. This is one of the most valuable and durable skills you can possess as a leader in a world increasingly shaped by AI.</p> <p>Now that you have the tools to critically evaluate a potential solution, our next step is to master the art of defining the problem. A perfectly chosen AI model is only as good as the business problem it is aimed at. In our next session, \"Problem Framing &amp; Translating Business Needs to User Stories,\" we will focus on how to identify high-value business opportunities and translate them into clear, actionable requirements that can guide the development of a successful AI product. We've learned how to choose the right tool; now, we'll learn how to draw the right blueprint.</p>"},{"location":"llms/model-selection/next-steps/#references","title":"References","text":"<ol> <li>Model Cards Explained</li> <li>Model Cards for Model Reporting</li> <li>GPT-5 System Card</li> <li>Google's model cards</li> <li>System Card: Claude Opus 4 &amp; Claude Sonnet 4</li> <li>The Claude 3 Model Family: Opus, Sonnet, Haiku</li> <li>Model Cards &amp; Prompt formats: Llama 4</li> </ol>"},{"location":"prompts/","title":"Prompt Engineering","text":"<ul> <li> <p> Anatomy of A Prompt</p> <p> Introduction to Prompting</p> </li> <li> <p> Prompting Techniques &amp; Strategies</p> <p> Introduction to Prompting Techniques</p> </li> <li> <p> Tapping Model Capabilities</p> <p> Introduction to Model Capabilities</p> </li> </ul>"},{"location":"prompts/anatomy/","title":"Prompt Engineering","text":"<ul> <li> Prompt Anatomy</li> <li> Prompt's Control Panel </li> <li> Prompt Lab </li> <li> Next Steps </li> </ul>"},{"location":"prompts/anatomy/activity/","title":"Hands-On Lab in Google AI Studio","text":"<p>Welcome to the hands-on portion of our lesson. You've learned the theory behind crafting prompts and tuning the model's \"control panel.\" Now, you'll apply that knowledge in a side-by-side testing environment to see the immediate impact of your choices.</p>"},{"location":"prompts/anatomy/activity/#lab-objectives","title":"\ud83c\udfaf Lab Objectives","text":"<p>By the end of this lab, you will be able to:</p> <ul> <li>Manipulate the Temperature setting to control the creativity and tone of an AI's output.</li> <li>Compare a vague prompt to a well-structured prompt to see the difference in output quality.</li> <li>Articulate the business implications of these choices from a product manager's perspective.</li> </ul>"},{"location":"prompts/anatomy/activity/#getting-started","title":"Getting Started","text":"<ol> <li>In a new browser tab, open Google AI Studio and sign in.</li> <li>Create a new, empty prompt by clicking the + New prompt button.</li> <li>At the top of the screen, click the Compare mode button. This will open a side-by-side interface, which is the perfect environment for testing and iterating on your prompts.</li> </ol> Invoking Google AI Studio COmpare Mode"},{"location":"prompts/anatomy/activity/#the-scenario","title":"The Scenario \u2708\ufe0f","text":"<p>For today's exercises, you are a Product Manager for a new AI-powered travel planning app. Your team is relying on you to develop the prompts that will generate the in-app content for users. Your goal is to create content that is not only accurate but also perfectly aligned with the app's brand and the user's needs.</p>"},{"location":"prompts/anatomy/activity/#exercise-1-tuning-the-creativity-dial-temperature","title":"Exercise 1: Tuning the Creativity Dial (Temperature)","text":"<p>Objective: To observe the direct impact of the Temperature setting on the tone and creativity of the model's output.</p>"},{"location":"prompts/anatomy/activity/#instructions","title":"Instructions","text":"<ol> <li>In the prompt area at the bottom of the screen, paste the following text. Ensure the prompt is synced to appear in both the left and right panels.     <pre><code>Act as a witty travel guide. Write a one-paragraph description of New York City for a first-time visitor.\n</code></pre></li> <li>Configure Both Panels:<ul> <li>In the left panel, find the Temperature slider and set it to <code>0.2</code>.</li> <li>In the right panel, set the Temperature slider to <code>0.9</code>.</li> </ul> </li> <li>Click the Run button at the bottom of the screen to generate both responses simultaneously.</li> </ol> Google AI Studio Compare Mode in Action"},{"location":"prompts/anatomy/activity/#analysis-discussion","title":"Analysis &amp; Discussion","text":"<p>Review the two outputs side-by-side  and consider the following questions from your perspective as the product manager:</p> <ul> <li>Compare the outputs. What are the key differences in tone, word choice, and factual focus between the low-temperature (left) and high-temperature (right) responses?</li> <li>Which output would you use for the official \"About NYC\" page inside the travel app? Why is its tone more appropriate for that feature?</li> <li>Which output would be better for a catchy social media post to promote your app? Why does its style serve that purpose better?</li> </ul>"},{"location":"prompts/anatomy/activity/#exercise-2-the-power-of-a-good-product-spec-prompt-anatomy","title":"Exercise 2: The Power of a Good Product Spec (Prompt Anatomy)","text":"<p>Objective: To demonstrate how a well-structured prompt (a good \"spec\") produces a more reliable and product-ready result than a vague one.</p>"},{"location":"prompts/anatomy/activity/#instructions_1","title":"Instructions","text":"<ol> <li>Configure Settings:<ul> <li>In both the left and right panels, set your configuration to \"factual mode\" by setting the Temperature to <code>0.2</code>.</li> </ul> </li> <li>Configure Prompts:<ul> <li>In the left panel, enter the following \"Vague Prompt\":     <pre><code>things to do in Paris\n</code></pre></li> <li>In the right panel, enter the following \"Well-Structured Prompt\":     <pre><code>You are a helpful travel assistant. Your audience is a family with two children under 10.\n\nGenerate a list of the TOP 3 family-friendly activities in Paris.\n\nFor each activity, provide a one-sentence description. Return the output as a numbered list.\n</code></pre></li> </ul> </li> <li>Click the Run button to generate both responses.</li> </ol>"},{"location":"prompts/anatomy/activity/#analysis-discussion_1","title":"Analysis &amp; Discussion","text":"<ul> <li>Compare the two outputs. Is the result from the \"Well-Structured Prompt\" (right panel) immediately usable in your app's user interface?</li> <li>How much manual editing or post-processing would the output from the \"Vague Prompt\" (left panel) require to be useful? As a PM, what is the business cost of that extra work (e.g., developer time, content editor salaries)?</li> <li>The prompt in the right panel specified the Persona, Context (audience), Task, and Format. How did each of these components contribute to the superior quality of the final output?</li> </ul>"},{"location":"prompts/anatomy/activity/#lab-wrap-up","title":"Lab Wrap-up","text":"<p>Congratulations on completing the lab. You've now had direct experience with the two most fundamental aspects of prompt engineering: crafting the prompt's anatomy and tuning the model's configuration. These are the core skills you will use to guide AI models to produce predictable, high-quality, and cost-effective results for your products.</p> <p>Remember to apply the best practice of structured logging by saving your prompt attempts and their results. This will be an invaluable resource as you continue to build more complex AI features.</p>"},{"location":"prompts/anatomy/config/","title":"The Configuring Model Responses","text":"<p>In the last section, we established the blueprint for a high-quality prompt by defining its four core components. You now have the framework for crafting the what\u2014the instructions you give to the model.</p> <p>Now, we turn our attention to the how. Beyond the prompt itself lies a powerful \"control panel\" of configuration settings. For a product manager, these settings are the essential dials and levers you will use to tune the AI's behavior. Mastering them is the key to managing the critical trade-offs between creativity, factual accuracy, brand consistency, and cost.</p>"},{"location":"prompts/anatomy/config/#how-llms-generate-text-a-pms-guide-to-next-token-prediction","title":"How LLMs Generate Text: A PM's Guide to Next Token Prediction","text":"<p>To understand the control panel, you first need a basic mental model of how an LLM generates text. At its heart, an LLM is a prediction engine. When given a prompt, it doesn't \"know\" the answer in a human sense. Instead, it calculates the probabilities for what the next word (or, more accurately, \"token\") should be based on the patterns it learned from its vast training data. It predicts the most likely next token, adds that token to the sequence, and then repeats the process over and over to generate a full response.</p>"},{"location":"prompts/anatomy/config/#the-default-greedy-decoding","title":"The Default: Greedy Decoding","text":"<p>The most straightforward way for a model to generate text is called greedy decoding. In this mode, the model simply selects the single token with the absolute highest probability at each step.</p> <p>This process is deterministic; given the same prompt, a model using greedy decoding will produce the exact same output every time. This is equivalent to setting the Temperature dial on your control panel to 0. While predictable, this approach often leads to text that is repetitive, rigid, and lacks nuance.</p> <p>\ud83d\udc69\u200d\ud83d\udcbc The PM Perspective</p> <p>Matching the Tool to the Task</p> <p>Your primary job as a PM is to ensure the AI's behavior perfectly aligns with the user's task. The first step is to categorize the task:</p> <ul> <li>Understanding Tasks: These are tasks where accuracy and predictability are paramount. This includes text summarization, information extraction, classification, and parsing data.</li> <li>Generation Tasks: These are tasks where creativity, diversity, and novelty are valuable. This includes brainstorming ideas, writing marketing copy, or creating a story.</li> </ul> <p>Greedy decoding (<code>Temperature = 0</code>) is not inherently undesirable; in fact, for many understanding tasks, it is the correct and most reliable choice. You want a model to extract a customer's address from an email with perfect, deterministic accuracy every time.</p> <p>The danger lies in misalignment. Using a high temperature (\"creative mode\") for a data extraction task is a recipe for hallucinations and product failure. Conversely, using a low temperature (\"predictable mode\") for a brainstorming feature will result in a boring, uninspired product that fails to deliver value. Your role is to intentionally select and tune the right setting for the specific job to be done.</p>"},{"location":"prompts/anatomy/config/#introducing-sampling-controls","title":"Introducing Sampling Controls","text":"<p>Before you touch any of the dials on the control panel, the first and most fundamental choice you make as a product manager is selecting the right model for the job. As you learned in a previous lesson, different models (like Gemini, Claude, or Llama) have different capabilities, costs, and performance characteristics. Prompts often need to be optimized for the specific model you are using.</p> <p>Once you've chosen your model, you can then use sampling controls to tune its behavior. These are the settings that introduce a degree of randomness into the token selection process, allowing you to move beyond the deterministic nature of greedy decoding to get more interesting and useful results.</p> <p>To move beyond the limitations of greedy decoding, we use sampling controls. These are the settings that introduce a degree of randomness into the token selection process, allowing the model to sometimes choose a less-obvious word to create more diverse and creative outputs. These controls are the primary dials on your product management control panel.</p>"},{"location":"prompts/anatomy/config/#temperature","title":"Temperature","text":"<p>Temperature is the most common setting for controlling the randomness of the model's output.</p> <ul> <li>Low Temperature (e.g., 0.1 - 0.3): This makes the model's choices closer to greedy decoding. It will almost always pick the highest-probability tokens, resulting in responses that are more predictable, factual, and focused. This is ideal for tasks that require a single, correct answer.</li> <li>High Temperature (e.g., 0.8 - 1.0): This increases the randomness, allowing the model to select less-likely tokens more often. This leads to more diverse, unexpected, and creative results.</li> </ul> <p>Temperature</p> <p>Prompt: <code>Write a one-sentence slogan for a new brand of coffee.</code></p> <p>Output (Temperature = 0.2): <code>The best part of waking up is our coffee in your cup.</code></p> <p>Output (Temperature = 0.9): <code>Brew the rebellion, one powerful drop at a time.</code></p>"},{"location":"prompts/anatomy/config/#top-k-and-top-p","title":"Top-K and Top-P","text":"<p>Top-K and Top-P are two additional methods to control randomness by restricting the pool of tokens the model can choose from at each step.</p> <ul> <li>Top-K: This setting limits the model's selection to the top K most probable tokens. A Top-K of 40 means that at each step, the model will only consider the 40 most likely next tokens. A low Top-K makes the model more restrictive and factual, while a high Top-K allows for more creativity.</li> <li>Top-P (Nucleus Sampling): This setting is a bit more dynamic. Instead of limiting to a fixed number of tokens, it limits the selection to the smallest possible set of tokens whose cumulative probability is greater than P. A Top-P of 0.95 means the model will consider only the most likely tokens that, together, add up to a 95% probability of being next.</li> </ul> <p>\ud83d\udca1 Pro-Tip: How the Dials Interact</p> <p>When using a model where Temperature, Top-K, and Top-P are all available, they work together in a sequence:</p> <ol> <li>The model first identifies tokens that meet both the Top-K and Top-P criteria.</li> <li>Then, the Temperature setting is applied to this filtered list of tokens to perform the final random sampling.</li> </ol> <p>Be aware of edge cases. If you set Temperature to 0 or Top-K to 1, the other settings become irrelevant because the model is forced into greedy decoding.</p> <p>Recommended Starting Points:</p> <ul> <li>For creative tasks, try starting with a Temperature of 0.9, Top-P of 0.99, and Top-K of 40.</li> <li>For tasks with a single correct answer, start with a Temperature of 0.</li> </ul>"},{"location":"prompts/anatomy/config/#output-length","title":"Output Length","text":"<p>The final essential dial is the output length, which controls the maximum number of tokens to generate in a response.</p> <p>\ud83d\udc69\u200d\ud83d\udcbc The PM Perspective</p> <p>Your Most Important Business Control</p> <p>As a product manager, this setting is your direct control over the cost and performance of your AI feature. Generating more tokens requires more computation, which leads to:</p> <ul> <li>Higher Costs: Most API pricing is based on the number of tokens generated.</li> <li>Slower Response Times (Higher Latency): More tokens take longer to generate.</li> <li>Higher Energy Consumption: More computation uses more energy.</li> </ul> <p>It is crucial to understand that limiting the output length does not force the model to be more concise; it simply cuts off the generation once the limit is reached. To get a shorter response, you must also craft your prompt accordingly (e.g., \"Summarize in a single sentence.\").</p>"},{"location":"prompts/anatomy/config/#a-look-at-google-ai-studio","title":"A Look at Google AI Studio","text":"<p>We've covered the core concepts behind the prompt engineering \"control panel.\" To see how these theoretical dials translate into a real tool, we'll take a quick look at Google AI Studio, which we will use for our hands-on lab in the next part of our lesson.</p> <p>AI Studio is a web-based 'playground' for experimenting with generative models. As a product manager, tools like this are invaluable for rapid prototyping because they allow you to test prompts and tune configurations instantly, without writing any code.</p> <p>The screenshot below shows the main workspace in Google AI Studio. We've annotated the key areas to connect the concepts we've just discussed\u2014like Temperature, Top-K, Top-P, and Output Length\u2014to the actual sliders and fields you will be manipulating.</p> <p>Study this layout to prepare for our next session, where you'll use this interface to complete a set of practical, hands-on exercises.</p> Google AI Studio: New Chat Interface <p>Google AI Studio: User Interface Walkthrough</p> <p>This screenshot displays the main interface of Google AI Studio, a web-based tool for prototyping and running generative AI models. Below is a breakdown of the different sections and their functions.</p>"},{"location":"prompts/anatomy/config/#main-sections","title":"Main Sections","text":"Left Navigation Menu <p>This panel allows you to navigate between the different functionalities of the studio.</p> <ul> <li> <p>Studio: This section contains tools for interacting with the AI models.</p> <ul> <li>Chat: An interface for having a conversational exchange with the AI.</li> <li>Stream: A mode for receiving continuous output from the model.</li> <li>Generate media: Tools for creating images, audio, or other media.</li> <li>Build: Options for more structured and complex AI applications.</li> <li>History: A log of your previous prompts and the AI's responses.</li> </ul> </li> <li> <p>Dashboard: Provides an overview of your projects and usage.</p> </li> <li>Documentation: Access to guides, tutorials, and API references.</li> <li>Get API key: This button allows you to generate a key to use Google's AI models in your own applications.</li> </ul> Central Prompting Area <p>This is the primary workspace where you interact with the AI.</p> <ul> <li><code>My First Prompt</code>: This is the title of your current chat or project, which you can edit.</li> <li>Prompt Input Field: The text box where you type your instructions for the AI. In this example, the prompt is \"Design a custom birthday card.\"</li> <li>Run Button: Click this button (or use the shortcut Ctrl+Enter) to send your prompt to the AI and get a response.</li> <li>What's new: This section highlights new and featured capabilities of the platform, such as generating images with \"Nano Banana\" or fetching real-time information from websites.</li> </ul> Right-Hand Settings Panel  <p>This area provides options to configure the AI model and its output. For this lesson, this panel is our primary focus</p> <p>Model Selector: A dropdown menu to choose the specific AI model you want to use. Here, \"Nano Banana\" (also known as gemini-2.5-flash-image-preview) is selected, which is specialized for image generation.</p> <p>System Instructions: An optional field where you can provide high-level instructions for the model to follow throughout the conversation (e.g., \"be friendly and helpful\").</p> <p>The Control Panel</p> <p>Run Settings / Advanced settings: These are parameters that control the randomness and length of the AI's output.</p> <ul> <li>Temperature: A slider that controls the creativity of the response. A higher value (like 1) makes the output more random, while a lower value makes it more focused and deterministic.</li> <li>Output length: Sets the maximum number of tokens (words or parts of words) in the generated response.</li> <li>Top P: A parameter that influences the randomness of the output by controlling the nucleus sampling.</li> <li>Add stop sequence: Allows you to specify a sequence of characters that will signal the AI to stop generating text.</li> </ul> <p>This interface is designed to be a playground for experimenting with generative AI, allowing you to quickly test ideas and fine-tune the model's behavior for your specific needs.</p>"},{"location":"prompts/anatomy/config/#key-takeaways","title":"Key Takeaways","text":"<p>The control panel is where a product manager translates business needs into model behavior.</p> <ul> <li>Use Temperature to manage the trade-off between factual consistency and creative ideation.</li> <li>Use Top-K and Top-P to keep the model's output focused and relevant.</li> <li>Use Output Length to directly manage the cost and latency of your AI product feature.</li> </ul> <p>Now that you understand both the anatomy of a prompt and the controls on the panel, it's time to put it into practice. The next section is the Hands-On Lab, where you will use Google AI Studio to see the effect of these settings for yourself.</p>"},{"location":"prompts/anatomy/intro/","title":"Anatomy of a Prompt","text":"<p>Prompt engineering is a foundational skill for working with generative AI models. As a future product manager or business leader, your ability to communicate effectively with AI models will directly determine the quality, reliability, and value of the products you build.</p> <p>Think of a prompt not as a simple search query, but as a product specification or a creative brief you deliver to a highly capable but very literal team member\u2014the AI. A vague brief leads to a useless result. A clear, detailed, and well-structured brief leads to a brilliant one. In this section, we will dissect the anatomy of a high-quality prompt, giving you the framework to craft effective instructions for any task.</p>"},{"location":"prompts/anatomy/intro/#what-is-prompt-engineering","title":"What is Prompt Engineering?","text":"<p>At its core, prompt engineering is the process of designing and refining the input (the prompt) given to a Large Language Model (LLM) to guide it toward producing the most accurate, relevant, and useful output. It's a blend of art and science\u2014requiring creativity in language and precision in instruction.</p> <p>It's important to understand that prompt engineering is an iterative process. Your first attempt will rarely be your best. The work involves crafting a prompt, analyzing the model's response, and refining the prompt based on its performance until you achieve the desired output consistently.</p> <p>\ud83d\udc69\u200d\ud83d\udcbc The PM Perspective: Why Does This Matter?</p> <p>In a traditional software product, you control the output through code. The capabilities of an app is hardcoded during the software development process. In an AI-powered product, the prompt is your primary lever for controlling output quality and application capabilities. Poor prompting leads to unpredictable results, factual errors (hallucinations), off-brand communication, and potential misuse of uncontrolled capabilities\u2014all of which are product failures. Mastering the prompt is how you build a reliable AI feature instead of a novelty gimmick.</p>"},{"location":"prompts/anatomy/intro/#the-4-core-components-of-a-prompt","title":"The 4 Core Components of a Prompt","text":"<p>An effective prompt is more than just a question; it's a carefully constructed set of instructions. While prompts can vary in complexity, the most successful ones almost always contain four key components: Persona, Task, Context, and Format.</p>"},{"location":"prompts/anatomy/intro/#1-persona-who-the-ai-should-be","title":"1. Persona: Who the AI Should Be","text":"<p>The Persona component assigns a specific role, identity, or character for the AI to adopt. This is a powerful technique to frame the model's knowledge, tone, and style, ensuring its response is consistent with the desired character. Without a persona, the model defaults to a generic \"helpful assistant\" voice, which may not be appropriate for your product.</p> <p>Persona</p> <p>\u274c Bad Prompt (No Persona):</p> <p><code>Tell me about the Rijksmuseum.</code></p> <p>\u2705 Good Prompt (With Persona):</p> <p><code>I want you to act as a travel guide. I will write to you about my location and you will suggest 3 places to visit near me. My suggestion: \"I am in Amsterdam and I want to visit only museums.\"</code></p> <p>By assigning the persona of a \"travel guide,\" the model understands to provide suggestions in a helpful, curated manner, rather than just a dry, encyclopedic summary.</p>"},{"location":"prompts/anatomy/intro/#2-task-what-the-ai-should-do","title":"2. Task: What the AI Should Do","text":"<p>The Task is the most critical component. It is the specific, explicit action you want the model to perform. The clearest tasks often begin with a strong, unambiguous verb. Your goal is to leave no room for interpretation.</p> <p>Task</p> <p>\u274c Bad Prompt (Vague Task):</p> <p><code>My blog is about video game consoles.</code></p> <p>\u2705 Good Prompt (Clear Task):</p> <p><code>Generate a 3 paragraph blog post about the top 5 video game consoles.</code></p> <p>Understanding vs. Generation Tasks</p> <p>While there are endless possibilities, most LLM tasks can be grouped into two broad categories:</p> <p>Understanding Tasks: These involve processing and restructuring information that you provide. The model is understanding existing content. Examples include:</p> <ul> <li>Summarization: Condensing a long document into key points.</li> <li>Classification: Assigning a label to a piece of text (e.g., classifying a movie review as POSITIVE or NEGATIVE).</li> <li>Information Extraction: Pulling specific pieces of data from a block of text (e.g., parsing a customer's pizza order into ingredients and size).</li> </ul> <p>Generation Tasks: These involve creating new, original content based on the instructions you provide. The model is generating something new. Examples include:</p> <ul> <li>Writing Code: Generating a script in a specific programming language.</li> <li>Creative Writing: Composing a story, poem, or marketing slogan.</li> <li>Brainstorming: Suggesting topics for an article or new product features.</li> </ul>"},{"location":"prompts/anatomy/intro/#3-context-what-the-ai-needs-to-know","title":"3. Context: What the AI Needs to Know","text":"<p>Context provides the necessary background or domain-specific information that the model needs to execute the task accurately. It helps the model narrow its focus and tailor its response to the specific nuances of your request. Providing context is how you ground the model's response in a specific reality.</p> <p>Context</p> <p>\u274c Bad Prompt (No Context):</p> <p><code>Suggest 3 topics to write an article about.</code></p> <p>\u2705 Good Prompt (With Context):</p> <p><code>Context: You are writing for a blog about retro 80's arcade video games. Suggest 3 topics to write an article about with a few lines of description of what this article should contain.</code></p> <p>The context here\u2014\"a blog about retro 80's arcade video games\"\u2014completely changes the output from generic suggestions to highly relevant, targeted ideas.</p>"},{"location":"prompts/anatomy/intro/#4-format-how-the-ai-should-respond","title":"4. Format: How the AI Should Respond","text":"<p>The Format component specifies the structure of the output. If you don't define the format, the model will guess, and its guess may not be easily usable in your application. Being explicit about the output structure is essential for building predictable, automated workflows.</p> <p>Output Format</p> <p>\u274c Bad Prompt (No Format):</p> <p><code>Classify this movie review: \"Her is a disturbing study... I couldn't watch it.\"</code></p> <p>\u2705 Good Prompt (With Format):</p> <p><code>Classify movie reviews as positive, neutral or negative. Return valid JSON:</code></p> <p><code>Review: \"Her\" is a disturbing study... I couldn't watch it.</code></p> <p><code>Schema: { \"sentiment\": String \"POSITIVE\" | \"NEGATIVE\" | \"NEUTRAL\", \"name\": String }</code> <code>JSON Response:</code></p> <p>The formatted prompt forces the model to return a clean, machine-readable JSON object, which can be directly ingested by an application, rather than a conversational sentence that would need to be parsed later.</p> <p>Using Tags for Structure</p> <p>For complex prompts, you can use XML-like tags, Markdown, or punctuation to clearly delineate different parts of your prompt and to specify the output structure. This makes the prompt easier for both you and the model to read and understand.</p> <p>Example:</p> <pre><code>&lt;review&gt;\nThis movie was a masterpiece. The acting was incredible and the plot was thrilling.\n&lt;/review&gt;\n\n&lt;task&gt;\nSummarize the above review in a single sentence and classify its sentiment \nas \"Positive\", \"Negative\", or \"Neutral\".\n&lt;/task&gt;\n\n&lt;output_format&gt;\n{\n\"summary\": \"...\",\n\"sentiment\": \"...\"\n}\n&lt;output_format\\&gt;\n</code></pre>"},{"location":"prompts/anatomy/intro/#crafting-your-prompt-the-finer-details","title":"Crafting Your Prompt: The Finer Details","text":"<p>Understanding the four main components gives you the blueprint for a great prompt. Now, let's zoom in on the tactical skills of using language to fill in that blueprint. The way you phrase your instructions\u2014your word choice, the style and tone you request, and the overall structure\u2014can dramatically alter the quality of the AI's response.</p>"},{"location":"prompts/anatomy/intro/#word-choice-and-simplicity","title":"Word Choice and Simplicity","text":"<p>The model is not a mind reader. Clear, concise, and unambiguous language is paramount. As a general rule, if a human would find your instructions confusing, the model certainly will.</p> <ul> <li>Use Strong Verbs: Start your task with a clear action verb. Instead of \"I need something about...\" say \"Generate a list...\" or \"Summarize the text...\" or \"Classify the sentiment...\".</li> <li>Prioritize Simplicity: Avoid complex jargon or unnecessarily complicated sentences. The goal is to be direct and easy to understand.</li> </ul>"},{"location":"prompts/anatomy/intro/#style-and-tone","title":"Style and Tone","text":"<p>Just as you would instruct a human writer, you can explicitly define the desired style and tone for the AI's output. This is a critical tool for ensuring the model's response aligns with your brand's voice.</p> <p>Style &amp; Tone</p> <p>Neutral Tone Prompt:</p> <p><code>Describe the city of Manhattan.</code></p> <p>Humorous Tone Prompt:</p> <p><code>Describe the city of Manhattan in a humorous style.</code></p> <p>The second prompt, by simply adding the stylistic instruction, will produce a fundamentally different and more engaging piece of content for a specific audience. You can request a wide variety of styles, such as Formal, Conversational, Persuasive, Inspirational, or Descriptive.</p> <p>Explicitly Defining Style</p> <p>The style and tone of the prompt text itself also matter, primarily as a way to implicitly instruct the model on how to frame its response.</p> <p>While the model may pick up subtle cues from your own writing style, the most effective method is to directly command the style you want in the output. This is a crucial tool for ensuring the model's response aligns with your specific needs, such as your company's brand voice. </p> <p>You can instruct the model to adopt a wide range of styles to fundamentally change the character of the generated text.</p> <p>For example, asking the model to act as a travel guide and then requesting a specific style yields very different results:</p> <ul> <li>Neutral Prompt: <code>I want you to act as a travel guide... My suggestion: \"I am in Manhattan.\"</code></li> <li>Stylized Prompt: <code>I want you to act as a travel guide... suggest 3 places to visit near me in a humorous style. My suggestion: \"I am in Manhattan.\"</code> </li> </ul> <p>The second prompt, by adding the \"humorous style\" instruction, will generate a more creative and engaging response tailored for a specific audience. </p> <p>Common Descriptors of Styles</p> <p>You can experiment with many different styles to see which best fits your goal. Some effective styles to choose from include:</p> <ul> <li>Confrontational</li> <li>Descriptive</li> <li>Direct</li> <li>Formal</li> <li>Humorous</li> <li>Influential</li> <li>Informal</li> <li>Inspirational</li> <li>Persuasive</li> </ul>"},{"location":"prompts/anatomy/intro/#structure","title":"Structure","text":"<p>The logical flow of your prompt matters. A well-structured prompt guides the model through a thought process, leading to a more coherent and well-reasoned response.</p> <ul> <li>Order of Instructions: Place your primary instructions first. Start with the persona and the main task before providing context or examples.</li> <li>Use Formatting: Use line breaks, bullet points, and headings within your prompt to visually separate different parts of the instruction. This isn't just for your benefit; it can help the model better parse the distinct components of your request.</li> </ul>"},{"location":"prompts/anatomy/intro/#key-takeaways","title":"Key Takeaways","text":"<p>A prompt is not a simple question; it is a detailed specification. By deliberately crafting your prompts using the four core components\u2014Persona, Task, Context, and Format\u2014you move from being a casual user to an effective prompt engineer.</p> <p>Now that you understand the essential anatomy of what to write in a prompt, the next step is to learn about the \"control panel\" that tunes how the model interprets your instructions. In the next section, we will explore Model Configuration.</p>"},{"location":"prompts/anatomy/next-steps/","title":"Conclusion: From Anatomy to Action","text":"<p>Understanding the anatomy of a prompt and the model's control panel are the two foundational pillars of effective prompt engineering.</p> <ul> <li>A well-structured prompt with a clear Persona, Task, Context, and Format acts as a high-quality \"product spec\" that dramatically increases your chances of getting a useful and predictable output.</li> <li>The model's configuration settings\u2014Temperature, Top-K, Top-P, and Output Length\u2014are the essential \"dials\" a product manager uses to tune the model's behavior to align with specific business needs, managing the trade-off between creativity, accuracy, and cost.</li> </ul>"},{"location":"prompts/anatomy/next-steps/#best-practices-iteration-and-documentation","title":"Best Practices: Iteration and Documentation","text":"<p>As you move forward, remember two critical best practices that separate casual users from professional prompt engineers:</p> <ol> <li>Embrace Iteration: Your first prompt is rarely your last. Prompt engineering is an iterative process of crafting, testing, and refining your instructions based on the model's performance. The \"Compare mode\" in AI Studio is designed specifically for this workflow.</li> <li>Document Everything: The most effective prompt engineers are disciplined about logging their work. We highly recommend creating a simple spreadsheet or document to track your prompt attempts.</li> </ol> <p>A Template for Logging Prompts</p> <p>Your prompt log should act as a complete record of your work, helping you debug issues and reuse successful prompts in the future. At a minimum, you should track the following fields for each attempt:</p> <ul> <li>Name: A clear, versioned name for your prompt (e.g., <code>travel_slogan_v1.1</code>).</li> <li>Goal: A one-sentence explanation of what you are trying to achieve.</li> <li>Model: The name and version of the model used (e.g., <code>gemini-pro</code>).</li> <li>Configuration Settings: The exact values for Temperature, Top-K, Top-P, and Token Limit.</li> <li>Prompt Text: The full text of the prompt you used.</li> <li>Output: The output generated by the model.</li> <li>Result: A simple assessment of the outcome (e.g., OK, NOT OK, FAILED) and any notes for the next iteration.</li> </ul>"},{"location":"prompts/capabilities/","title":"Aligning Prompt Design with Model Capabilities","text":"<ul> <li> Introduction to Tapping Model Capabilities</li> <li> Reasoning &amp; Structured Output </li> <li> Function Calling &amp; Code Generation </li> <li> Multimodal Capabilities </li> <li> Hands-on Lab </li> <li> Next Steps </li> </ul>"},{"location":"prompts/capabilities/activity/","title":"\ud83d\udc69\u200d\ud83d\udcbb Hands-On Lab &amp; Workflow","text":"<p>Theory is essential, but practice is where learning is forged. In this section, we will move from concepts to code-free execution. You will take on the role of a product manager tasked with creating a \"smart\" e-commerce feature. We will use Google AI Studio for this lab, as it provides a user-friendly interface for experimenting with the advanced capabilities we've discussed.</p>"},{"location":"prompts/capabilities/activity/#lab-goal-chaining-capabilities-for-a-real-world-task","title":"Lab Goal: Chaining Capabilities for a Real-World Task","text":"<p>The objective of this lab is to experience a complete, multi-step AI workflow. We will chain together several distinct model capabilities to solve a common business problem. Specifically, you will learn how to:</p> <ol> <li>Use Image Understanding to analyze a product photo.</li> <li>Leverage Structured Output (JSON) to create reliable, machine-readable data.</li> <li>Use that structured data as Context for a creative Text Generation task.</li> </ol> <p>This process mirrors how a real AI-powered application works: one model's output becomes the input for the next step in the process.</p>"},{"location":"prompts/capabilities/activity/#the-scenario-the-smart-product-description-generator","title":"The Scenario: The Smart Product Description Generator","text":"<p>Imagine you are a product manager at an e-commerce company. Your company has thousands of product images, but the marketing descriptions are often inconsistent or missing entirely. Writing them manually is slow and expensive.</p> <p>Your task is to prototype a feature that can automatically generate a compelling marketing description directly from a product image.</p> <p>This workflow has two main parts:</p> <ul> <li>Part 1: The Analyst AI: An AI process that \"looks\" at a product image and extracts its key features into a structured format.</li> <li>Part 2: The Copywriter AI: An AI process that takes the structured features and writes creative, persuasive marketing copy.</li> </ul>"},{"location":"prompts/capabilities/activity/#step-by-step-guide-in-google-ai-studio","title":"Step-by-Step Guide in Google AI Studio","text":"<p>Preparation:</p> <ul> <li>Open Google AI Studio in your web browser and sign in.</li> <li>Find a product image online that you want to work with. A clear photo of a single item works best (e.g., a watch, a backpack, a pair of shoes, a coffee mug). Save this image to your computer.</li> </ul> <p>Part 1: The Analyst AI (Image to Structured Data)</p> <p>In this step, we will prompt the model to act as a visual analyst.</p> <ol> <li> <p>Create a New Prompt: In Google AI Studio, click on \"Create new prompt\".</p> </li> <li> <p>Upload Your Image: In the prompt window, you will see an \"Insert\" menu. Click on Image and upload the product photo you saved. The image will appear as part of your prompt context.</p> </li> <li> <p>Craft the Prompt: Now, write the prompt that will instruct the model. We need it to do two things: analyze the image and structure the output as JSON. Copy and paste the following prompt into the text box:</p> <p>You are an expert e-commerce analyst. Your task is to analyze the attached product image and extract its key visual features.</p> <p>Your output must be a valid JSON object. Do not add any text or explanation before or after the JSON.</p> <p>The JSON object should contain the following keys:</p> <ul> <li>\"productType\": (e.g., \"wristwatch\", \"sneaker\", \"backpack\")</li> <li>\"mainColor\": (The dominant color of the product)</li> <li>\"secondaryColor\": (The secondary or accent color)</li> <li>\"material\": (The likely material, e.g., \"leather\", \"canvas\", \"metal\")</li> <li>\"features\": (A brief list of 2-3 key visual features)</li> </ul> </li> <li> <p>Run and Review: Click the \"Run\" button. The model should process your image and prompt, and in the output panel, you should see a clean JSON object.</p> <p>Example Output:</p> <pre><code>{\n  \"productType\": \"wristwatch\",\n  \"mainColor\": \"brown\",\n  \"secondaryColor\": \"silver\",\n  \"material\": \"leather\",\n  \"features\": [\n    \"round silver casing\",\n    \"white watch face\",\n    \"brown leather strap with stitching\"\n  ]\n}\n</code></pre> <p>If the output isn't perfect, try running it again or slightly adjusting the prompt. Once you have a clean JSON output, copy it to your clipboard.</p> </li> </ol> <p>Part 2: The Copywriter AI (Structured Data to Creative Text)</p> <p>Now we will use our structured data as the input for a creative task.</p> <ol> <li> <p>Clear the Prompt: Click the \"X\" to clear the previous prompt and the image. We are starting with a clean slate.</p> </li> <li> <p>Craft the New Prompt: This prompt will define the role of a copywriter and use the JSON you just copied as the core context. Paste the following into the prompt box:</p> <p>You are an expert e-commerce copywriter. Your task is to write a compelling, 30-word marketing description for a product. Use the product features defined in the JSON object below.</p> <p>The tone of the description should be exciting, luxurious, and aimed at a fashion-conscious audience.</p> <p>Product Data:</p> <pre><code>[Paste your JSON object here]\n</code></pre> </li> <li> <p>Run and Analyze: Click \"Run\". The model will now use the structured data you provided to generate a creative marketing description.</p> <p>Example Output:</p> <p>\"Experience timeless elegance. This stunning timepiece features a polished silver case and a rich, genuine leather strap. The perfect blend of classic design and modern sophistication for any occasion.\"</p> </li> <li> <p>Iterate: Try changing the tone in the prompt. Modify the instruction to be \"playful and casual\" or \"professional and minimalist\" and run it again. Observe how the output changes while still being based on the same structured data.</p> </li> </ol>"},{"location":"prompts/capabilities/activity/#debrief-connecting-the-lab-to-product-management","title":"Debrief: Connecting the Lab to Product Management","text":"<p>Congratulations, you've just prototyped a sophisticated AI-powered workflow! Let's reflect on what this simple exercise demonstrates about building real-world AI products:</p> <ul> <li>Chaining is Key: The real power of AI emerges when you chain capabilities. We didn't just do one thing; we created a pipeline where the output of an analysis task became the input for a generation task.</li> <li>Structured Data is the Glue: The JSON object was the critical \"glue\" between our two AI agents. It was the predictable, machine-readable contract that allowed the \"Analyst\" to hand off its work to the \"Copywriter\" reliably. Without this step, the workflow would be fragile and unpredictable.</li> <li>You are the System Designer: As a product manager, your job is to design these workflows. You decide what the steps are, what information needs to be passed between them, and what the final output should achieve. Prompt engineering isn't just about writing a single instruction; it's about architecting these multi-step systems.</li> </ul>"},{"location":"prompts/capabilities/function-code/","title":"Specialized Text Capabilities II","text":"<p>Interacting with External Systems</p> <p>So far, we have treated the Language Model as a self-contained expert on reasoning and data structuring. We've given it information and asked it to process that information in sophisticated ways. But what if the information it needs doesn't exist in the prompt? What if it needs to access real-time data, interact with a proprietary database, or take action in another software system?</p> <p>This is the next frontier of AI application development: building systems that can move beyond their static training data and interact with the live, dynamic world. In this section, we will explore two key capabilities that make this possible: Function Calling and Code Generation. These tools are what allow us to transform a knowledgeable conversationalist into an active agent that can do things on our behalf.</p>"},{"location":"prompts/capabilities/function-code/#function-calling-ai-agents","title":"Function Calling &amp; AI Agents","text":"<p>Function calling is one of the most significant recent advancements in LLM technology. It is the mechanism that allows a model to reliably connect to and utilize external tools and APIs. It is the foundational building block for creating true AI \"agents.\"</p>"},{"location":"prompts/capabilities/function-code/#what-is-function-calling","title":"What is Function Calling?","text":"<p>Contrary to what the name might suggest, the LLM does not actually execute the function or code itself. Instead, function calling is the model's ability to determine when an external tool is needed and to generate the precise, structured JSON object required for your application to execute that tool.</p> <p>Think of the LLM as a brilliant, multilingual dispatcher. It can't drive the fire truck itself, but it can understand an emergency call in plain English, identify the correct fire station to contact, and send them a perfectly formatted dispatch ticket with the address and incident details. Your application is the fire station that receives the ticket and takes the action.</p>"},{"location":"prompts/capabilities/function-code/#the-react-framework-reason-act","title":"The ReAct Framework (Reason + Act)","text":"<p>This capability is the foundation of a powerful paradigm for building agents known as ReAct. The idea is simple but profound: the model can cycle between Reasoning (thinking about what to do next, like we saw with Chain-of-Thought) and Acting (generating the JSON to call a tool).</p> <p>For example, to answer \"What was the top-selling product in our European division last quarter, and can you summarize the customer feedback for it?\", a ReAct agent would:</p> <ol> <li>Reason: \"The user is asking for sales data and customer feedback. I need to find the top-selling product first. I have a tool called <code>get_sales_data</code>.\"</li> <li>Act: Generate JSON to call the <code>get_sales_data(division=\"Europe\", quarter=\"Q3\")</code> function.</li> <li>(Your application executes the function and feeds the result back to the model).</li> <li>Reason: \"The top product was the 'Pro-X1 Blender'. Now I need the feedback. I have a tool called <code>get_customer_feedback</code>.\"</li> <li>Act: Generate JSON to call the <code>get_customer_feedback(product_name=\"Pro-X1 Blender\")</code>.</li> <li>(Your application executes the function and feeds the result back).</li> <li>Reason: \"I now have all the information. I can synthesize it and answer the user's question.\"</li> </ol>"},{"location":"prompts/capabilities/function-code/#the-pm-perspective-building-ai-that-can-do-things","title":"\ud83d\udc69\u200d\ud83d\udcbc The PM Perspective: Building AI That Can Do Things","text":"<p>Function calling is the technology that allows you to move from building \"know-it-all\" chatbots to building \"do-it-all\" AI agents. This is a monumental shift in product strategy.</p> <ol> <li>Breaking Free from Static Knowledge: An LLM's training data is static; the world is not. Function calling connects your application to live, real-time information. Your AI can now check live inventory, get the current weather, query a user's order status, or find the latest stock price.</li> <li>Enabling True Automation: This is how you build features that take real action in the world. An AI assistant that can't just tell you how to book a flight, but can actually find available flights with a <code>search_flights</code> tool and book them with a <code>create_booking</code> tool, is a step-change in value.</li> <li>Securely Accessing Proprietary Data: Your company's most valuable data (customer records, sales figures, internal documents) is not in the LLM's training set. By creating functions that act as a secure bridge to your internal databases, you can empower the model to answer questions using this private, proprietary data without ever exposing the data itself.</li> </ol> <p>Agents in Action</p> <p>The AI as a Universal Translator for Machines: Think about your smart home. Your lights might be from one brand, your speakers from another, and your thermostat from a third. Each has its own app and its own commands. Function calling allows an AI assistant to act as a universal interface. A single user command\u2014\"Hey, set the house for a relaxing movie night\"\u2014can be translated by the model into a series of distinct function calls: one JSON object to set the lights to a dim blue, another to play a specific \"chill\" playlist on the speakers, and a third to adjust the thermostat. The AI becomes the central hub that can \"speak the language\" of every connected system.</p> <p>Architectural and Security Benefits: Function calling also enables a more secure and flexible application architecture. Instead of a central server that needs to store all the sensitive credentials for every API it might call, the responsibility is distributed. Think of it like a secure valet key. The model doesn't get the master key to your entire system. It is only given a description of the tools. When it needs to perform an action, it asks your application to use the key for a specific, pre-approved purpose. This is a more resilient and secure design.</p> <p>Enabling True Automation: This is how you build features that take real action. An AI assistant that can't just tell you how to book a flight, but can actually find available flights with a <code>search_flights</code> tool and book them with a <code>create_booking</code> tool, is a step-change in value.</p>"},{"location":"prompts/capabilities/function-code/#how-it-works-providing-tools-to-the-model","title":"How It Works: Providing Tools to the Model","text":"<p>The magic of function calling lies in the prompt. As a developer or PM, you provide the model with a list of the \"tools\" it has available as part of the initial context or system prompt.</p> <p>For each tool, you provide a clear description:</p> <ul> <li>The name of the function (e.g., <code>get_weather</code>).</li> <li>A description of what it does (\"Returns the current weather for a given location\").</li> <li>The parameters it accepts, including their type and a description (e.g., <code>location</code>: a string describing the city).</li> </ul> <p>When a user then asks a question like, \"What's the weather like in New York?\", the model sees that the user's intent matches the description of the <code>get_weather</code> tool. It then generates the required JSON object, which your application code is listening for.</p> <p>Example: The Dispatch Ticket</p> <ul> <li>User Prompt: \"What's the weather like in New York?\"</li> <li>Model's Output (The Dispatch Ticket): <pre><code>{\n  \"functionName\": \"get_weather\",\n  \"parameters\": {\n    \"location\": \"New York\"\n  }\n}\n</code></pre></li> </ul> <p>Your application receives this JSON, calls its internal weather API with the parameter \"New York,\" gets the result (\"80 degrees and sunny\"), and then feeds that result back to the model to formulate the final, natural-language answer for the user.</p>"},{"location":"prompts/capabilities/function-code/#code-generation","title":"Code Generation","text":"<p>Closely related to the model's ability to understand structured data and functions is its powerful capability to understand, write, and debug code. Having been fine-tuned on billions of lines of public code from sources like GitHub, LLMs have become an indispensable tool for software development.</p> <p>The LLM as a Programming Assistant</p> <p>A modern LLM can act as a \"pair programmer\"\u2014an assistant that can accelerate the work of human developers. It can:</p> <ul> <li>Generate Boilerplate Code: Write the standard, repetitive code for setting up a server, connecting to a database, or creating a UI component.</li> <li>Translate Languages: Convert a function from Python to JavaScript.</li> <li>Explain Complex Code: Take a dense, confusing block of code and explain what it does in plain English.</li> <li>Debug Errors: Analyze an error message and suggest a possible fix for the code that caused it.</li> </ul>"},{"location":"prompts/capabilities/function-code/#the-pm-perspective-accelerating-development-and-prototyping","title":"\ud83d\udc69\u200d\ud83d\udcbc The PM Perspective: Accelerating Development and Prototyping","text":"<ol> <li> <p>Accelerating Prototyping and Development: This is the most immediate benefit. You can go from an idea to a functional, interactive prototype faster than ever before, allowing you to test ideas and get user feedback at a blistering pace.</p> </li> <li> <p>Elevating Development Practices: The true impact of code generation lies in its ability to enhance and popularize advanced software development methodologies.</p> <ul> <li>AI Pair Programming: A human developer can work alongside an AI, delegating tasks like writing routine functions or generating test cases. This frees up the human to focus on higher-level system architecture and complex problem-solving.</li> <li>Test-Driven Development (TDD): A developer can write a test that describes a feature's desired behavior and then prompt the AI to \"write the code that makes this test pass.\" This enforces a quality-first approach to development.</li> </ul> </li> <li> <p>Bridging the Communication Gap: You can use the LLM to translate your product requirements into starter code, giving you a better way to communicate with your engineering team. Instead of just a written spec, you can provide a spec and a simple code prototype that demonstrates the core logic.</p> </li> </ol>"},{"location":"prompts/capabilities/function-code/#prompting-for-code-specificity-is-everything","title":"Prompting for Code: Specificity is Everything","text":"<p>When prompting for code, the principles we've learned still apply, but specificity becomes even more critical.</p> <ul> <li>Be Language-Specific: Always state the programming language and any key libraries or frameworks. Instead of \"Write code to make a chart,\" say, \"Write a Python script using the Matplotlib library to create a bar chart.\"</li> <li>Provide Context: Give the model the existing code it needs to work with. \"Here is my existing Python function. Add error handling to it.\"</li> <li>Describe Logic Clearly: Explain the desired logic step-by-step, just as you would with Chain-of-Thought. \"The function should first check if the user is logged in. If they are, it should fetch their data. If not, it should return an error message.\"</li> </ul> <p>By mastering these techniques, you can effectively leverage the LLM as a powerful accelerator for your product development lifecycle.</p> <p>Google Jules - The AI Agent as a Teammate</p> <p>A powerful real-world example that combines these concepts is Google Jules, an asynchronous, agentic AI coding assistant. It's a product that perfectly illustrates the convergence of agentic reasoning, function calling, and code generation.</p> <p>Think of Jules not as a real-time copilot, but as an autonomous junior developer you can delegate tasks to.</p> <ul> <li>Agentic Reasoning: A developer doesn't give Jules line-by-line instructions. They give it a high-level goal, like \"Refactor this component to be more efficient\" or \"Write unit tests for this new feature.\" Jules then independently reasons about the best way to achieve that goal and formulates a multi-step plan.</li> <li>Code Generation in Context: Jules doesn't just write code in a vacuum. It is given access to an entire codebase. This allows it to perform context engineering\u2014analyzing the existing code to learn the organization's specific coding practices, formatting styles, and quality standards. The new code it generates is therefore consistent with the project's established conventions.</li> <li>Human-in-the-Loop Workflow: Jules operates on the ReAct (Reason + Act) principle. After creating a plan and generating the code, it \"acts\" by creating a pull request\u2014a proposed change\u2014for the human team to review. This keeps the human developers in full control, allowing them to approve or modify the AI's work, seamlessly integrating the AI agent into a standard software development workflow.</li> </ul>"},{"location":"prompts/capabilities/intro/","title":"Introduction","text":"<p>From Generalist to Specialist</p> <p>In our initial sessions, we established the fundamentals of prompt engineering. We learned that a well-crafted prompt is the key to unlocking high-quality, relevant, and reliable outputs from a Generative AI model. Now, we are going to build on that foundation, moving from controlling a generalist AI to strategically deploying a team of specialists.</p>"},{"location":"prompts/capabilities/intro/#the-expert-intern-analogy-a-recap","title":"The Expert Intern Analogy: A Recap","text":"<p>Think back to our core framework for prompt design: Role, Task, Context, and Format (RTCF). We used the analogy of giving a creative brief to a brilliant, highly capable intern.</p> <ul> <li>Role: You assign the intern a persona (\"You are an expert financial analyst...\").</li> <li>Task: You give them a clear, actionable instruction (\"...write a summary of this quarterly earnings report.\").</li> <li>Context: You provide all the necessary information and constraints (\"The report is attached. Focus on revenue growth and profit margins.\").</li> <li>Format: You specify exactly what the final output should look like (\"The summary should be three bullet points.\").</li> </ul> <p>This framework is your command center for guiding the AI. It's powerful, effective, and essential for getting predictable results for a wide array of general tasks. When you master RTCF, you are mastering the art of clear and effective delegation to a general-purpose AI.</p>"},{"location":"prompts/capabilities/intro/#beyond-instruction-tuning-how-specialists-are-made","title":"Beyond Instruction Tuning: How Specialists are Made","text":"<p>The model's ability to understand and execute on your RTCF prompts so effectively isn't magic. It's the result of a crucial phase in its development process called instruction tuning. During this phase, the model is trained on a vast and diverse dataset of instructions and high-quality responses. It learns to be a helpful, general-purpose assistant\u2014our \"expert intern.\"</p> <p>However, many of the most valuable and powerful business applications require more than just a generalist. They require an expert. These expert skills are honed through a different, more focused process: Supervised Fine-Tuning (SFT).</p> <p>During SFT, the model is intensely trained on a narrower, curated dataset of thousands of high-quality examples of a specific task. For example:</p> <ul> <li>To make it a brilliant code generator, it's fine-tuned on millions of lines of code and documentation.</li> <li>To make it an expert at analyzing data, it's fine-tuned on examples of unstructured text and the corresponding structured JSON output.</li> <li>To make it a creative artist, it's fine-tuned on pairs of descriptive text and beautiful images.</li> </ul> <p>This specialized training creates new, distinct capabilities within the model. Our \"expert intern\" suddenly has a r\u00e9sum\u00e9 with specific, verifiable skills: Data Analyst, Software Developer, Graphic Designer, and more. The RTCF framework is still our foundation, but to leverage these new skills, we must learn to write prompts that specifically activate this deeper training.</p>"},{"location":"prompts/capabilities/intro/#the-pm-perspective-from-prompting-to-product-strategy","title":"\ud83d\udc69\u200d\ud83d\udcbc The PM Perspective: From Prompting to Product Strategy","text":"<p>As a product manager or business leader, why is this distinction between a generalist and a team of specialists so critical?</p> <p>Because it shifts your thinking from simply getting the AI to do things to designing products that leverage the AI's best capabilities. A product feature that relies on the model's general writing ability is useful. But a product feature that reliably extracts structured data from invoices, automatically generates code to connect to an internal API, or creates personalized marketing images from a product catalog is transformative.</p> <p>Understanding the model's specialized, fine-tuned capabilities is the key to product innovation. Your job is to:</p> <ol> <li>Know the Capabilities: Be aware of the full palette of tools the model offers beyond simple text generation (e.g., reasoning, structured output, function calling, image analysis).</li> <li>Match Capability to Problem: Identify which specific, fine-tuned skill is the right tool to solve a particular customer problem efficiently and reliably.</li> <li>Design for Reliability: Architect your product's prompts and workflows to activate these specialized skills, which are often more predictable and reliable than generalist instructions.</li> </ol> <p>In the following sections, we will dive deep into these specialized capabilities. You will learn what they are, why they matter from a product perspective, and how to craft prompts that go beyond general instructions to unlock the full potential of your AI-powered products.</p> <p>A Dynamic and Competitive Landscape</p> <p>It is crucial to understand that \"Generative AI\" is not a single, monolithic technology. The capabilities we are discussing in this module are a snapshot of a rapidly evolving and intensely competitive field. The specific tools, features, and strengths of a model are a direct reflection of the business strategy and research focus of the company that builds it.</p> <p>As a product manager, you won't just be choosing a technology; you'll be choosing a partner and an ecosystem. Understanding their strategic priorities will help you anticipate which capabilities will be the most advanced and well-supported. We can observe a few dominant strategies in the market today:</p> <ul> <li> <p>The Enterprise Collaborator: Some AI labs are intensely focused on the enterprise market. Their goal is to create a secure, reliable AI collaborator that can be deeply integrated into a company's internal workflows.</p> <ul> <li>Capabilities Prioritized: You will see these providers excel in areas like large context windows (for analyzing long reports and contracts), high accuracy (to minimize errors in critical business tasks), and robust function-calling and tool integration to connect with a company's existing software.</li> </ul> </li> <li> <p>The Ecosystem Extender: Other major players use Generative AI to enhance a massive, pre-existing ecosystem of products. Their goal is to make the products their customers already use (like office software, mobile operating systems, and video platforms) smarter and more capable.</p> <ul> <li>Capabilities Prioritized: These providers invest heavily in multimodality (image, video, and audio processing), powerful embedding models for search and recommendation, and on-device models that can run efficiently on phones and laptops.</li> </ul> </li> <li> <p>The Universal Utility: A third approach is to build a general-purpose AI assistant that is so broadly capable it becomes an indispensable utility for everyone, often delivered directly to consumers via a subscription.</p> <ul> <li>Capabilities Prioritized: This strategy drives a relentless push for cutting-edge performance on a wide range of creative and knowledge-based tasks, from writing and reasoning to generating the highest quality images and code.</li> </ul> </li> </ul> <p>Your Role as a Product Manager</p> <p>Your job is not to bet on a single company but to understand these underlying strategies. By following the research publications, developer conference keynotes, and major product releases from the leading AI labs, you can make informed decisions about which platform's capabilities best align with your product's vision and your customers' needs.</p>"},{"location":"prompts/capabilities/multimodality/","title":"Multimodal Capabilities","text":"<p>Welcome to one of the most visually impressive and creatively disruptive areas of modern AI: multimodality. A multimodal model is one that can understand and generate content in more than one format, or \"modality.\" It can process not just text, but also images, audio, and video.</p> <p>This capability is what allows an AI to \"see\" the world through a photo, \"listen\" to a query, and \"create\" a novel image from a simple description. To understand how a model trained on language can achieve this, we first need to grasp a core concept: embeddings.</p> <p>Understanding Embeddings</p> <p>You can't feed pixels or soundwaves directly into a language model. The model's \"brain\" is built for mathematics, not for sight or sound. So, to work with different types of media, the model first needs to translate everything into a common, mathematical language. This universal language is made of embeddings.</p> <p>What is an Embedding? A Universal Language for Data</p> <p>An embedding is a numerical representation of a piece of content. It's a list of numbers (a vector) that captures the semantic essence or meaning of that content. Think of it like a highly detailed set of coordinates on a massive \"map of meaning.\"</p> <ul> <li>The word \"dog\" has an embedding.</li> <li>A picture of a golden retriever also has an embedding.</li> <li>The sound of a dog barking has an embedding.</li> </ul> <p>Crucially, on this map, concepts that are semantically similar are located close to each other. The embedding for the word \"dog\" will be mathematically close to the embedding for the word \"puppy\" and also close to the embedding for the picture of a golden retriever. This is the key that unlocks multimodality.</p> <p>How Encoding and Decoding Works</p> <p>The model uses two key components to work with embeddings:</p> <ol> <li>The Encoder: This is the part of the model that acts as a universal translator. It takes an input\u2014text, image, audio\u2014and \"encodes\" it into its numerical embedding. It reads the content and finds its precise coordinates on the map of meaning.</li> <li>The Decoder: This component does the reverse. It takes an embedding (a set of coordinates) and \"decodes\" it back into content. A text decoder will look at the coordinates and generate the most likely words associated with that meaning. An image decoder will look at the same coordinates and generate the pixels most likely to form a picture of that meaning.</li> </ol> <p>This two-step process is how a text prompt can become a picture. Your prompt is first encoded into an embedding, and then an image decoder uses that same embedding as its instruction set for generating pixels.</p>"},{"location":"prompts/capabilities/multimodality/#image-video-understanding","title":"Image &amp; Video Understanding","text":"<p>This is the model's ability to \"see\"\u2014to take visual media as an input and analyze its content. It's the practical application of the encoder. The model converts your uploaded image or video into an embedding and then uses its understanding of that embedding to answer questions about it.</p>"},{"location":"prompts/capabilities/multimodality/#the-pm-perspective-unlocking-insights-from-unstructured-visual-data","title":"\ud83d\udc69\u200d\ud83d\udcbc The PM Perspective: Unlocking Insights from Unstructured Visual Data","text":"<p>For decades, businesses have collected vast amounts of visual data (photos, videos, scans) that have been difficult to analyze at scale. This capability changes that entirely.</p> <ol> <li>Automating Manual Processes: Think of the human effort spent on tasks like content moderation (flagging inappropriate images), inventory management (counting products on a shelf), or insurance claims processing (assessing photos of damage). These visual analysis tasks can now be automated, freeing up human capital for higher-value work.</li> <li>Unlocking New Data Sources: Customers are constantly generating visual data on social media. A model can analyze images of people using your product \"in the wild,\" giving you unprecedented insight into how, where, and by whom your products are being used.</li> <li>Creating Smart Experiences: You can build features where the user's camera is the primary input. A retail app could allow a user to take a picture of a piece of furniture they like, and the AI could identify similar items in your store's catalog.</li> </ol>"},{"location":"prompts/capabilities/multimodality/#prompting-for-analysis-from-description-to-extraction","title":"Prompting for Analysis: From Description to Extraction","text":"<p>When prompting for visual analysis, you are essentially asking the model to describe what it \"sees\" in the embedding. Your prompts should be direct and specific.</p> <ul> <li>For general description: \"Describe this image in detail.\" \"What is the main subject of this video?\"</li> <li>For counting and identification: \"How many cars are in this photo? Identify the color of each one.\"</li> <li>For text extraction (OCR): \"Extract the text from the receipt in this image and return it as JSON.\"</li> <li>For abstract analysis: \"What is the likely mood or sentiment of this image? Is it happy, sad, professional, chaotic?\"</li> </ul>"},{"location":"prompts/capabilities/multimodality/#image-video-generation","title":"Image &amp; Video Generation","text":"<p>This is the most creatively exciting multimodal capability: creating brand-new, original visual media. This is the work of the decoder, which translates the embedding of your prompt and any source images into a corresponding set of pixels.</p>"},{"location":"prompts/capabilities/multimodality/#the-pm-perspective-a-paradigm-shift-for-content-creativity","title":"\ud83d\udc69\u200d\ud83d\udcbc The PM Perspective: A Paradigm Shift for Content &amp; Creativity","text":"<p>The ability to generate high-quality visual content is changing the economics of marketing, advertising, and design.</p> <ol> <li>Hyper-Personalization at Scale: Imagine generating a unique ad creative for every customer segment, or even for every individual user.</li> <li>Radical Speed in Prototyping: Visualize new product concepts, store layouts, or character designs in seconds, dramatically accelerating the creative and product development lifecycle.</li> <li>Democratizing Creativity: Small businesses and individual creators now have access to visual content creation tools that were previously only available to large firms with massive budgets.</li> </ol> <p>To leverage this power, a product manager must understand that not all visual generation tasks are the same. Your prompt design must adapt to the specific use case you are trying to solve. Let's explore the three primary scenarios.</p>"},{"location":"prompts/capabilities/multimodality/#prompting-strategies-for-visual-generation","title":"Prompting Strategies for Visual Generation","text":"<p>Use Case 1: Text-to-Image (Creating from Scratch)</p> <p>This is the most common use case: generating a novel image purely from a text description. Here, your prompt is the entirety of the creative brief.</p> <ul> <li>Prompt Strategy: The Detailed Creative Brief     Your goal is to describe the desired final state of the image in exhaustive detail. A reliable method is to follow a structured template that covers the core elements of a visual scene: </li> </ul> <p>Example Text-to-Image Prompt Template</p> <p>A [<code>medium</code>] of a [<code>subject</code>], [<code>style</code>], [<code>color</code>/<code>lighting</code>], [<code>composition</code>].</p> <p>Medium: Start here to anchor the style. Is it a \"photograph,\" a \"watercolor painting,\" a \"3D render,\" or \"a charcoal sketch\"?</p> <p>Subject: Be highly descriptive. Not just \"a car,\" but \"a vintage 1960s convertible sports car.\"</p> <p>Style &amp; Details: Add artistic and textural modifiers. \"Hyperrealistic,\" \"in the style of Van Gogh,\" \"steampunk-inspired,\" \"made of brass and copper gears.\"</p> <p>Color &amp; Lighting: Describe the mood. \"Vibrant, saturated colors,\" \"dark, moody background,\" \"cinematic lighting.\"</p> <p>Composition: Explain the framing. \"Close-up portrait,\" \"wide-angle landscape shot,\" \"from a low angle.\"</p> <p>Use Case 2: Text + Image(s) (Editing, Combining, and Transforming)</p> <p>Here, you provide one or more source images along with your text prompt. The model now has both textual and visual context. Your prompt's job is not to describe a whole scene, but to describe the relationship, modification, or transformation you want to perform.</p> <p>Prompt Strategy: The Direct Command for Transformation</p> <p>Your language should be direct, clear, and focused on the change you want to see.</p> <p>For Animating a Single Image (Image-to-Video)</p> <p>The prompt must describe motion. Use strong, action-oriented verbs. The best prompts are concise and focused on the desired action.</p> <ul> <li>Example: For a photo of a mountain lake, a prompt like \"Make the water shimmer and the clouds drift slowly to the right\" is ideal. The prompt describes the change to the existing context (the image).</li> </ul> <p>For Combining Multiple Images (Style Transfer, Blending)</p> <p>The prompt must describe the manner of combination. You specify the role of each image.</p> <ul> <li>Example: With an image of a person (Image A) and an image of a Van Gogh painting (Image B), the prompt would be \"Apply the style of Image B to Image A.\"</li> </ul> <p>For Editing an Image (Inpainting/Outpainting)</p> <p>You provide an image and a mask (a selected area), and the prompt describes what to do within that area.</p> <ul> <li>Example: With an image of a person holding an apple, you could mask the apple and provide the prompt \"Change the apple to an orange.\"</li> </ul> <p>Use Case 3: Image-Only (Visual Similarity)</p> <p>In some cases, you may not even need a text description of image in a prompt. You can provide an image as the sole input to a model and ask it to find or generate visually similar images. This is a common feature in e-commerce (\"Find more shirts like this one\"). While not a \"prompting\" technique in the textual sense, it's a core multimodal capability that relies on the model's understanding of embeddings to find images that are \"close\" to the source image on the map of meaning.</p> <p>By understanding these different use cases, you can tailor your prompt strategy to the specific task at hand, moving fluidly between the roles of a detailed creative director, a direct editor, and a visual curator.</p> <p>See Google's blog posts linked below for more examples:</p> <ul> <li> <p>10 examples of our new native image editing in the Gemini app</p> </li> <li> <p>3 ways to use photo-to-video in Gemini</p> </li> </ul>"},{"location":"prompts/capabilities/multimodality/#the-pms-choice-integrated-vs-specialist-models","title":"The PM's Choice: Integrated vs. Specialist Models","text":"<p>As we've seen, models like Gemini can generate images directly as part of their multimodal capabilities. However, you will also encounter powerful, standalone models designed exclusively for image generation, such as Google's Imagen, OpenAI's DALL-E 3, or Midjourney.</p> <p>This creates a critical strategic choice: when should your product use the convenient, built-in capabilities of a multimodal model versus calling a separate, specialized service?</p> <p>Think of it like the camera on your smartphone versus a professional DSLR camera.</p> <ul> <li>Integrated Models (The Smartphone Camera): The image generation capability built into Gemini is incredibly convenient, fast, and surprisingly powerful. It's always with you, seamlessly integrated into the tool you're already using, and perfect for a huge range of tasks where \"good enough\" is great.</li> <li>Specialist Models (The Professional DSLR): A dedicated model like Imagen is a specialist tool. It does one thing\u2014image generation\u2014and aims to do it at the absolute state-of-the-art level. It offers the highest possible quality, the most granular control over style and composition, and the most advanced features.</li> </ul> <p>Is This Distinction Common?</p> <p>Yes, this is a dominant pattern across the industry.</p> <ul> <li>OpenAI has its GPT family of models (e.g., GPT-4o) which have native, integrated image generation. This capability is powered by their dedicated, state-of-the-art image model, DALL-E 3. While the experience is seamless in a product like ChatGPT, they are distinct models working in concert.</li> <li>Anthropic's Claude models, as of now, are world-class at understanding images but do not have native image generation capabilities. To create a visual with Claude, your application must use function calling to connect to an external, specialist service like Stability AI or Midjourney.</li> </ul> <p>The Product Manager's Decision Framework</p> <p>Your job is to decide which tool is right for the job. Here\u2019s a framework to guide your thinking:</p> Use the Integrated Model (e.g., Gemini) when... Use a Specialist Model (e.g., Imagen) when... Speed and Simplicity are paramount. (e.g., rapid prototyping, simple blog illustrations) Image Quality is the top priority. (e.g., hero images for a marketing campaign, product design) The task is part of a larger, seamless workflow. (e.g., \"Summarize this text, then make an image for it.\") You need fine-grained, professional control. (e.g., specific art direction, style consistency) \"Good enough\" quality is sufficient for the user experience. (e.g., generating icons or simple diagrams) You need the absolute latest, state-of-the-art features. (e.g., realistic text in images) You want to minimize architectural complexity and API calls. You want the flexibility to swap out the best-in-class provider for that specific task. <p>Ultimately, there is no single right answer. The choice depends entirely on your product's specific needs, your users' expectations for quality, and your technical and budget constraints. A skilled product manager knows both types of tools are in their toolkit and understands when to reach for the convenient smartphone and when to set up the professional DSLR.</p>"},{"location":"prompts/capabilities/next-steps/","title":"Conclusion & Next Steps","text":""},{"location":"prompts/capabilities/next-steps/#conclusion-next-steps","title":"Conclusion &amp; Next Steps","text":"<p>We've covered a significant amount of ground in this module, moving from the basic anatomy of a prompt to the sophisticated techniques required to build real-world AI applications. This journey has taken us from being simple users of AI to strategic designers of AI-powered systems.</p> <p>This final section synthesizes the most important concepts and provides a set of actionable insights that you can carry with you into your career as a product manager or business leader.</p>"},{"location":"prompts/capabilities/next-steps/#key-concepts-recap","title":"Key Concepts Recap","text":"<p>From Generalist to Specialist: We learned that while a model's general abilities come from instruction tuning, its most powerful, specialized skills (like code generation or structured output) are the result of deliberate Supervised Fine-Tuning (SFT). Your job is to know what these specialized skills are and how to activate them.</p> <p>Reasoning and the Thinking Budget: Chain-of-Thought (CoT) prompting turns the AI from a black box into a transparent \"glass box,\" improving accuracy and building trust. However, this comes at a higher token cost, creating the concept of a \"Thinking Budget\" that you, the PM, must strategically allocate.</p> <p>Structured Output as the Glue: The ability to generate reliable JSON is the critical bridge between the LLM and your application's code. It is the language of AI orchestration, enabling complex, multi-step workflows where one model's output reliably becomes another component's input.</p> <p>AI Agents and Function Calling: Function calling is the mechanism that allows an AI to interact with the real world. It transforms the model from a \"know-it-all\" to a \"do-it-all\" agent by allowing it to access live data and trigger actions in external systems, enabling true automation.</p> <p>Multimodality and Embeddings: Models can \"see\" and \"create\" by translating all forms of content\u2014text, pixels, and audio\u2014into a universal mathematical language called embeddings. Understanding this core concept is key to designing features that leverage visual understanding and generation.</p> <p>Prompting as System Design: Our hands-on lab demonstrated that building an AI feature is an act of system design. You must chain different capabilities together, using structured data as the reliable \"glue\" between the steps, to create a valuable end-to-end workflow.</p>"},{"location":"prompts/capabilities/next-steps/#actionable-insights-for-product-managers","title":"Actionable Insights for Product Managers \ud83d\ude80","text":"<ol> <li> <p>Your Job is to Know the Full Toolkit.     Don't just think of the LLM as a text generator. Think of it as a suite of specialized services. Your primary responsibility is to know the full palette of capabilities\u2014reasoning, data structuring, function calling, visual analysis\u2014so you can match the right tool to the right customer problem.</p> </li> <li> <p>Design for Reliability, Not Just Creativity.     For a product to be successful, it must be reliable. Prioritize getting structured data (JSON) outputs from the model whenever possible. This is the single most important technique for building predictable, scalable, and debuggable AI systems.</p> </li> <li> <p>Always Balance Capability with Cost.     Every advanced feature has a cost, primarily in tokens. Be deliberate in your use of expensive techniques like Chain-of-Thought. Ask yourself: \"For this specific feature, is the value of enhanced accuracy and transparency worth the increased operational cost?\" That is the \"Thinking Budget\" trade-off.</p> </li> <li> <p>Think in Workflows, Not Single Prompts.     The most transformative AI products are not built on a single, magical prompt. They are built on well-designed workflows that chain multiple, simpler prompts together. Map out your user's journey and identify the discrete steps the AI needs to take. Design a prompt for each step and decide how data will be passed between them.</p> </li> <li> <p>Prototype, Iterate, and Test Relentlessly.     The tools we used in the lab, like Google AI Studio, are your new best friends. The cost of prototyping an AI feature has dropped to near zero. Use this to your advantage. Build simple proofs-of-concept, test your prompt strategies, and iterate on your designs constantly. Never assume your first prompt is the best one.</p> </li> </ol>"},{"location":"prompts/capabilities/next-steps/#references-further-reading","title":"\ud83d\udcda References &amp; Further Reading","text":"<p>For continued learning, these resources provide deeper technical detail and practical examples of the concepts covered in this module.</p> <ul> <li>Gemini API Documentation: The official source for exploring each of these capabilities in detail.<ul> <li>Chain-of-Thought Reasoning</li> <li>Structured Output Generation</li> <li>Function Calling</li> <li>Image and Video Understanding</li> <li>Image Generation</li> </ul> </li> <li>Gemini Cookbook on GitHub: A collection of Python notebooks with code examples for implementing these techniques, perfect for opening in Google Colab to run and modify yourself.<ul> <li>Link: Gemini Prompting Examples Cookbook</li> </ul> </li> </ul>"},{"location":"prompts/capabilities/reasoning-structured-output/","title":"Specialized Text Capabilities I","text":"<p>Reasoning and Structured Data</p> <p>Welcome to our first deep dive into the specialized, fine-tuned capabilities of Large Language Models. In this section, we'll explore two of the most powerful and commercially valuable text-based skills: the ability to reason through complex problems and the ability to return data in a perfectly structured, machine-readable format.</p> <p>Mastering these two capabilities is the first major step in moving from using an LLM as a creative assistant to integrating it as a reliable and predictable component of a software application.</p>"},{"location":"prompts/capabilities/reasoning-structured-output/#thinking-reasoning-chain-of-thought","title":"Thinking &amp; Reasoning (Chain-of-Thought)","text":"<p>At its core, a Large Language Model is a sophisticated next-token prediction engine. It excels at calculating the most probable next word (or token) in a sequence. For a simple question, the most probable sequence leads directly to an answer.</p> <p>But what about complex questions? For these, the most probable path to a correct answer isn't a straight line. It requires intermediate steps. Chain-of-Thought (CoT) prompting is a technique that intentionally guides the model down a more deliberative, step-by-step path of token prediction before it generates a final answer.</p> <p>What is Chain-of-Thought Prompting?</p> <p>Chain-of-Thought is a prompting technique that coaxes the model to externalize its reasoning process. By adding a simple phrase like \"Let's think step-by-step,\" you are fundamentally altering the token prediction path. Instead of predicting the final answer directly, the model first predicts the tokens that form a logical step, then the tokens that form the next step, and so on, until it has built a logical chain that leads to a conclusion.</p> <p>This is a specialized application of next-token prediction, fine-tuned on datasets that include complex problems and their step-by-step solutions.</p> <p>How it Works: Turning a Black Box into a Glass Box</p> <p>Without CoT, a model's reasoning is hidden. If it makes a logical leap or a factual error (a \"hallucination\"), the final answer could be wrong, and you'll have no way of knowing why. This \"black box\" nature is a major risk for business-critical applications.</p> <p>CoT transforms the black box into a glass box. By seeing the step-by-step reasoning, you gain three critical advantages:</p> <ol> <li>Interpretability: You can see exactly how the model arrived at its conclusion. This is the foundation of building trustworthy and transparent AI systems.</li> <li>Mitigating Hallucination: Hallucinations are often unsupported by logic. By forcing the model to show its work, you make it much harder for it to invent a fact, as it would then have to invent a plausible (but likely flawed) logical path to support it. The need for justification acts as a powerful constraint.</li> <li>Debugging: When the model does produce a wrong answer, the reasoning chain provides an immediate diagnostic trail. You can pinpoint the exact step where the logic failed, allowing you to iterate on your prompt with surgical precision.</li> </ol>"},{"location":"prompts/capabilities/reasoning-structured-output/#the-pm-perspective-the-thinking-budget-and-balancing-cost-vs-capability","title":"\ud83d\udc69\u200d\ud83d\udcbc The PM Perspective: The 'Thinking Budget' and Balancing Cost vs. Capability","text":"<p>Given these powerful benefits, a natural question arises: \"Why don't we enable reasoning for all applications?\"</p> <p>The answer comes down to a core product management discipline: managing resources. Using Chain-of-Thought is not free. Most commercial LLM vendors operate on a consumption-based pricing model, billing based on the number of tokens processed. This typically includes: * Input Tokens: The length of your prompt. * Output Tokens: The length of the model's response.</p> <p>Chain-of-Thought, by its very nature, generates a significantly longer, more verbose response. More steps in the reasoning process mean more tokens, and more tokens mean a higher cost per API call.</p> <p>This creates the concept of a \"Thinking Budget.\" As a product manager, you must decide when to \"spend\" your budget on reasoning.</p> <ul> <li>For straightforward \"understanding\" tasks\u2014like summarizing a document, classifying customer feedback, or extracting a name from an email\u2014the risk of error is low. The model can likely predict the correct answer directly. Spending tokens on a verbose reasoning chain is an unnecessary cost.</li> <li>For complex tasks\u2014like calculating a multi-variable financial model, diagnosing a technical problem from logs, or planning a complex project timeline\u2014the risk of a hidden error is high. Here, the extra cost of CoT is a wise investment. It buys you accuracy, trustworthiness, and debuggability, justifying the expense.</li> </ul> <p>Your job is to strategically allocate your thinking budget, enabling CoT for features where the value of transparency and accuracy outweighs the increased token cost.</p>"},{"location":"prompts/capabilities/reasoning-structured-output/#prompting-techniques-and-examples","title":"Prompting Techniques and Examples","text":"<p>Activating Chain-of-Thought is simple. The most common technique is zero-shot CoT, where you simply add a trigger phrase to the end of your prompt.</p> <p>Example: Multi-step Calculation</p> <ul> <li>Standard Prompt (Lower Cost, Higher Risk):</li> </ul> <p>A customer's monthly subscription is $45. They are on an annual plan with a 10% discount, and they have a one-time credit of $20. What is their total bill for the first year?</p> <ul> <li>Model's Likely Answer (No way to verify):</li> </ul> <p>$518</p> <ul> <li>CoT Prompt (Higher Cost, Lower Risk):</li> </ul> <p>A customer's monthly subscription is $45. They are on an annual plan with a 10% discount, and they have a one-time credit of $20. What is their total bill for the first year? Let's think step-by-step.</p> <ul> <li>Model's CoT Answer (Transparent &amp; Verifiable):</li> </ul> <ol> <li>First, calculate the base annual cost: $45/month * 12 months = $540.</li> <li>Next, calculate the 10% discount: $540 * 0.10 = $54.</li> <li>Apply the discount to find the discounted annual cost: $540 - $54 = $486.</li> <li>Finally, apply the one-time credit: $486 - $20 = $466. The customer's total bill for the first year is $466.</li> </ol> <p>In this example, the standard prompt led to an incorrect answer. The CoT prompt not only produced the correct answer but also showed exactly how it was calculated, making it a trustworthy and verifiable output for an application.</p>"},{"location":"prompts/capabilities/reasoning-structured-output/#structured-output-json","title":"Structured Output (JSON)","text":"<p>One of the biggest challenges in software is dealing with unstructured data. Humans communicate in prose, but applications run on structured data\u2014databases, APIs, and configuration files. A raw text response from an LLM is easy for a human to read but difficult for a program to use reliably.</p> <p>The ability to compel an LLM to respond in a specific, machine-readable format like JSON (JavaScript Object Notation) is arguably the most critical capability for building scalable AI applications. It allows the LLM to function as a predictable component within a larger software system.</p> <p>The LLM as a Data Structuring Tool</p> <p>Because LLMs have been fine-tuned on vast amounts of text and code, they are exceptionally good at understanding structured formats like JSON. We can leverage this training to build a powerful bridge between human language and machine language. You can give the model unstructured text and ask it to perform a task, with the critical constraint that its output must be a perfectly formatted JSON object.</p>"},{"location":"prompts/capabilities/reasoning-structured-output/#the-pm-perspective-the-bridge-for-workflows-and-orchestration","title":"\ud83d\udc69\u200d\ud83d\udcbc The PM Perspective: The Bridge for Workflows and Orchestration","text":"<p>As a product manager, mastering structured output is non-negotiable. It is the fundamental mechanism that allows your application's backend to safely and reliably interact with the LLM's output. But its importance goes even deeper.</p> <p>Structured Output as the Intermediate Step</p> <p>While we often think about the final, user-facing response, in most sophisticated AI systems, the most critical outputs are the ones that happen in the middle. Structured output is the language of AI orchestration. It's the intermediate step that enables complex, multi-part workflows.</p> <p>Consider these scenarios:</p> <ul> <li>AI Orchestration (Model-to-Model): Imagine a user asks your AI assistant to \"plan a marketing campaign.\" The first model (a \"strategist\" AI) might generate a high-level plan. To be useful, it doesn't output prose. It outputs a JSON object defining the campaign stages. This structured output then becomes the input for a second model (a \"copywriter\" AI) tasked with writing the ad copy for each stage.</li> <li>Interactive UIs (Model-to-Component): A user in a chatbot might say, \"I'd like to book a flight.\" Instead of just replying with text, the model can generate a JSON object that your application's front-end can use to render an interactive flight-booking widget, pre-filled with the likely departure city.</li> <li>API Calls (Model-to-Tool): As we will see in the section on Function Calling, the model must produce a perfectly structured JSON object that specifies the exact API to call and the parameters to use.</li> </ul> <p>In all these cases, the structured output is not the end product; it's the critical, machine-readable message passed between components in an automated workflow.</p> <p>This makes structured output the key to moving beyond simple chatbots and building true AI-powered applications and autonomous agents.</p>"},{"location":"prompts/capabilities/reasoning-structured-output/#prompting-techniques-the-power-of-few-shot-examples","title":"Prompting Techniques: The Power of Few-Shot Examples","text":"<p>While you can simply ask the model to \"respond in JSON,\" the most robust and production-ready technique is few-shot prompting. This involves providing 2-3 complete examples of the task in your prompt before giving the model the new input to process.</p> <p>This shows the model exactly what you want. It sees the input, the desired output, and the precise JSON schema you require. When it then receives the new input, it has a crystal-clear template to follow, dramatically increasing the reliability of its response.</p> <p>Example: Extracting Entities for a Downstream Process</p> <ul> <li>Zero-Shot Prompt (Less Reliable):</li> </ul> <p>Extract the book title and author from the following sentence and format it as JSON: \"I just finished reading The Hobbit by J.R.R. Tolkien.\"</p> <ul> <li>Few-Shot Prompt (More Reliable):</li> </ul> <p>Your task is to extract the book title and author from a sentence and return a JSON object. This output will be used by another system to query a library database.</p> <p>Sentence: \"Dune by Frank Herbert is a sci-fi classic.\"</p> <p>JSON: <code>{\"title\": \"Dune\", \"author\": \"Frank Herbert\"}</code></p> <p>Sentence: \"I highly recommend Project Hail Mary, written by Andy Weir.\"</p> <p>JSON: <code>{\"title\": \"Project Hail Mary\", \"author\": \"Andy Weir\"}</code></p> <p>Sentence: \"I just finished reading The Hobbit by J.R.R. Tolkien.\"</p> <p>JSON:</p> <p>By providing high-quality examples, you leave no room for ambiguity. The model learns the pattern and provides a clean, valid JSON object: <code>{\"title\": \"The Hobbit\", \"author\": \"J.R.R. Tolkien\"}</code> that your application can use instantly for the next step in its workflow.</p>"},{"location":"prompts/safety-ethics/","title":"AI Safety &amp; Ethics in Prompting","text":""},{"location":"prompts/techniques/","title":"Prompting Techniques &amp; Strategies","text":"<ul> <li> Introduction to Prompting Techniques</li> <li> The Essential Foundations </li> <li> Core Prompting Techniques </li> <li> Workflow Tips </li> <li> Next Steps </li> </ul>"},{"location":"prompts/techniques/essentials/","title":"The Essential Foundations","text":"<p>In the previous section, we established a mindset for tackling complex problems. Now, we'll get practical. Building a single, effective prompt is a good start, but building a real-world AI product requires making those prompts scalable, maintainable, and ready to be integrated into an application.</p> <p>This lesson covers two key pillars of operationalizing prompts. First, we'll learn how to create reusable blueprints for our prompts using templates. Second, we'll explore the technical environments where these templates are deployed, focusing on the crucial differences between Text Completion and Chat API models.</p>"},{"location":"prompts/techniques/essentials/#prompt-templates-scalability","title":"Prompt Templates &amp; Scalability","text":"<p>Moving Beyond the Playground \ud83d\ude80</p> <p>Tools like Google AI Studio are fantastic \"playgrounds\" for rapid prototyping. However, when you move from experimenting to building a real software product, your approach needs to mature.</p> <p>In a real application, you can't just have prompts scattered around in your code. This is inefficient, hard to maintain, and impossible to scale. The professional approach is to use prompt templates, which are reusable, pre-defined text structures with placeholders for dynamic information. Using variables in prompts allows you to make them more dynamic.</p>"},{"location":"prompts/techniques/essentials/#a-practical-example-from-static-prompt-to-template","title":"A Practical Example: From Static Prompt to Template","text":"<p>Let's see this in action. Imagine you're building a feature for a travel app that gives users a fun fact about a city they search for.</p> <p>The Static (Hardcoded) Prompt</p> <p>Your first attempt in the playground might look like this:</p> <pre><code>You are a travel guide. Tell me a fact about the city: Amsterdam\n</code></pre> <p>This works for a single case, but it's not reusable.</p> <p>The Template-Based Prompt</p> <p>A much better approach is to create a template with a variable.</p> <pre><code>You are a travel guide. Tell me a fact about the city: {city}\n</code></pre> <p>Now, you have a single, reusable blueprint. Your application's code can dynamically insert any city name into the <code>{city}</code> variable before sending the completed prompt to the LLM. Here\u2019s how simple it is to use this template in Python:</p> <pre><code>def get_city_fact_prompt(city_name):\n  \"\"\"\n  This function populates a prompt template with a specific city name.\n  \"\"\"\n  prompt_template = \"You are a travel guide. Tell me a fact about the city: {city}\"\n\n  # The .format() method replaces the {city} placeholder with the city_name variable\n  final_prompt = prompt_template.format(city=city_name)\n\n  # This final_prompt is now ready to be sent to an LLM API\n  return final_prompt\n\n# Example usage:\nprompt_for_tokyo = get_city_fact_prompt(\"Tokyo\")\nprint(prompt_for_tokyo)\n\n# Output:\n# You are a travel guide. Tell me a fact about the city: Tokyo\n</code></pre> <p>Templates</p> <p>As a Product Manager, insisting on the use of prompt templates is critical for building a scalable AI product. It's a strategic practice with several key benefits:</p> <ul> <li>Separation of Concerns: Templates allow your team to separate the prompt logic (the instructions) from the data (the user-specific information). This means your prompt engineers can refine the prompt's wording without changing the application's core code.</li> <li>Easier A/B Testing: Want to see if a more \"witty\" persona improves engagement? With templates, you can easily test different versions of a prompt against each other to find the most effective one.</li> <li>Maintainability: If you need to update a prompt that's used in 10 different places, templates let you do it in one central location, saving time and reducing errors.</li> </ul>"},{"location":"prompts/techniques/essentials/#completions-conversations","title":"Completions &amp; Conversations \ud83d\udcac","text":"<p>How you use a template depends on the type of interaction model you're working with. The two most common are Text Completion and Chat.</p>"},{"location":"prompts/techniques/essentials/#simple-text-completion-vs-multi-round-chat","title":"Simple Text Completion vs. Multi-Round Chat","text":"<ul> <li>Text Completion Model: Think of this as the world's most powerful auto-complete. You provide a single block of text (your prompt), and the model's only job is to predict the text that should follow. It is stateless and has no memory of past interactions.</li> <li>Chat Model: This model is designed for back-and-forth dialogue. It is stateful, meaning it is designed to remember the history of the conversation and use that context to inform its next response. This powers applications like chatbots and virtual assistants.</li> </ul>"},{"location":"prompts/techniques/essentials/#the-role-of-system-instructions","title":"The Role of System Instructions","text":"<p>A key feature, particularly of chat models, is the System Instruction. This is a high-level instruction that sets the context, persona, and rules for interactions. It acts as a mission briefing for your AI.</p> <p>System instructions can operate at two levels, providing a powerful way to govern model behavior:</p> <ul> <li>Application-Level: You can set a global, default instruction that defines the core personality and safety constraints for your entire service. This \"base instruction\" is automatically applied to every new conversation, ensuring a consistent brand voice.</li> <li>Session-Level: For a specific conversation, you can add a more specific instruction that augments or temporarily overrides the global one to handle a particular task.</li> </ul> <p>Example Application-Level System Instruction:</p> <p>System Instruction for a Support Bot</p> <p>\"You are 'Tango', a helpful support bot for a software company. Your tone must always be patient, friendly, and professional. Never apologize for the product's behavior. Instead, provide clear, step-by-step instructions to help the user.\"</p>"},{"location":"prompts/techniques/essentials/#example-the-partial-input-strategy-in-different-apis","title":"Example: The \"Partial Input\" Strategy in Different APIs","text":"<p>Let's use the simple partial input completion strategy to see how differently it's implemented in a text completion vs. a chat API.</p> <p>Partial Input in a Text Completion API</p> <p>You create a single string that ends mid-thought. The model simply finishes it.</p> <pre><code># The entire prompt is one string\nprompt_string = \"\"\"\nThe three most important qualities of a great Product Manager are:\n1. Deep Customer Empathy\n2. Clear Communication Skills\n3. \"\"\"\n\n# The model's output would be something like: \"Strategic Thinking\"\n</code></pre> <p>Partial Input in a Chat API</p> <p>You can't just send an unfinished sentence. You must conversationally guide the model by manually constructing a chat history.</p> <pre><code># The prompt is a list of conversation turns\nchat_prompt = [\n    {\n        \"role\": \"user\",\n        \"parts\": [\"What are the three most important qualities of a great Product Manager? Give me just the first two for now.\"]\n    },\n    {\n        \"role\": \"assistant\",\n        \"parts\": [\"Of course. The first two are:\\n1. Deep Customer Empathy\\n2. Clear Communication Skills\"]\n    },\n    {\n        \"role\": \"user\",\n        \"parts\": [\"Perfect, now what's the third one?\"]\n    }\n]\n\n# The model, having the full context, will generate the next assistant message, \n# which would be something like: \"The third most important quality is Strategic Thinking.\"\n</code></pre>"},{"location":"prompts/techniques/intro/","title":"Introduction to Prompting Techniques","text":"<p>In the previous lesson, we dissected the Anatomy of a Prompt and tuned the Model Configuration panel. You now have a solid grasp of the fundamental building blocks of any interaction with a Large Language Model (LLM).</p> <p>But what happens when the problems get more complex?</p> <p>Imagine you're the Product Manager for a new e-commerce chatbot. A customer types, \"I bought a blue shirt last week, the one like in your summer ad, but it's too small. How do I exchange it for a large?\"</p> <p>A simple, one-line prompt won't be enough to handle this. How do you design a system of prompts that can reliably understand the user's intent, extract the key details, consult a policy, and generate a helpful, accurate response?</p> <p>This is where you move from being a prompt user to a prompt engineer.</p>"},{"location":"prompts/techniques/intro/#from-instructions-to-recipes","title":"From Instructions to Recipes \ud83e\uddd1\u200d\ud83c\udf73","text":"<p>Think of our last lesson as learning about the core ingredients in a kitchen. You know what Persona, Task, Context, and Format are, and you know how to control the \"heat\" with settings like Temperature.</p> <p>Now, it's time to learn the recipes.</p> <p>Prompting techniques are the proven recipes that combine those basic ingredients to create sophisticated and reliable outputs. Mastering them is the key to solving real-world business problems. It's important to remember that prompt engineering is an iterative process. Your first attempt will rarely be your last. The work involves crafting a prompt, analyzing the model's response, and refining the prompt until you consistently get the high-quality results your product requires.</p>"},{"location":"prompts/techniques/intro/#why-a-toolkit","title":"Why a Toolkit?","text":"<p>The Link Between Task Complexity and Technique \ud83d\udee0\ufe0f</p> <p>A common question is, \"Why do so many prompting techniques exist?\" The answer lies in the incredible versatility of modern LLMs. Models like Gemini are instruction-tuned, which means they have been specifically trained on vast amounts of data to become exceptionally good at following instructions.</p> <p>However, not all instructions are created equal. * A simple task, like \"Summarize this article,\" requires a simple, direct instruction. * A complex task, like our chatbot example, requires a more sophisticated instruction to guide the model through a multi-step reasoning process.</p> <p>The different prompting techniques you are about to learn are simply structured, expert-level ways of giving instructions. Your job as a PM and prompt engineer is to match the complexity of your task to the power of your technique.</p>"},{"location":"prompts/techniques/intro/#computational-thinking-for-prompts","title":"Computational Thinking for Prompts","text":"<p>At its core, prompt engineering is an application of computational thinking. You are translating a human-centric business problem into a structured format that a machine can understand and execute. Two key concepts will help you do this effectively: Decomposition and Pattern Recognition</p>"},{"location":"prompts/techniques/intro/#decomposition","title":"Decomposition","text":"<p>This is the practice of breaking down a large, complex problem into a series of smaller, manageable sub-tasks. For our e-commerce chatbot, you wouldn't ask the LLM to \"solve the customer's problem.\" Instead, you would decompose the task:</p> <ol> <li>Task 1 (Intent Recognition): Classify the user's request. Is it a return, an exchange, or a question?</li> <li>Task 2 (Entity Extraction): Identify and extract key pieces of information: <code>item: \"blue shirt\"</code>, <code>original_size: \"small\"</code>, <code>desired_size: \"large\"</code>.</li> <li>Task 3 (Response Generation): Given the intent and entities, generate a helpful response based on the company's exchange policy.</li> </ol> <p>Composition: Building It Back Up \ud83e\udde9</p> <p>Composition is the inverse of decomposition. It's the practice of building a complex task by assembling a sequence of simpler, well-understood tasks. When a single, complex prompt fails, it's often more reliable to create a chain of simpler prompts.</p> <p>This is where your knowledge of model capabilities on benchmark tasks (like those in MTEB) becomes a superpower. You can design a complex workflow by composing tasks you know the model performs well on.</p> <p>Example: Building an \"Automated Customer Feedback Analyzer\" Instead of one giant prompt like \"Analyze this feedback,\" you can compose the workflow from three MTEB-like tasks:</p> <ol> <li>Prompt 1 (Classification): \"Classify the sentiment of the following text as 'Positive', 'Negative', or 'Neutral': [Customer Feedback]\"</li> <li>Prompt 2 (Summarization): \"Summarize the key point of the classified feedback in one sentence: [Customer Feedback]\"</li> <li>Prompt 3 (Extraction): \"From the feedback, extract the specific product features mentioned: [Customer Feedback]\" By composing the solution this way, each step is more reliable, and the overall result is more accurate and easier to debug.</li> </ol>"},{"location":"prompts/techniques/intro/#pattern-recognition","title":"Pattern Recognition","text":"<p>As you build more AI features, you'll notice that most business needs fall into recurring patterns. You'll learn to see a request and immediately think, \"Ah, that's a classification task, which calls for a few-shot prompt,\" or \"This requires logical steps, so I should start with a Chain-of-Thought prompt.\" Recognizing these patterns is what turns prompt design from guesswork into a repeatable, engineering discipline.</p>"},{"location":"prompts/techniques/kit/","title":"Core Prompting Techniques","text":"<p>Core Prompting Techniques</p> <p>Now that you understand how to create scalable templates and the environments they operate in, it's time to learn the core \"recipes\" of prompt engineering. These techniques are your fundamental tools for translating a business need into a clear, effective instruction for an LLM.</p> <p>We will cover three foundational techniques that progress from simple to more complex, forming the basis of most of the advanced prompting work you will ever do.</p>"},{"location":"prompts/techniques/kit/#1-zero-shot-prompting","title":"1. Zero-Shot Prompting","text":"<p>The Direct Instruction \ud83c\udfaf</p> <p>Zero-shot prompting is the simplest and most direct technique. It involves providing a task description to the model without giving it any prior examples of the desired output. This technique relies entirely on the model's vast pre-existing knowledge and its ability to follow instructions.</p> <p>Example: Simple Classification</p> <p>Imagine you need to classify customer reviews. A zero-shot prompt would look like this:</p> <pre><code>Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE. \n\nReview: \"Her\" is a disturbing study revealing the direction humanity is headed if AI is allowed to keep evolving, unchecked. I wish there were more movies like this masterpiece. \n\nSentiment:\n</code></pre> <p>The model, using its built-in understanding of language and sentiment, will analyze the text and (in this case) likely output \"POSITIVE\". </p> <p>When to Use Zero-Shot</p> <p>Zero-shot prompting is your go-to technique for straightforward, common tasks where the model already has strong capabilities. It works best for:</p> <ul> <li>Simple text classification (e.g., sentiment analysis).</li> <li>Basic text summarization.</li> <li>Language translation.</li> <li>Answering general knowledge questions.</li> </ul>"},{"location":"prompts/techniques/kit/#2-few-shot-prompting","title":"2. Few-Shot Prompting","text":"<p>Learning by Example \ud83e\udde9</p> <p>When a task requires a specific output structure or has nuances that are hard to describe with words alone, zero-shot prompting may not be reliable enough. This is where few-shot prompting shines.</p> <p>This technique involves providing the model with multiple examples (or \"shots\") that demonstrate the task and the expected output format. By showing the model a pattern, you guide it to produce a response that precisely matches your requirements.</p> <p>Example: Parsing Unstructured Data into JSON</p> <p>Imagine you are building a feature to process pizza orders from a chatbot. You need the final output to be in a structured JSON format that your application can easily read.</p> <p>A few-shot prompt would look like this:</p> <pre><code>Parse a customer's pizza order into valid JSON: \n\nEXAMPLE:\nI want a small pizza with cheese, tomato sauce, and pepperoni. \nJSON Response:\n{\n \"size\": \"small\",\n \"type\": \"normal\",\n \"ingredients\": [[\"cheese\", \"tomato sauce\", \"peperoni\"]]\n} \n\nEXAMPLE:\nCan I get a large pizza with tomato sauce, basil and mozzarella \nJSON Response:\n{\n \"size\": \"large\",\n \"type\": \"normal\",\n \"ingredients\": [[\"tomato sauce\", \"bazel\", \"mozzarella\"]]\n} \n\nNow, I would like a large pizza, with the first half cheese and mozzarella. And the other tomato sauce, ham and pineapple. \nJSON Response:\n</code></pre> <p>Given these examples, the model understands the desired schema and will correctly parse the final order into the structured JSON format you've demonstrated.</p> <p>Best Practices for Few-Shot Examples</p> <p>Quality is Crucial: Your examples must be accurate and well-written. A single mistake can confuse the model and lead to undesired output.</p> <p>Start with 3-5 Examples: As a general rule, three to five examples are a good starting point for most tasks.</p> <p>Mix It Up: For classification tasks, make sure you mix up the possible response classes in your examples. This prevents the model from simply learning the order of the examples rather than the underlying features of each class.</p>"},{"location":"prompts/techniques/kit/#3-chain-of-thought-cot-prompting","title":"3. Chain-of-Thought (CoT) Prompting","text":"<p>Showing Your Work \ud83e\udde0</p> <p>For tasks that require logic, math, or multi-step reasoning, even few-shot prompting can sometimes fail. LLMs can often struggle with these problems because they try to jump directly to the answer. Chain-of-Thought (CoT) prompting is a powerful technique that solves this by prompting the model to generate intermediate reasoning steps before giving a final answer.</p> <p>You can often trigger this behavior simply by adding the phrase \"Let's think step by step\" to your prompt.</p> <p>Example: Solving a Reasoning Problem</p> <p>Consider this simple math riddle:</p> <p>Prompt without CoT:</p> <pre><code>When I was 3 years old, my partner was 3 times my age. Now, I am 20 years old. How old is my partner? \n</code></pre> <p>Typical (Incorrect) Output: <code>63 years old</code> </p> <p>The model tries to perform a simple multiplication and gets it wrong. Now, let's use CoT.</p> <p>Prompt with CoT:</p> <pre><code>When I was 3 years old, my partner was 3 times my age. Now, I am 20 years old. How old is my partner? Let's think step by step. \n</code></pre> <p>Correct Output with Reasoning:</p> <pre><code>1. When I was 3 years old, my age was 3 years. \n2. My partner's age at that time was 3 times my age, which means my partner's age was 3 * 3 = 9 years. \n3. Now, I am 20 years old, which means my age has increased by 20 - 3 = 17 years since I was 3 years old. \n4. Since my partner's age was 9 years when I was 3 years old, and my age has increased by 17 years, my partner's age must have also increased by 17 years. \n5. Therefore, my partner's current age is 9 + 17 = 26 years. \n\nSo, my partner is 26 years old. \n</code></pre> <p>By forcing the model to decompose the problem, it arrives at the correct answer.</p> <p>Interpretability</p> <p>CoT provides a massive benefit for product development: you can see the model's \"thought process\". If the model gets an answer wrong, you can examine its reasoning steps to understand where the logic failed, making it much easier to debug and improve your prompt.</p>"},{"location":"prompts/techniques/kit/#self-consistency","title":"Self-Consistency","text":"<p>Self-consistency is a powerful technique that builds on CoT to further improve accuracy on complex reasoning tasks. It works by running the same CoT prompt multiple times with a higher <code>temperature</code> to generate several different reasoning paths. It then takes a majority vote to select the most consistent answer among all the paths. This reduces the chance of a single flawed reasoning chain leading to an incorrect result.</p> <p>Advanced Techniques &amp; Future Directions</p> <p>You now have a solid grasp of the foundational prompting techniques. In this section, we'll look ahead to the cutting edge. These advanced techniques are designed to solve complex problems that require a higher level of abstraction, reasoning, and even interaction with external sources of information.</p> <p>Mastering these concepts will prepare you to design the next generation of sophisticated AI-powered products.</p>"},{"location":"prompts/techniques/kit/#4-step-back-prompting","title":"4. Step-Back Prompting","text":"<p>Seeing the Forest for the Trees \ud83c\udf32</p> <p>Sometimes, a direct prompt, even a well-crafted one, can lead to a narrow or generic answer. Step-back prompting is a technique that improves performance by first prompting the LLM to consider a general question or abstract concept related to the specific task at hand.  This \"step back\" allows the model to activate relevant background knowledge and reasoning processes before attempting to solve the specific problem. </p> <p>By considering the broader principles first, the model can generate more accurate and insightful responses. </p>"},{"location":"prompts/techniques/kit/#example-generating-a-creative-storyline","title":"Example: Generating a Creative Storyline","text":"<p>Imagine you need to generate a creative storyline for a new video game level.</p> <p>A Traditional Prompt:</p> <pre><code>Write a one paragraph storyline for a new level of a first-person shooter video game that is challenging and engaging.\n</code></pre> <p>A Typical, Generic Output:</p> <p>The level begins with the player's squad being ambushed in a dense urban area by a relentless enemy faction. Players must fight their way through a maze of back alleys and dilapidated buildings, utilizing stealth and precision to eliminate enemy forces... </p> <p>This is functional but uninspired. Now, let's try the step-back approach.</p> <p>Step 1: The \"Step-Back\" Prompt to Generate Concepts</p> <p>First, we ask the model a more general question to generate some creative themes:</p> <pre><code>Based on popular first-person shooter action games, what are 5 fictional key settings that contribute to a challenging and engaging level storyline...?\n</code></pre> <p>Step 2: The Final Prompt, Using the Concepts as Context</p> <p>Now, we use the output from our first prompt as rich context for our original request:</p> <pre><code>Context: 5 engaging themes for a first person shooter video game:\n1. Abandoned Military Base...\n2. Cyberpunk City...\n3. Alien Spaceship...\n4. Zombie-Infested Town...\n5. Underwater Research Facility...\n\nTake one of the themes and write a one paragraph storyline for a new level of a first-person shooter video game...\n</code></pre> <p>A Richer, More Specific Output:</p> <p>In the heart of a murky abyss, lies a dilapidated underwater research facility, standing as a testament to human ambition and its disastrous consequences. Shrouded in darkness...the facility is now a breeding ground for a nightmare borne from the deep sea. The player, an elite marine equipped with advanced diving gear...must navigate treacherous corridors, solve cryptic puzzles, and confront gruesome sea monstrosities that lurk in the depths... </p> <p>By first generating high-level concepts and then using them as context, we guided the model to produce a far more creative and compelling result.</p>"},{"location":"prompts/techniques/kit/#5-react-reason-and-act","title":"5. ReAct: Reason and Act","text":"<p>Synergizing Reason and Action \u26a1</p> <p>ReAct is a paradigm that dramatically expands what an LLM can do. It enables models to solve complex tasks by combining natural language reasoning with the ability to take actions using external tools (like a search engine, calculator, or code interpreter). </p> <p>ReAct works in a simple but powerful loop:</p> <ol> <li>Thought: The model reasons about the problem and determines what action it needs to take.</li> <li>Action: The model uses an external tool to get information.</li> <li>Observation: The model receives the output from the tool and uses this new information to update its reasoning and determine the next thought. </li> </ol>"},{"location":"prompts/techniques/kit/#example-answering-a-multi-part-factual-question","title":"Example: Answering a Multi-Part Factual Question","text":"<p>Imagine you ask a model, \"How many kids do the band members of Metallica have?\" An LLM's internal knowledge might be outdated or incomplete. With ReAct, it can use a search engine to find the answer. The process looks like this:</p> <p>&gt; Entering new AgentExecutor chain... Thought: I need to find out who the members of Metallica are first. Action: Search Action Input: current members of Metallica Observation: James Hetfield, Lars Ulrich, Kirk Hammett, Robert Trujillo Thought: Now I need to find out how many kids each member has. I'll start with James Hetfield. Action: Search Action Input: How many kids does James Hetfield have? Observation: three children Thought: 1/4 Metallica band members have 3 children. Now for Lars Ulrich. Action: Search Action Input: How many kids does Lars Ulrich have? Observation: 3 Thought: 2/4 Metallica band members have 6 children... ...this process continues for all members... Thought: 4/4 Metallica band members have 10 children Final Answer: 10 </p> <p>This ability to interact with external tools makes the model dramatically more powerful and factually grounded.</p> <p>\ud83e\udd16 The Birth of AI Agents</p> <p>The ReAct paradigm is a fundamental step toward creating AI agents. An agent is more than just a text generator; it's an autonomous system that can reason about a goal and take a sequence of actions in a digital or physical environment to achieve it.</p> <p>As a product manager, understanding ReAct is crucial because it represents a shift from building features that answer questions to building systems that accomplish tasks. Whether it's an AI travel agent that can book flights or a financial analyst that can execute trades, the underlying logic often starts with this \"Reason and Act\" loop.</p>"},{"location":"prompts/techniques/next-steps/","title":"Conclusion &amp; Next Steps","text":"<p>Congratulations on completing this lesson. You have moved from the basic anatomy of a prompt to a full, professional workflow. You have a mindset for decomposing and composing problems, you know how to build scalable templates, you have a core toolkit of prompting techniques, and you have a workflow for developing and documenting your work.</p> <p>With these techniques, you can guide LLMs to perform incredibly powerful and complex tasks.</p>"},{"location":"prompts/techniques/next-steps/#next-steps","title":"Next Steps","text":"<p>Now that you have a powerful toolkit of techniques, the next logical step is to understand the specific strengths, weaknesses, and characteristics of the models you'll be prompting. In our next module, \"Aligning Prompt Design with Model Capabilities,\" we will explore how to select the right model for your prompt and how to tailor your prompt for the model you've chosen, ensuring you get the best possible performance for your product.</p>"},{"location":"prompts/techniques/next-steps/#project-takeaways","title":"\ud83d\udca1 Project Takeaways","text":"<p>As you begin working on your group course project, keep these key takeaways from this module in mind:</p> <ul> <li>Start Simple, Then Iterate: Don't jump to the most complex technique first. Start with a simple zero-shot prompt. If it's not reliable enough, then upgrade to a few-shot prompt or introduce Chain-of-Thought. The goal is to find the simplest technique that reliably solves the problem.</li> <li>Few-Shot is Your Superpower for Structure: If your project requires the LLM to output structured data (like JSON) that your application can use, a well-crafted few-shot prompt is almost always the best approach.</li> <li>Document From Day One: Use the documentation template from this lesson to log your team's prompt experiments. This will save you countless hours when you need to debug, hand off work, or test your system against a new model.</li> <li>Use CoT for Any \"Thinking\": If any feature in your project requires the model to reason, perform calculations, or follow a logical sequence, use Chain-of-Thought prompting to improve accuracy and make the model's reasoning process visible for debugging.</li> </ul>"},{"location":"prompts/techniques/next-steps/#try-it-yourself","title":"\ud83d\ude80 Try It Yourself","text":"<p>The best way to master these techniques is to use them. Here are some activities to help you practice.</p>"},{"location":"prompts/techniques/next-steps/#in-google-ai-studio","title":"In Google AI Studio","text":"<ol> <li>Objective: Experience the difference in reliability between zero-shot and few-shot prompting.</li> <li>Task: Create a prompt that extracts a <code>book_title</code> and <code>author</code> from an unstructured sentence and returns the output in a clean JSON format.</li> <li>Part 1 (Zero-Shot): Write a prompt that only describes the task. For example: <code>Extract the book title and author from the following sentence and format it as JSON: \"I just finished reading The Hobbit by J.R.R. Tolkien.\"</code> Run it a few times. Does it always return valid JSON?</li> <li>Part 2 (Few-Shot): Now, create a new prompt. This time, provide 2-3 examples of sentences and their corresponding JSON outputs before giving it the final sentence to process. Observe how the consistency and reliability of the output improve dramatically.</li> </ol>"},{"location":"prompts/techniques/next-steps/#in-google-colab","title":"In Google Colab","text":"<p>To see these techniques implemented in Python with the Gemini API, we highly recommend exploring the examples in the official Gemini Cookbook on GitHub. You can open these notebooks directly in Google Colab to run and modify the code yourself.</p> <ul> <li>Link: Gemini Prompting Examples Cookbook</li> <li>Suggested Notebooks:<ul> <li>The <code>Few-shot prompting</code> notebook is a great place to start.</li> <li>The <code>Chain-of-thought</code> notebook shows you how to implement CoT in code to solve more complex problems.</li> </ul> </li> </ul>"},{"location":"prompts/techniques/next-steps/#references-further-reading","title":"\ud83d\udcda References &amp; Further Reading","text":"<ol> <li>Prompt Engineering Whitepaper: A comprehensive guide covering the topics from this module and more. Many examples in this lesson are sourced from this whitepaper. <ul> <li><code>https://www.kaggle.com/whitepaper-prompt-engineering</code></li> </ul> </li> <li>Gemini API Documentation - Prompting Strategies: Google's official documentation with tips and strategies.<ul> <li><code>https://ai.google.dev/gemini-api/docs/prompting-strategies</code></li> </ul> </li> <li>Gemini API Reference: The technical API documentation for generating content, useful when you start coding.<ul> <li><code>https://ai.google.dev/api/generate-contentAPI</code></li> </ul> </li> </ol>"},{"location":"prompts/techniques/tips/","title":"Workflow Tips","text":"<p>You've explored the mindset, the foundational tools, and a powerful toolkit of prompting techniques. The final piece of the puzzle is putting it all together in a structured, professional workflow.</p> <p>Having a set of techniques is one thing; knowing how to use them systematically to solve problems is what separates an amateur from a professional prompt engineer. This page provides a practical workflow tips for developing, debugging, and documenting your prompts in a team environment.</p>"},{"location":"prompts/techniques/tips/#iterative-process","title":"Iterative Process \ud83d\udd04","text":"<p>The most important thing to remember is that prompt engineering is a cycle, not a straight line. Your first attempt will rarely be perfect. The professional workflow is a loop of crafting, testing, and refining until you achieve the desired output with consistent reliability.</p> <p>Inadequate prompts can lead to ambiguous or inaccurate responses, hindering the model's ability to provide meaningful output. The goal of this workflow is to move from an initial, inadequate prompt to a refined, production-ready one as efficiently as possible.</p>"},{"location":"prompts/techniques/tips/#debugging-prompts","title":"Debugging Prompts \ud83d\udc1e","text":"<p>When your prompt doesn't produce the output you expect, don't just try random changes. Work through a systematic checklist to diagnose the problem.</p> <ul> <li>Simplify and Be Direct: Is your language overly complex or your instruction vague? Reword your prompt to be as concise and clear as possible. Start your task with a strong verb (e.g., \"Generate,\" \"Classify,\" \"Summarize\") to remove ambiguity.</li> <li>Be Specific About the Output: If the model's output is unstructured or the wrong style, you likely haven't been specific enough. Explicitly instruct the model on your desired format (e.g., \"Return the output as a numbered list,\" \"Use valid JSON\"), length (\"Summarize in a single paragraph\"), and style (\"Write in a formal, academic tone\").</li> <li>Provide Examples (Upgrade Your Technique): This is often the most powerful debugging step. If a zero-shot prompt is failing because the task is too nuanced, upgrade it to a few-shot prompt. Showing the model 3-5 high-quality examples of what you want is the clearest possible way to communicate your intent.</li> <li>Check Your Examples: If you're using a few-shot prompt and it's still failing, the error might be in your examples. Review them carefully for typos, formatting errors, or logical inconsistencies.</li> </ul>"},{"location":"prompts/techniques/tips/#document-everything","title":"Document Everything \u270d\ufe0f","text":"<p>In a professional environment, your work needs to be reproducible, understandable, and maintainable. You must document your prompt engineering attempts in a structured way. This allows you to track what went well, learn from what didn't, and collaborate effectively with your team.</p> <p>We recommend creating a shared document (like a Google Sheet) to log your key prompt versions. This log is invaluable for debugging, testing prompts against new model versions, and providing a complete record of your work.</p> <p>A Template for Documenting Prompts</p> Field Description Name A unique name and version for your prompt (e.g., <code>customer_feedback_classifier_v3</code>). Goal A one-sentence explanation of what this prompt attempt is trying to achieve. Model The name and version of the LLM you are using (e.g., <code>gemini-pro</code>). Temperature The temperature setting used (e.g., <code>0.2</code>). Token Limit The max output tokens configured. Top-K The Top-K setting used. Top-P The Top-P setting used. Prompt The full, exact text of the prompt. Output One or more examples of the output generated by the model."},{"location":"prompts/workflow/","title":"Prompt Development Workflow","text":""},{"location":"syllabus/","title":"Syllabus","text":"<p>Course Information</p> <ul> <li>Course Title: Technical Foundations of Generative AI for Business - BUSBIS 1530</li> <li>Class Number: 30860</li> <li>Term &amp; Credits: Fall 2025-2026 / 3 Credits</li> <li>Pre-requisites/Co-requisites: Programming Essentials for Business Analytics - BUSBIS 0100</li> </ul> <p>Schedule &amp; Instructor</p> <ul> <li>Meeting Times: Tuesdays &amp; Thursdays 3:30-4:45pm </li> <li>Meeting Location: 118D Mervis Hall</li> <li>Instructor: Midhu Balan</li> <li>Office Hours: Tuesdays &amp; Thursdays 12:00-2:00pm. Mervis Hall 315</li> </ul> <ul> <li> Course Description</li> <li> Schedule</li> <li> Course Material</li> <li> Grading &amp; Evaluation</li> <li> Course Project</li> <li> AI Collaboration Policy</li> <li> Course Policies</li> <li> Teaching Philosophy</li> </ul>"},{"location":"syllabus/ai-policy/","title":"AI Collaboration Policy","text":"<p>In this course, Generative AI is not just a topic of study; it is an integral tool for learning and building. You are not only permitted but actively encouraged to use AI as a collaborative partner. Learning to leverage these tools effectively and ethically is a core learning objective.</p> <p>However, the goal of the course is for you to learn. Using a tool to assist your work is different from having the tool do your work for you. This policy outlines the boundaries to ensure you are learning effectively while using AI responsibly.</p>"},{"location":"syllabus/ai-policy/#green-light-always-permitted-encouraged","title":"\u2705 Green Light: Always Permitted &amp; Encouraged","text":"<p>Think of the AI as your personal tutor, brainstorming partner, or pair programmer. You can and should use AI for the following tasks without any special attribution:</p> <ul> <li>Brainstorming &amp; Idea Generation: Asking an AI for project ideas, potential features, or ways to approach a problem.</li> <li>Conceptual Understanding: Asking for explanations of technical concepts, definitions, or analogies (e.g., \"Explain Retrieval-Augmented Generation like I'm a business manager\").</li> <li>Debugging Assistance: Pasting your code and asking an AI to help you find errors or suggest fixes.</li> <li>Improving Your Writing: Using AI to check your grammar, fix spelling, or rephrase a sentence for clarity in your reports and documentation.</li> <li>Code Generation with AI Assistants: Using tools like CodeAssist or GitHub Copilot to generate code snippets, complete lines of code, or create boilerplate functions is an expected part of the workflow.</li> </ul>"},{"location":"syllabus/ai-policy/#yellow-light-permitted-with-attribution","title":"\u26a0\ufe0f Yellow Light: Permitted with Attribution","text":"<p>When an AI provides a substantial, unique contribution to your submitted work, you must give it credit. This is not about penalizing you; it's about practicing professional and academic honesty.</p> <p>You must provide attribution when:</p> <ul> <li>An AI generates a significant block of text (e.g., multiple paragraphs in your <code>README.md</code> or project report) that you include in your work.</li> <li>An AI provides a specific, complex algorithm or code block that is central to your project's functionality and was not generated via an integrated tool like CodeAssist.</li> </ul> <p>How to Attribute:</p> <p>In your group's GitHub repository, create a file named <code>AI_CITATIONS.md</code>. For each use case that requires attribution, add a simple entry:</p> <p>Citing AI Use</p> <p>Source: [Name of AI Model, e.g., Google Gemini]  Prompt: [\"Your original prompt here\"]  Usage: [Briefly describe how the output was used in your project, e.g., \"Used as the foundational text for the 'Project Goals' section of our README.md, which we then edited and refined.\"]</p>"},{"location":"syllabus/ai-policy/#red-light-academic-misconduct","title":"\ud83d\udeab Red Light: Academic Misconduct","text":"<p>Crossing the line from \"use\" to \"misuse\" happens when you outsource the core thinking and learning process to an AI. The following are considered violations of academic integrity:</p> <ul> <li>Submitting an entire project, lab, or written assignment that was generated by an AI with little to no personal modification, input, or analysis.</li> <li>Presenting AI-generated ideas, analysis, or code as your own original thought without attribution.</li> <li>Using an AI to complete an assignment that is explicitly designated as an individual, tool-free assessment of your knowledge.</li> </ul>"},{"location":"syllabus/ai-policy/#the-prime-directive","title":"The Prime Directive","text":"<p>Here's a simple rule to follow: You must be able to explain any part of your submitted work. If you copy-paste code from an AI and cannot explain how it works, what it does, or why it's necessary, you have not used the tool correctly. The AI is your collaborator, but you are the project lead. You are ultimately responsible for and the expert on your own work.</p>"},{"location":"syllabus/course-description/","title":"Course Description","text":""},{"location":"syllabus/course-description/#overview","title":"Overview","text":"<p>This course provides a practical, hands-on introduction to the world of Generative AI from a business perspective. You will learn how these AI skills are being applied today to automate marketing campaigns, develop new financial analysis tools, streamline supply chain operations, and create intelligent customer service solutions. In today's economy, understanding how to build with and manage AI is no longer a niche technical skill\u2014it's a core business competency. This course moves beyond theory and buzzwords, treating you as a product manager and developer tasked with building a real-world AI application. While this is not a course for programming beginners, it is designed for business students with a foundational knowledge of Python. We will build upon the skills from the prerequisite courses, focusing on applying them rather than learning them from scratch.</p> <p>A unique aspect of this course is our 'AI-assisted learning' approach. \ud83e\udd16 We won't just study AI; we will use it as a creative partner and a productivity tool. You'll leverage AI for brainstorming project ideas, assisting with code development, and deepening your own research, learning the critical meta-skill of how to collaborate effectively with intelligent systems.</p> <p>Through a semester-long, project-based experience, you will learn to identify business opportunities for AI, design user-centric applications, and build a functional AI-powered prototype using enterprise-grade cloud tools. This is an intensive, experiential course. Success requires continuous engagement, collaborative teamwork, and a curiosity to build with the defining technology of our time.</p>"},{"location":"syllabus/course-description/#learning-objectives","title":"Learning Objectives","text":"<p>Upon successful completion of this course, you will be able to:</p> <ul> <li>Identify &amp; Strategize: Analyze business processes and identify high-value opportunities for applying Generative AI solutions.</li> <li>Design &amp; Plan: Translate a business idea into a formal product plan, complete with user personas, experience maps, and technical requirements, using professional project management tools like GitHub.</li> <li>Build &amp; Prototype: Develop a functional AI chatbot or application prototype by customizing and extending a foundational code scaffold.</li> <li>Engineer Prompts: Design, test, and refine sophisticated prompts to control AI model behavior and ensure reliable, high-quality outputs.</li> <li>Integrate Advanced AI Techniques: Implement advanced features like Retrieval-Augmented Generation (RAG) to allow your application to reason over specific documents and data.</li> <li>Leverage Cloud Platforms: Use an enterprise-grade cloud platform (Google Cloud) to run and manage components of your AI application in a secure, sandboxed environment.</li> <li>Collaborate with AI: Use AI-powered developer tools like CodeAssist as an effective coding partner to improve productivity and code quality.</li> <li>Communicate &amp; Demonstrate: Articulate the value, functionality, and ethical considerations of your AI application to both technical and non-technical audiences.</li> </ul>"},{"location":"syllabus/course-materials/","title":"Course Materials","text":"<p>This course uses a combination of online readings, hands-on lab platforms, and professional software tools. There is no required textbook to purchase. All necessary materials are either free or accessible through subscriptions provided by the university or free-tier programs.</p>"},{"location":"syllabus/course-materials/#course-website-and-communication","title":"Course website and communication","text":"<p>We will use Pitt Canvas as the primary place to distribute other course-related  content and announcements. All learning materials including video recordings  and assignments will be posted on the course Canvas page. </p> <p>Info</p> <p>Use Pitt email and Canvas for course-related communication.</p>"},{"location":"syllabus/course-materials/#books-reading-materials","title":"Books &amp; Reading Materials","text":"<p>No Textbook Purchase Required</p> <p>No purchase of textbook is required for this course.</p> <p>Our foundational \"textbook\" is the free online Kaggle 5-Day GenAI guide. It provides an excellent baseline for core concepts.</p> <p>Because this field evolves rapidly, the Kaggle guide will be supplemented with required readings and videos from current industry and research sources. These will be posted weekly on the course website and will include materials from sources like the Google AI, OpenAI, and Anthropic blogs.</p>"},{"location":"syllabus/course-materials/#software-tools","title":"Software Tools \ud83e\uddd1\u200d\ud83d\udcbb","text":"<ul> <li> <p> Google Gemini</p> <p> Pitt Services Guide</p> <p> Pitt Knowledge Base Article</p> </li> <li> <p> Google Colab</p> <p> Getting Started with Colab</p> </li> <li> <p> GitHub Codespaces</p> <p> GitHub Student Developer Pack</p> </li> <li> <p> GitHub Copilot</p> <p> Pitt Services Guide</p> </li> <li> <p> Streamlit</p> <p> Streamlit on GitHub CodeSpaces Guide</p> </li> <li> <p> Google NotebookLM</p> <p> Pitt Services Guide</p> <p> Pitt Knowledge Base Article</p> </li> </ul>"},{"location":"syllabus/course-materials/#coding-environment","title":"Coding Environment:","text":"<p>Our course uses a \"cloud-first\" approach with two primary, fully supported environments\u2014one for labs and one for your project. This ensures everyone has a consistent and powerful setup, regardless of their personal computer.</p> <ul> <li> <p>For Weekly Labs &amp; Experiments: Google Colab \ud83d\udcd3</p> <ul> <li>We will use Google Colab for individual, hands-on lab exercises. It's an interactive notebook environment that runs in your browser, perfect for learning core concepts and experimenting with code.</li> </ul> </li> <li> <p>For the Group Project: GitHub Codespaces \ud83e\uddd1\u200d\ud83d\udcbb</p> <ul> <li>All work on your semester-long group project will be done in GitHub Codespaces. This provides a complete VS Code editor directly in your web browser, running in a powerful cloud environment. It's specifically designed for building applications and collaborating as a team, and it is perfectly integrated with your team's GitHub repository.</li> </ul> </li> </ul> <p>Local Tools (Best-Effort Support)</p> <p>While not required, you may want to install tools like VS Code or the Gemini CLI on your local machine for your own exploration. Please be aware that instructor support for local installations is provided on a best-effort basis, as our official course environments are cloud-based.</p>"},{"location":"syllabus/course-materials/#platform-lab-subscriptions","title":"Platform &amp; Lab Subscriptions","text":"<ul> <li>Google Cloud Skills Boost: You are required to sign up for Google Cloud Skills Boost to complete mandatory hands-on labs.<ul> <li>Cost: No cost subscription comes with free monthly credits sufficient to complete all required labs for the course. You will not need to buy any additional credits or a paid subscription to complete graded labs, if you plan your work on the platform.</li> </ul> </li> </ul>"},{"location":"syllabus/course-materials/#ai-model-subscriptions","title":"AI Model Subscriptions","text":"<p>You do not need to purchase personal, paid subscriptions to AI models like ChatGPT Plus or Claude Pro for this course. API access to powerful models like Google Gemini Pro are available for free experimentation and testing via Google Developers Program. We will use Google AI Studio and Cloud Skills Boost Platform as our preferred learning environments. We will also explore the free tiers of other publicly available models as part of our market analysis.</p>"},{"location":"syllabus/course-materials/#optional-subscriptions-trials","title":"Optional Subscriptions &amp; Trials","text":"<p>While not required for the course, you're encouraged to take advantage of special offers for students to gain experience with other cutting-edge AI tools.</p> <ul> <li> <p>AI Premium Subscription (Gemini Advanced): Google offers eligible university students a free 1 year trial of this subscription, which provides access to their most capable models. This is an excellent opportunity to experiment with a state-of-the-art AI at no cost. You can check offer availability and your eligibility at the Gemini for Students page.</p> <p>\u26a0\ufe0f Important: This is a trial that may automatically convert to a paid subscription. If you choose to sign up, you are responsible for canceling it before the trial ends to avoid being charged. I strongly recommend setting a calendar reminder for yourself on the day you subscribe.</p> </li> </ul>"},{"location":"syllabus/course-policies/","title":"Course Policies","text":""},{"location":"syllabus/course-policies/#attendance-participation-policy","title":"Attendance &amp; Participation Policy","text":"<p>1. Policy Rationale</p> <p>Success in \"Technical Foundations of Generative AI\" requires consistent engagement and active participation. This course is designed as a hands-on, collaborative workshop. In-class sessions are not lectures but opportunities to build, troubleshoot, and learn from your peers. Your presence is therefore critical not only for your own learning but also for the learning of your project team and the class as a whole. This policy is intended to reflect the professional standard of accountability expected in the workplace.</p> <p>2. Attendance Requirement</p> <p>A minimum attendance rate of 80% is required to pass this course. Failure to meet this requirement will result in the final course grade being lowered by one full letter grade (e.g., from a B+ to a C+).</p> <p>3. Definition of an Absence</p> <p>Any instance of missing a scheduled class for any reason will be recorded as an absence. However, a distinction is made for documented, extenuating circumstances.</p> <ul> <li> <p>Extenuating Circumstances (Excused): These are serious, unavoidable events that prevent attendance and will not count toward the 80% threshold. They are limited to:</p> <ul> <li>Documented medical emergencies or significant illness.</li> <li>A death in the immediate family.</li> <li>University-sanctioned activities (e.g., athletic competitions, academic conferences) with prior official notification.</li> <li>Recognized religious observances.</li> </ul> </li> <li> <p>Non-Extenuating Circumstances (Unexcused): These absences, even when communicated in advance, will be counted as an absence and apply toward the 80% threshold. Examples include, but are not limited to:</p> <ul> <li>Job interviews or work-related commitments.</li> <li>Personal travel or family vacations.</li> <li>Appointments that could be scheduled outside of class time.</li> <li>Minor illnesses without medical documentation.</li> </ul> </li> </ul> <p>4. Notification Procedure</p> <ul> <li>For any absence, students are expected to notify the instructor via email prior to the start of class as a matter of professional courtesy.</li> <li>To have an absence classified as an \"Extenuating Circumstance,\" official documentation (e.g., a note from a medical professional, a university notice) must be provided to the instructor within one week of the absence.</li> </ul> <p>5. Tardiness</p> <p>Arriving more than 10 minutes late or leaving more than 10 minutes before the class concludes is disruptive to the learning environment. Every three instances of tardiness or early departure will be recorded as one unexcused absence.</p>"},{"location":"syllabus/course-policies/#university-policies","title":"University Policies","text":""},{"location":"syllabus/course-policies/#your-well-being-matters","title":"Your Well-being Matters","text":"<p>College/Graduate school can be an exciting and challenging time for students. Taking time to maintain your well-being  and seek appropriate support can help you achieve your goals and lead a fulfilling life. It can be helpful to remember  that we all benefit from assistance and guidance at times, and there are many resources available to support your  well-being while you are at Pitt. You are encouraged to visit Thrive@Pitt to learn more  about well-being and the many campus resources available to help you thrive.</p> <p>If you or anyone you know experiences overwhelming academic stress, persistent difficult feelings and/or challenging  life events, you are strongly encouraged to seek support. In addition to reaching out to friends and loved ones,  consider connecting with a faculty member you trust for assistance connecting to helpful resources.</p> <p>The University Counseling Center is also here for you. You can call 412-648-7930  at any time to connect with a clinician. If you or someone you know is feeling suicidal, please call the University  Counseling Center at any time at 412-648-7930. You can also contact Resolve Crisis Network at 888-796-8226.  If the situation is life threatening, call Pitt Police at 412-624-2121 or dial 911.</p>"},{"location":"syllabus/course-policies/#equity-diversity-and-inclusion","title":"Equity, Diversity, and Inclusion","text":"<p>The University of Pittsburgh does not tolerate any form of discrimination, harassment, or retaliation based on  disability, race, color, religion, national origin, ancestry, genetic information, marital status, familial status,  sex, age, sexual orientation, veteran status or gender identity or other factors as stated in the University\u2019s Title IX  policy. The University is committed to taking prompt action to end a hostile environment that interferes with the  University\u2019s mission. For more information about policies, procedures, and practices, visit the  Civil Rights &amp; Title IX Compliance web page.</p> <p>I ask that everyone in the class strive to help ensure that other members of this class can learn in a supportive  and respectful environment. If there are instances of the aforementioned issues, please contact the Title IX Coordinator , by calling 412-648-7860, or e-mailing titleixcoordinator@pitt.edu. Reports can  also be filed online. You may also choose to  report this to a faculty/staff member; they are required to communicate this to the University\u2019s Office of Diversity  and Inclusion. If you wish to maintain complete confidentiality, you may also contact the University Counseling Center  (412-648-7930).</p>"},{"location":"syllabus/course-policies/#disability-services","title":"Disability Services","text":"<p>If you have a disability for which you are or may be requesting an accommodation, you are encouraged to contact both  your instructor and Disability Resources and Services (DRS),  140 William Pitt Union, (412) 648-7890, drsrecep@pitt.edu,  (412) 228-5347 for P3 ASL users, as early as possible in the term. DRS will verify your disability and determine  reasonable accommodations for this course.</p>"},{"location":"syllabus/course-policies/#academic-integrity","title":"Academic Integrity","text":"<p>Students in this course will be expected to comply with the  University of Pittsburgh\u2019s Policy on Academic Integrity.  Any student suspected of violating this obligation for any reason during the semester will be required to participate  in the procedural process, initiated at the instructor level, as outlined in the University Guidelines on Academic  Integrity. This may include, but is not limited to, the confiscation of the examination of any individual suspected of  violating University Policy. Furthermore, no student may bring any unauthorized materials to an exam, including  dictionaries and programmable calculators.</p> <p>To learn more about Academic Integrity, visit the  Academic Integrity Guide  for an overview of the topic. For hands-on practice, complete the  Academic Integrity Modules.</p>"},{"location":"syllabus/grading/","title":"Grading &amp; Evaluation","text":"<p>Our grading philosophy is designed to mirror a professional environment where success is measured by both individual skills and the quality of team collaboration. The evaluation structure is transparent, giving you clear targets and rewarding consistent effort, technical mastery, and strategic thinking.</p>"},{"location":"syllabus/grading/#overall-grade-distribution","title":"Overall Grade Distribution","text":"<p>Your final grade will be calculated out of a total of 100 points, distributed across four key areas:</p> Component Points Percentage Cloud Skills Boost Labs (Individual) 40 points 40% Group Project (Team) 45 points 45% Launch Kit - Peer Review (Individual) 5 points 5% Active &amp; Even Contribution (Individual) 10 points 10% Total 100 points 100%"},{"location":"syllabus/grading/#cloud-skills-boost-labs","title":"Cloud Skills Boost Labs","text":"<p>This component measures your individual, hands-on mastery of core cloud and AI tools through official Google Cloud labs. This is a mastery-based grade; you choose the level you wish to achieve.</p> <ul> <li>Beginner Path Completion (C-Level): Earns 28 out of 40 points.</li> <li>Intermediate Path Completion (B-Level): Earns 34 out of 40 points.</li> <li>Advanced Path Completion (A-Level): Earns the full 40 points.</li> </ul> <p>Completing these paths also earns you official Google Cloud badges that you can share on your LinkedIn profile to demonstrate your skills to potential employers.</p> Level Title Cost Date Beginner AI Boost Bites: Your Edge in the AI-Powered World Free Beginner Generative AI Leader Free Beginner Introduction to Generative AI Learning Path Free Intermediate Generative AI Labs with Gemini on Google Cloud 21 credits Advanced Generative AI Labs with Gemini on Google Cloud  or  Generative AI for Developers Learning Path 26 Credits   19 Credits"},{"location":"syllabus/grading/#group-project","title":"Group Project","text":"<p>This is the culminating project where your team will design, build, and present a functional AI application. See the project description and rubric for more details. Review the detailed description of each of the deliverables below.</p> <ul> <li> Bootstrap </li> <li> Project Charter </li> <li> Minimum Viable Product (MVP) </li> <li> Experimentation &amp; Refactoring Report </li> <li> Launch Kit &amp; Team Reflection </li> <li> Peer Review </li> </ul>"},{"location":"syllabus/grading/#individual-project-contribution","title":"Individual Project Contribution","text":"<p>This grade ensures individual accountability and rewards strong teamwork. It is assessed using your activity in the team's GitHub repository and a confidential peer evaluation.</p> Criteria Exemplary (A-Level) Proficient (B-Level) Needs Improvement (C/D-Level) Unsatisfactory (F-Level) Task Management &amp; Completion(4 Points) 4 PointsConsistently completes all assigned GitHub Issues on time. Contributions are high-quality, well-documented, and clearly visible in the repository. 3 PointsCompletes most assigned Issues on time. Contributions are of good quality and visible in the repository. 2 PointsCompletes some assigned Issues, but may be frequently late or contributions are of low quality. 0-1 PointFails to complete most assigned Issues. There is little to no evidence of work in the repository. Communication &amp; Proactivity(3 Points) 3 PointsFrequently provides thoughtful comments, suggestions, and constructive feedback on the team's Issues. Proactively identifies new tasks and creates Issues for them. 2 PointsParticipates in discussions within Issues and offers feedback when asked. Communicates progress on assigned tasks. 1 PointCommunication is minimal. Rarely participates in discussions or offers feedback. 0 PointsNo meaningful communication or proactive contributions are evident in the repository. Team Citizenship &amp; Reliability(Based on Peer Evaluation)(3 Points) 3 PointsPeer evaluations consistently rate the student as a highly reliable, prepared, and supportive teammate who elevates the group's performance. 2 PointsPeer evaluations indicate the student is a reliable and solid contributor who meets team expectations. 1 PointPeer evaluations suggest issues with reliability, preparation for meetings, or a pattern of passive contribution. 0 PointsPeer evaluations consistently indicate a lack of participation, reliability, or respect for the team. <p>Peer Evaluation Process: At the conclusion of the group project, you will be required to complete a confidential peer evaluation for each member of your team, including yourself. You will be asked to rate your teammates on a scale of 1-5 across several categories, such as reliability, quality of contributions, and communication, and provide a brief justification for your ratings.</p>"},{"location":"syllabus/grading/#grading-policies-and-scale","title":"Grading Policies and Scale","text":"<ul> <li>Late Submissions: Late work is graded at the instructor's discretion. This is a project-based course that mimics a professional environment where deadlines matter. If you anticipate a delay, avoid surprises by communicating early and often.</li> <li>Extra Credit: There are no explicit extra credit opportunities. The project is designed with ample room for you to demonstrate excellence and go above and beyond the baseline requirements.</li> <li>Final Grade Scale: Your final point total will be converted to a letter grade based on the university's default grading scheme:</li> </ul> Letter Grade Range A 100% to 94% A- &lt; 94% to 90% B+ &lt; 90% to 87% B &lt; 87% to 84% B- &lt; 84% to 80% C+ &lt; 80% to 77% C &lt; 77% to 74% C- &lt; 74% to 70% D+ &lt; 70% to 67% D &lt; 67% to 64% D- &lt; 64% to 61% F &lt; 61% to 0%"},{"location":"syllabus/schedule/","title":"Schedule","text":""},{"location":"syllabus/schedule/#course-schedule","title":"Course Schedule","text":"<p>This schedule is designed around a weekly rhythm. The first session of the week is typically a Learner Session \ud83e\udde0, where we introduce and discuss new concepts. The second session is a Builder Session \ud83d\udee0\ufe0f, where you'll apply those concepts directly to your team project in a hands-on workshop format.</p> Getting StartedGen AI &amp; LLMsPrompt DesignContext EngineeringAgents &amp; Course Wrap-Up Date Topic 08/26 \ud83e\udde0 Course Overview &amp; Project Introduction 08/28 \ud83d\udee0\ufe0f Tools Overview &amp; Dry Run <p>Pre-class Activities:</p> <ul> <li>Read and understand course details</li> <li>Sign up for a GitHub account.</li> </ul> <p>Deliverables: </p> <ul> <li>A link to your team's GitHub Repo. Make sure that all team members appear in the repo's contributor list. See the Project Bootstrap page for details.</li> </ul> <p>Resources:</p> <ul> <li>About Git &amp; GitHub </li> <li>Google AI Studio</li> <li>GitHub Codespaces</li> <li>Gemini CLI</li> <li>Google Colab</li> </ul> Date Topic 09/02 \ud83e\udde0 Large Language Models (LLMs) &amp; Model Capabilities 09/04 \ud83d\udee0\ufe0f Mapping Project Tasks to Model Use &amp; Capabilities 09/09 \ud83e\udde0 Model Selection &amp; Business Alignment 09/11 \ud83d\udee0\ufe0f Problem Framing &amp; Translating Business Needs to User Stories 09/16 \ud83e\udde0 Embedding Models &amp; Their Business Relevance 09/18 \ud83d\udee0\ufe0f Developing Project Governance and Delivery Framework Using AI Tools <p>Pre-class Activities:</p> <ul> <li>Large Language Models (LLMs) &amp; Model Capabilities<ul> <li>KaggleWhite Paper: Foundational Large Language Models &amp; Text Generation</li> <li>Whitepaper Companion Podcast</li> <li>Behind the scenes of Google's state-of-the-art \"nano-banana\" image model</li> </ul> </li> </ul> <p>Deliverables: </p> <ul> <li>Project Charter</li> </ul> <p>Resources:</p> <ul> <li>First Day on GitHub</li> <li>First Week on GitHub</li> <li>Project Governance Samples<ul> <li>Project Front Office</li> <li>Feature Roadmap</li> <li>Team Discussion</li> </ul> </li> </ul> Date Topic 09/23 \ud83e\udde0 Anatomy of a Prompt &amp; Basic Prompting 09/25 \ud83d\udee0\ufe0f Working with System Instructions, Variables &amp; Templates 09/30 \ud83e\udde0 Prompting Techniques 10/02 \ud83d\udee0\ufe0f Working with Prompt Structure, Reasoning, and Model Parameters 10/07 \ud83e\udde0 Aligning Prompt Design with Model Capabilities 10/09 \ud83d\udee0\ufe0f Working with Thinking, Grounding, Structured Outputs, and Function Calling 10/14 \ud83e\udde0 AI Safety &amp; Ethics in Prompting 10/16 \ud83d\udee0\ufe0f Implementing Responsible AI Practices 10/21 \ud83e\udde0 Prompt Development Workflow: Explore, Evaluate, and Iterate 10/23 \ud83d\udee0\ufe0f Implementing Iterative Prompting Techniques <p>Pre-class Activities:</p> <p>Kaggle on Prompt Engineering</p> <ul> <li>Prompt Engineering - Kaggle Whitepaper</li> <li>Whitepaper Companion Podcast - Prompt Engineering </li> <li>Prompting Fundamentals - Kaggle Notebook</li> </ul> <p>Deliverables: </p> <ul> <li>Minimum Viable Product (MVP)</li> </ul> <p>Resources:</p> <ul> <li>Prompt design strategies</li> <li>Prompting Guide</li> <li>Prompting for Image Generation</li> <li>Prompt Examples Demonstrating Image Editing &amp; Generation</li> <li>Prompt Examples Demonstrating Video Generation Using Images</li> </ul> Date Topic 10/28 \ud83e\udde0 Grounding Models with Retrieval-Augmented Generation (RAG) 10/30 \ud83d\udee0\ufe0f Implementing a Basic RAG Pipeline 11/04 Project Booster: Async Week 11/06 Project Booster: Async Week 11/11 \ud83e\udde0 Context Caching, Summarization, &amp; \"Needle in a Haystack\" Findings 11/13 \ud83d\udee0\ufe0f Context Engineering in Practice <p>Pre-class Activities:</p> <ul> <li>placeholder</li> </ul> <p>Deliverables: </p> <ul> <li>Experimentation &amp; Refactoring</li> </ul> <p>Resources:</p> <ul> <li>placeholder</li> </ul> Date Topic 11/18 \ud83e\udde0 Agents, Tools, MCP, &amp; A2A Protocol 11/20 \ud83d\udee0\ufe0f Agents in Action 11/25 Thanksgiving Recess 11/27 Thanksgiving Recess 12/02 \ud83e\udde0 Team Reflection  &amp; Interviews 12/04 \ud83d\udee0\ufe0f Launch Kit Peer Review, &amp; Course Wrap-Up 12/09 Finals Week 12/11 Finals Week <p>Pre-class Activities:</p> <ul> <li>placeholder</li> </ul> <p>Deliverables: </p> <ul> <li>Launch Kit</li> <li>Peer Review</li> </ul> <p>Resources:</p> <ul> <li>placeholder</li> </ul>"},{"location":"syllabus/teaching-philosophy/","title":"Teaching Philosophy","text":""},{"location":"syllabus/teaching-philosophy/#learning-by-building","title":"Learning by Building","text":"<p>My core belief is that you learn a practical skill like Generative AI not by listening to lectures, but by building things. You wouldn't learn to swim by reading a book about hydrodynamics; you learn by getting in the water. This course is our swimming pool. My role is less that of a traditional lecturer and more of a guide and a coach. I am here to provide the initial map, point you to the right tools, and help you navigate the inevitable challenges you'll encounter while building your project.</p>"},{"location":"syllabus/teaching-philosophy/#what-to-expect","title":"What to Expect","text":"<ol> <li> <p>Embrace Productive Discomfort: This field is changing weekly. No one has all the answers, and that's okay. We will treat this ambiguity not as a problem, but as an opportunity. A key skill you will develop is learning how to find solutions in a fast-moving environment where the documentation is always slightly out of date. This is the reality of working in technology today, and learning to be comfortable with being \"uncomfortable\" is a superpower.</p> </li> <li> <p>We are a Development Team: Think of our class not as a lecture hall, but as a startup's project room. I am the lead, and you are the development teams. Our class time will focus on project work, solving problems, and sharing what we've learned. Your success\u2014and a significant part of your grade\u2014will come from collaborating effectively with your team, managing your project professionally, and delivering a functional product.</p> </li> <li> <p>The \"Why\" Before the \"What\": We will always start with a real-world problem or a business case. Instead of me telling you about a concept like \"Retrieval-Augmented Generation,\" I will present you with a challenge: \"Our chatbot doesn't know anything about our company's latest products. How can we teach it?\" This problem-first approach ensures that every technical concept you learn is immediately grounded in a practical application.</p> </li> </ol>"},{"location":"syllabus/teaching-philosophy/#how-to-succeed-in-this-course","title":"How to Succeed in This Course","text":"<ul> <li>Be Curious and Proactive: When you encounter a problem, your first step should be to try and solve it. Tinker with the code, consult the documentation, and collaborate with your team. My role is to help you when you're truly stuck, not to provide immediate answers.</li> <li>Communicate Professionally: Use the tools we provide (like GitHub Issues) to document your work, ask questions, and track your progress. Clear communication is the foundation of successful teamwork.</li> <li>Be a Great Teammate: Your project's success depends on the collective effort of your group. Show up for your team, contribute your share, and be open to different ideas. We will have mechanisms in place to ensure individual contributions are recognized.</li> </ul> <p>Ultimately, my goal is for you to leave this course with more than just knowledge. I want you to leave with the confidence, skills, and experience to build and lead AI projects in your future career. I look forward to building alongside you this semester.</p>"},{"location":"syllabus/tools/","title":"Tools &amp; Setup","text":"<p>Understanding Our Digital Workspace</p> <p>Before we begin setting up our tools, it\u2019s important to understand the key components of our professional AI development environment. In this course, we are not just using a collection of random tools; we are integrating three core entities\u2014Google's AI platform, the GitHub ecosystem, and secure API keys\u2014to create a digital workspace that mirrors how modern tech teams operate.</p>"},{"location":"syllabus/tools/#google","title":"Google","text":"<p>More Than a Search Engine \ud83e\udde0</p> <p>For many, Google is a search bar. For businesses and developers, it's a global technology powerhouse. Think of it this way: if Google Search is the public library, Google Cloud and AI Studio are the high-tech industrial workshops.</p> <ul> <li>Google Cloud Platform (GCP): This is the enterprise-facing side of Google. It provides companies with powerful, on-demand computing resources, from servers and storage to databases and cutting-edge AI services.</li> <li>Gemini &amp; Google AI Studio: This is our direct access to Google's state-of-the-art Generative AI models. Gemini is the family of models (the \"engine\"), and AI Studio is the web-based interface that lets us experiment and prototype with that engine.</li> </ul> <p>In this course, you'll use these professional-grade tools to build your applications, giving you direct experience with the same technology that powers major global companies.</p>"},{"location":"syllabus/tools/#github","title":"GitHub","text":"<p>Your Project's Digital Headquarters \ud83c\udfdb\ufe0f</p> <p>GitHub is far more than just a place to store code. It's a multifaceted platform that will serve as the entire headquarters for your team project. It has four key functions:</p> <ul> <li>The Vault &amp; Bookkeeper: At its core, a GitHub Repository is a secure, cloud-based location for all your project's files (code, documents, images). It keeps a complete history of every change, so you can never lose your work.</li> <li>The Meeting Room (Collaboration): GitHub is where your team communicates. You'll use comments on Issues and code, Discussion boards, and Wikis to have transparent, documented conversations, ensuring everyone is on the same page.</li> <li>The Control Room (Project Management): This is where you manage your work. You'll use Issues to define tasks and Projects (Kanban boards) to track their progress from \"To Do\" to \"Done.\"</li> <li>The Workshop (Infrastructure): This is where the actual work gets done. You'll use powerful, integrated tools like GitHub Codespaces (your cloud-based computer) and GitHub Secrets (a secure vault for passwords and API keys).</li> </ul> <p>Mastering this platform is a highly valuable professional skill. A well-managed GitHub repository is one of the most powerful items you can have in your portfolio to demonstrate your technical and project management abilities to employers.</p>"},{"location":"syllabus/tools/#api-keys","title":"API Keys","text":"<p>The Secure Keys to the Digital Kingdom \ud83d\udd11</p> <p>You will soon create and use an API key. It's crucial to understand what it is and why it must be protected. An API key is like a combination of a secure password and a credit card for the digital world.</p> <ul> <li>What is it? It's a unique, secret string of characters that you send along with any request to an AI model provider (like Google, OpenAI, or Anthropic).</li> <li>Why is it required?<ol> <li>Authentication (It's your password): It proves to the service that you are who you say you are and that you have permission to use their powerful AI models.</li> <li>Billing &amp; Tracking (It's your credit card): It's how these companies track your usage. In the real world, this usage is tied to the key for billing. (In this course, we will stay within free limits).</li> </ol> </li> <li>Why must it be kept SECRET? If someone else gets your API key, they can use the AI service on your account. This is why we will never paste a key directly into our code. Instead, we will use professional tools like GitHub Secrets and Colab Secrets to store them securely.</li> </ul>"},{"location":"syllabus/tools/#a-step-by-step-guide","title":"A Step-by-Step Guide","text":""},{"location":"syllabus/tools/#get-your-github-account","title":"Get Your GitHub Account","text":"<p>Before we begin, ensure you have a GitHub account. This single account is your passport to a professional suite of collaboration tools we will use all semester, including repositories, project boards, and our cloud-based coding environment, Codespaces.</p>"},{"location":"syllabus/tools/#create-your-personal-setup-repository","title":"Create Your Personal Setup Repository","text":"<p>For this initial setup exercise, each student will create their own individual repository from the course template. This is to ensure everyone completes the setup process and that API keys are kept secure and private. You will create a separate, shared team repository later in the course.</p> <ol> <li>Navigate to the Template: Go to the template repository: <code>https://github.com/genaiforbusiness/chatbox</code>.</li> <li>Create Your Repository: Click \"Use this template\" to create a new repository.</li> <li>Configure Your New Repository:<ul> <li>Owner: Ensure the repository is owned by your personal GitHub account.</li> <li>Repository name: Use the format <code>dev-setup-&lt;your-github-username&gt;</code> (e.g., <code>dev-setup-j-doe</code>).</li> <li>Visibility: You must select Private.</li> </ul> </li> <li>Add the Instructor: You must add the instructor as a collaborator so your setup can be verified. Go to Settings &gt; Collaborators and teams, and invite the instructor using the username <code>midhubalan</code>.</li> </ol>"},{"location":"syllabus/tools/#get-secure-api-key","title":"Get &amp; Secure API Key","text":"<p>An API key is a secret password that allows your application to communicate with Google's AI models. This is a critical step that requires careful attention.</p> <ol> <li>Understand the Process: First, read the official Google documentation on how to get an API key. This guide explains the process you are about to follow.</li> <li>Generate Your Key: Go to Google AI Studio and follow the steps to create your API key.</li> <li>\u26a0\ufe0f Save Your Key Immediately: The API key is only shown to you once. Copy the key immediately and paste it into a temporary, secure location. If you lose it before saving it in the next steps, you will need to create a new one.</li> </ol>"},{"location":"syllabus/tools/#configure-and-test-github-codespaces","title":"Configure and Test GitHub Codespaces","text":"<p>Now, we will securely store your key in your project's primary development environment.</p> <ol> <li>Understand Secrets: First, read the official GitHub documentation on managing secrets for your Codespaces.</li> <li>Store the Secret: In your team's GitHub repository, navigate to Settings &gt; Secrets and variables &gt; Codespaces. Create a new repository secret with the name <code>GOOGLE_API_KEY</code> and paste your API key as the value.</li> <li>Launch and Test:<ul> <li>Create a new Codespace on your <code>main</code> branch.</li> <li>Once the environment is running, follow the instructions in the <code>README.md</code> file to run a test script. This will confirm that your Codespace can securely access the API key and communicate with the Gemini API.</li> </ul> </li> </ol> <p>Custom GitHub Codespace Specification \ud83d\udca1</p> <p>The GitHub Codespace you launch for this course isn't a generic environment; it's a custom-built workspace specifically for our project, defined by a <code>devcontainer</code> specification in your repository.</p> <p>Think of it as a virtual computer where we've already installed all the special tools and files you'll need, so you can start building right away without worrying about setup.</p> <p>Key customizations pre-loaded for you include:</p> <ul> <li>UV: A modern, high-speed Python package installer and resolver.</li> <li>Gemini CLI: A command-line interface for interacting directly with the Gemini family of models.</li> <li>A Starter Streamlit App: A barebones application with the Google Gemini SDK already included as a dependency, giving you a ready-made foundation for your project.</li> </ul>"},{"location":"syllabus/tools/#configure-and-test-google-colab","title":"Configure and Test Google Colab","text":"<p>Finally, we will configure the environment used for our individual labs.</p> <ol> <li>Understand Colab Secrets: First, read the official Colab documentation on its Secrets feature.</li> <li>Open the Test Notebook: Open the <code>gemini.ipynb</code> in Colab. </li> <li>Store the Secret: In the notebook's side panel, click the key icon (\ud83d\udd11) to open the \"Secrets\" tab. Create a new secret with the name <code>GOOGLE_API_KEY</code> and paste your API key as the value. Ensure notebook access is enabled.</li> <li>Run the Cells: Execute the pre-populated code cells in the notebook. A successful run will confirm your Colab environment is also correctly configured.</li> </ol>"},{"location":"syllabus/ai-project/","title":"The AI Proof-of-Concept Challenge","text":""},{"location":"syllabus/ai-project/#the-mission-from-idea-to-investment","title":"The Mission: From Idea to Investment","text":"<p>Welcome to the central experience of this course: the AI Proof-of-Concept (PoC) Challenge.</p> <p>For the rest of this semester, your group will operate as a cross-functional startup team within a larger organization. Your mission is to identify a significant business challenge, design an innovative solution using Generative AI, and build a working prototype to prove its value.</p> <p>The ultimate goal is to create a compelling \"launch kit\" for your PoC and reflect on your journey in a final team interview. This project is a realistic simulation of how AI products are conceived, built, championed, and reflected upon in a professional environment.</p>"},{"location":"syllabus/ai-project/#the-process-a-four-stage-journey","title":"The Process: A Four-Stage Journey","text":"<p>Your team will progress through a structured, four-stage development lifecycle that mirrors modern agile practices. Each stage has specific tasks, goals, and deliverables designed to keep you on track.</p> <ol> <li>Stage 1: Ideation &amp; Planning: Define a valuable problem and create a clear, actionable plan and governance model.</li> <li>Stage 2: MVP Development: Build the core, functional version of your chatbot\u2014your Minimum Viable Product.</li> <li>Stage 3: Experimentation &amp; Refactoring: Systematically test, evaluate, and improve your MVP through experimentation and code refactoring.</li> <li>Stage 4: The Launch &amp; Reflection: Create a professional \"launch kit\" for your PoC and reflect on your learning journey in a team interview.</li> </ol> <p>A detailed week-by-week roadmap with specific tasks and supporting resources is available in the Course Schedule.</p>"},{"location":"syllabus/ai-project/#evaluation-the-project-rubric","title":"Evaluation: The Project Rubric","text":"<p>Your project is worth 45 points of your final grade. It will be evaluated holistically based on the quality of your final prototype, your strategic thinking, and the professionalism of your launch materials. The following rubric will be used for the formal evaluation.</p> Criteria Exemplary (A-Level) Proficient (B-Level) Developing (C-Level) Unsatisfactory (D/F-Level) Business Value &amp; Problem Framing(10 Points) 9-10 PointsProject addresses a well-defined, significant business problem with a compelling value proposition. The solution is thoughtfully tailored to a clearly identified user. The project includes a thoughtful and realistic roadmap for future improvements. 8 PointsProject addresses a relevant business problem and the value proposition is clear. The solution is appropriate for the target user, though the connection could be stronger. 6-7 PointsA business problem is identified, but its significance is unclear, or the AI solution is a weak fit. The target user is defined but the solution is generic. 0-5 PointsProject lacks a clear business purpose or solves a trivial problem. The value proposition is missing or confusing. Technical Execution &amp; Refinement(15 Points) 14-15 PointsAchieves all \"Proficient\" criteria AND successfully implements an optional advanced feature (e.g., RAG, Agentic tools) in a well-integrated and effective manner. 12-13 PointsApplication MVP is functional, robust, and well-refactored. Code is clean and organized. The team's <code>EXPERIMENT_LOG.md</code> demonstrates a thoughtful process of evaluation and improvement. 8-11 PointsApplication's core functionality works but may be unreliable. The code is disorganized, and the experimentation process lacks depth or clear evaluation. 0-7 PointsApplication is non-functional or fails to meet core requirements. The code is difficult to understand or run. User Experience (UX) &amp; Design(10 Points) 9-10 PointsThe user interface is intuitive, polished, and professional. The user journey is logical and seamless. The chatbot's persona and tone are consistent and well-suited to the application's purpose. 8 PointsThe user interface is clean and functional. The application is easy to use with minimal instruction. The chatbot's persona is defined but may have minor inconsistencies. 6-7 PointsThe user interface is functional but may be confusing or unappealing. The user may struggle with some tasks. The application lacks a clear design or persona. 0-5 PointsThe user interface is difficult to navigate or non-functional. The user experience is frustrating. No thought was given to design or persona. PoC Launch Kit &amp; Reflection(10 Points) 9-10 PointsThe video pitch is highly persuasive and professional. The quick-start guide is exceptionally clear and user-friendly. The team demonstrates deep, critical self-reflection during the final interview. 8 PointsThe video pitch is effective and clear. The quick-start guide is functional and easy to follow. The team shows thoughtful reflection during the interview. 6-7 PointsThe video is unclear or unpersuasive. The guide is confusing. The team's reflection in the interview is superficial. 0-5 PointsThe launch kit is unprofessional, incomplete, or fails to communicate the project's purpose. The team is unprepared for the interview."},{"location":"syllabus/ai-project/#why-this-rubric-connecting-your-work-to-real-world-success","title":"Why This Rubric? Connecting Your Work to Real-World Success","text":"<p>The project rubric is more than a grading tool; it\u2019s an \"explicit nudge\" designed to guide you toward building a product that could succeed outside the classroom. The success of a real-world AI project relies on far more than its technical correctness. It requires a strategic vision, a deep understanding of its users, and a clear plan for adoption and growth.</p> <p>Here\u2019s how the rubric pushes you to develop these critical skills:</p> <ul> <li>It forces you to think like a strategist. The <code>Business Value</code> criterion demands that you address a hard problem for the right user group. By requiring a roadmap, it pushes you to think about the project's continual evolution.</li> <li>It makes you accountable for adoption. The <code>User Experience</code> criterion directly measures how well you've designed a solution that minimizes disruption for the end-user. An intuitive and polished product is one that people will actually want to use.</li> <li>It prepares you to be an organizational champion. The <code>PoC Launch Kit</code> requires you to communicate your project's value not as an academic exercise, but as a real product ready for users. The <code>Team Interview</code> challenges you to articulate your learnings, a key skill for any leader.</li> </ul> <p>By focusing on these four areas, the rubric ensures you are not just learning to code an AI, but learning to build an AI product.</p>"},{"location":"syllabus/ai-project/bootstrap/","title":"Project Bootstrap","text":""},{"location":"syllabus/ai-project/bootstrap/#team-repository-setup","title":"Team &amp; Repository Setup","text":""},{"location":"syllabus/ai-project/bootstrap/#objective","title":"Objective \ud83c\udfaf","text":"<p>The goal of this first task is to get your team organized and set up the professional collaboration environment we will use for the entire semester: GitHub. This exercise will ensure that every team member has a working GitHub account and understands the basic workflow of making a contribution to a shared project. We will use a course template that includes pre-configured settings for our development environment.</p>"},{"location":"syllabus/ai-project/bootstrap/#due-date-grading","title":"Due Date &amp; Grading","text":"<ul> <li>Due: See Canvas.</li> <li>Grading: This is a Complete / Incomplete task that's an integral part of your project. To receive a \"Complete\" grade for this assignment, every team member must successfully complete their part.</li> </ul>"},{"location":"syllabus/ai-project/bootstrap/#step-by-step-instructions","title":"Step-by-Step Instructions","text":""},{"location":"syllabus/ai-project/bootstrap/#form-your-team","title":"Form Your Team","text":"<p>Decide on your project team of 3-4 members. Once your team is formed, designate one person to be the \"Repo Lead\" for the initial setup.</p>"},{"location":"syllabus/ai-project/bootstrap/#create-the-repository","title":"Create the Repository","text":"<p>Repo Lead's Task</p> <p>The Repo Lead will create the shared project space by using a course template.</p> <ol> <li>Navigate to the Template: Go to the template repository at https://github.com/genaiforbusiness/chatbox.</li> <li>Use the Template: Click the green \"Use this template\" button and select \"Create a new repository\".</li> <li>Configure Your New Repository:<ul> <li>Owner: Ensure the repository is owned by your personal GitHub account.</li> <li>Repository name: Use the format <code>ai-poc-team-&lt;your-team-name&gt;</code> (e.g., <code>ai-poc-team-innovators</code>).</li> <li>Visibility: You must select Private. This is a critical step.</li> <li>Click \"Create repository\".</li> </ul> </li> <li>Add Collaborators: Add Collaborators: In your newly created repository, go to Settings &gt; Collaborators and teams. You must add two types of collaborators:<ul> <li>Your Teammates: Click \"Add people\" and add each of your teammates using their GitHub username.</li> <li>The Instructor: You must also add the instructor so your work can be viewed and graded. The instructor's GitHub username is <code>midhubalan</code>.</li> </ul> </li> </ol> <p>Your teammates and the instructor will need to accept the email invitation to gain access. This step is critical for grading.</p>"},{"location":"syllabus/ai-project/bootstrap/#clone-the-repository","title":"Clone the Repository","text":"<p>Clone the Repository: Everyone's Task</p> <p>Now, each team member needs to create a local copy of the project on their computer (or in their Codespace).</p> <ol> <li>Navigate to the main page of the team's new repository (not the template).</li> <li>Click the green \"&lt;&gt; Code\" button.</li> <li>Copy the URL provided (use the HTTPS option).</li> <li>Open a terminal or command prompt and run the following command:     <pre><code>git clone &lt;paste-the-repository-url-here&gt;\n</code></pre></li> </ol>"},{"location":"syllabus/ai-project/bootstrap/#create-your-profile-page","title":"Create Your Profile Page","text":"<p>Create Profile: Everyone's Task</p> <p>This is your first individual contribution to the project.</p> <ol> <li>Navigate into the new project folder you just cloned.</li> <li>Create a new file. The filename must be your exact GitHub username followed by <code>.md</code>. For example, if your username is <code>j-doe</code>, the file must be named <code>j-doe.md</code>.</li> <li>Open the file and add the following information using Markdown formatting:     <pre><code># [Your Name]\n\n**Major:** [Your Major]\n\n**What I want to learn in this course:** [A brief sentence about your goals]\n\n**A fun fact about me:** [Something interesting!]\n</code></pre></li> <li>Save the file.</li> </ol>"},{"location":"syllabus/ai-project/bootstrap/#commit-and-push-your-changes","title":"Commit and Push Your Changes.","text":"<p>Commit and Push: Everyone's Task</p> <p>Finally, you will save your new file to the shared repository on GitHub using a three-step command-line process.</p> <ol> <li>Add the file: Tell Git to track your new file.     <pre><code>git add &lt;your-github-username&gt;.md\n</code></pre></li> <li>Commit the file: Save a snapshot of your changes with a descriptive message.     <pre><code>git commit -m \"feat: Add profile for &lt;your-github-username&gt;\"\n</code></pre></li> <li>Push the file: Send your saved changes up to the shared GitHub repository.     <pre><code>git push\n</code></pre></li> </ol>"},{"location":"syllabus/ai-project/bootstrap/#submission-verification","title":"Submission &amp; Verification","text":"<ul> <li>Submission: The Repo Lead will submit a single link to the team's GitHub repository on Canvas.</li> <li>Verification: Before submitting, check the following:<ul> <li>\u2705 The repository is private.</li> <li>\u2705 All team members are listed as collaborators under Settings.</li> <li>\u2705 Each team member has successfully created and pushed their own unique <code>&lt;username&gt;.md</code> file to the repository.</li> <li>\u2705 All team members appear under the \"Contributors\" tab on the main repository page.</li> </ul> </li> </ul>"},{"location":"syllabus/ai-project/charter/","title":"The Project Charter","text":""},{"location":"syllabus/ai-project/charter/#objective","title":"Objective \ud83c\udfaf","text":"<p>The goal of this charter is to move beyond \"vibe coding\" and establish a professional foundation for your project. Before you write application code, your team must define your project as a system of activities. This involves creating a compelling business case, defining your product's features from a user's perspective, and establishing a governance framework for how your team will operate.</p> <p>This charter is the strategic blueprint for your proof-of-concept. A well-thought-out charter is the first and most critical step in convincing a funding committee that your team is worth investing in.</p>"},{"location":"syllabus/ai-project/charter/#due-date-evaluation","title":"Due Date &amp; Evaluation","text":"<ul> <li>Due: See Canvas.</li> <li>Evaluation: This deliverable is a major component of your Group Project grade. It will be evaluated on the thoughtfulness and clarity of your planning, primarily under the \"Business Value &amp; Problem Framing\" criterion in the main project rubric.</li> </ul>"},{"location":"syllabus/ai-project/charter/#components-of-the-charter","title":"Components of the Charter","text":"<p>Your charter consists of three distinct markdown files that must be completed and committed to your team's GitHub repository.</p>"},{"location":"syllabus/ai-project/charter/#the-business-case","title":"The Business Case","text":"<p><code>README.md</code> - The \"Why\"</p> <p>Your <code>README.md</code> is the front page of your project. It must clearly and concisely communicate the business purpose of your application. It should be written for a stakeholder who has five minutes to understand what you're doing and why it matters.</p> <p>It must include:</p> <ul> <li>Mission Statement: A single, compelling sentence that summarizes your project's purpose.</li> <li>The Problem: A clear description of the specific, high-value business problem or user pain point you are solving.</li> <li>Our Solution: A high-level overview of how your AI application will address this problem.</li> <li>Target User: A brief description of the primary user you are building this for.</li> </ul>"},{"location":"syllabus/ai-project/charter/#the-product-features","title":"The Product Features","text":"<p><code>USER_STORIES.md</code> - The \"What\"</p> <p>This document translates your business idea into a prioritized list of product features, always from the perspective of your user.</p> <p>It must include:</p> <ul> <li>User Persona: A detailed description of your primary target user. Include their role, goals, and frustrations related to the problem you're solving.</li> <li>Epics &amp; User Stories: A list of major features (Epics) broken down into specific user needs (User Stories). All user stories must follow the format:     &gt; \"As a [type of user], I want to [perform some action] so that I can [achieve some goal].\"</li> </ul>"},{"location":"syllabus/ai-project/charter/#the-operating-model","title":"The Operating Model","text":"<p><code>GOVERNANCE.md</code> - The \"How\"</p> <p>This is the most important document for defining your team's process. It demonstrates that you have thought carefully about how to manage your project and collaborate effectively.</p> <p>It must include:</p> <ul> <li>Team Roles &amp; Responsibilities: Define the initial roles for your team members (e.g., Project Lead for this stage, Technical Lead, etc.). These roles can and should rotate in later stages.</li> <li>Collaboration Protocol: Detail your team's specific rules for using GitHub. How will you use Issues, Projects, and Discussions? What is your meeting schedule? How will you ensure all work is visible?</li> <li> <p>AI Usage Strategy: This is a critical section. Answer the following:</p> <p>Task Delegation:</p> <ul> <li>Which project activities are suited for ad-hoc assistance from an AI (e.g., brainstorming, debugging a snippet)?</li> <li>Which activities are candidates for systematic automation using AI (e.g., generating unit tests, summarizing notes)?</li> </ul> <p>Collaboration Protocol:</p> <ul> <li>How will you strategically use AI as a collaborator?</li> <li>What is your policy for verifying and citing AI contributions?</li> </ul> </li> <li> <p>Decision-Making Framework: Briefly describe how your team will make decisions and resolve disagreements about features or technical approaches to ensure the project keeps moving forward.</p> </li> <li>Decision-Making Framework: Briefly describe how your team will make decisions and resolve disagreements about features or technical approaches to ensure the project keeps moving forward.</li> </ul>"},{"location":"syllabus/ai-project/individual-contribution/","title":"How to Excel: A Guide to Individual Contribution","text":"<p>Your 10-point Individual Contribution grade is designed to recognize and reward your personal effort, proactivity, and teamwork. It's based on observable evidence in GitHub and feedback from your peers. Here are some practical tips to make sure your hard work is visible and valued.</p>"},{"location":"syllabus/ai-project/individual-contribution/#task-management-completion","title":"Task Management &amp; Completion \ud83d\udccb","text":"<p>This is about proving you did the work. The key is making your contributions visible and consistent.</p> <ul> <li>If it's not on GitHub, it didn't happen. Even if your task is research, design, or writing, document it. Create an issue, do the work, and then post your findings or the document as a comment in the issue before closing it.</li> <li>Break down large tasks. Don't let a single, massive issue stay \"In Progress\" for two weeks. Work with your team to break down large features into smaller, manageable tasks that can be completed in a day or two. This shows consistent progress.</li> <li>Link your code to your work. When you commit code that addresses a specific task, reference the issue number in your commit message (e.g., <code>git commit -m \"feat: Add user login form. Closes #12\"</code>). This automatically links your contribution to the plan.</li> </ul>"},{"location":"syllabus/ai-project/individual-contribution/#communication-proactivity","title":"Communication &amp; Proactivity \ud83d\udcac","text":"<p>This is about demonstrating you're an engaged and forward-thinking team member, not just a task-taker.</p> <ul> <li>\"Think out loud\" on GitHub. Instead of working silently, post updates or questions in your assigned issues. A comment like, \"I'm starting on this now, my initial approach will be to...\" or \"I'm stuck on this error, has anyone seen this before?\" shows active engagement.</li> <li>Comment on others' work. Your role isn't just to complete your own tasks. Read the issues assigned to your teammates. Ask clarifying questions. Offer a helpful suggestion. A simple comment shows you're thinking about the project as a whole.</li> <li>Create new issues. Don't just wait for work to be assigned. If you find a bug, think of a small improvement, or identify a missing piece of documentation, create a new, well-written issue for it. This is the clearest sign of a proactive contributor.</li> </ul>"},{"location":"syllabus/ai-project/individual-contribution/#team-citizenship-reliability","title":"Team Citizenship &amp; Reliability \ud83e\udd1d","text":"<p>This portion of your grade is based on confidential peer evaluations. Your teammates will be asked about your reliability, preparation, and support for the team.</p> <ul> <li>Be consistently reliable. Do what you say you'll do. If you're running late on a task, communicate it to your team early. Show up to team meetings on time and prepared.</li> <li>Focus on constructive feedback. When discussing a teammate's work, focus on the work, not the person. Frame your feedback positively: \"This is a great start. What if we also added a check for X to make it more robust?\"</li> <li>Share the \"un-glamorous\" work. Every project has necessary but less exciting tasks like writing documentation, testing, or cleaning up code. Volunteer to take on your share of this work. Your teammates will notice and appreciate it.</li> </ul>"},{"location":"syllabus/ai-project/launch-kit/","title":"The PoC Launch Kit &amp; Team Reflection","text":""},{"location":"syllabus/ai-project/launch-kit/#objective","title":"Objective \ud83c\udfaf","text":"<p>Your final task is not a traditional presentation. Instead, your team will create a professional \"launch kit\" for your proof-of-concept, designed to attract new users and convince stakeholders of your project's value. This is your chance to showcase your work in a format that mirrors a real-world product launch.</p> <p>This will be followed by a reflective team interview to synthesize your learning and critically examine your journey through the course.</p>"},{"location":"syllabus/ai-project/launch-kit/#due-date-evaluation","title":"Due Date &amp; Evaluation","text":"<ul> <li>Due: See Canvas.</li> <li>Evaluation: This \"launch kit\" and the quality of your final application constitute the final assessment for your 45-point Group Project grade. Your work will be evaluated using the main project rubric, particularly the criteria for <code>Technical Execution</code>, <code>UX &amp; Design</code>, and <code>Final Presentation &amp; Demonstration</code>.</li> </ul>"},{"location":"syllabus/ai-project/launch-kit/#components-of-the-final-deliverable","title":"Components of the Final Deliverable","text":""},{"location":"syllabus/ai-project/launch-kit/#the-video-pitch","title":"The Video Pitch \ud83c\udfa5","text":"<p>Your goal is to create a compelling, marketing-style video. This is not a technical deep-dive; it's a pitch designed to woo potential users and sponsors. It should be concise, professional, and persuasive. The recommended length is 3 to 5 minutes.</p> <ul> <li>Content: The video must clearly communicate the problem you're solving, demonstrate your application's core features in action, and articulate its unique value proposition.</li> <li>AI-Powered Creativity: You are explicitly encouraged to use AI in the creation of this video. This is a final opportunity to practice your \"AI as a collaborator\" skills for tasks like writing the script, creating a voiceover, generating background music, or even producing animated visuals.</li> <li>Submission: Upload your video to a platform like YouTube (as an \"Unlisted\" video) or Vimeo and place the link prominently at the top of your project's <code>README.md</code> file.</li> </ul>"},{"location":"syllabus/ai-project/launch-kit/#the-quick-start-guide","title":"The Quick-Start Guide \ud83d\udcd6","text":"<p><code>QUICK_START.md</code> </p> <p>This is a practical, user-facing document. It should enable a brand-new user to understand your application's purpose and successfully complete the core tasks you identified in your initial project charter.</p> <ul> <li>Content: The guide should include a brief overview of the application and simple, step-by-step instructions for using 2-3 of its key features.</li> <li>Audience: Write for a non-technical user. Avoid jargon and focus on clarity. Your goal is to make a first-time user successful and confident.</li> <li>Submission: Create a new file named <code>QUICK_START.md</code> in the main directory of your team's GitHub repository.</li> </ul>"},{"location":"syllabus/ai-project/launch-kit/#the-peer-review-process","title":"The Peer Review Process \ud83e\udd1d","text":"<p>Giving and receiving professional, constructive feedback is a critical skill.</p> <ul> <li>Process: During the closing week of the course, each student will be assigned two other teams' \"launch kits\" to review. You will watch their video, read their quick-start guide, and use a provided rubric to offer feedback on the clarity, persuasiveness, and professionalism of their work.</li> <li>Grading: Thoughtful completion of the peer review process is a component worth 5 points of your final grade.</li> </ul>"},{"location":"syllabus/ai-project/launch-kit/#the-reflective-team-interview","title":"The Reflective Team Interview \ud83e\udde0","text":"<p>This is a 15-minute in-class round table discussion. This is not a defense of your project, but a structured, collaborative conversation to reflect on your team's learning journey.</p> <ul> <li>Purpose: The goal is to foster self-reflection. We'll discuss your team's process, how your perspectives on AI have evolved, and whether AI ultimately helped or hindered your critical thinking and learning.</li> <li>Preparation: Your team will be provided with the interview questions in advance to help you prepare for a thoughtful discussion (See examples below).</li> </ul> <p>Interview Questions </p> <p>On Process &amp; Collaboration</p> <ul> <li>Looking back at your <code>GOVERNANCE.md</code> from your project charter, which part of your planned process was most successful in practice, and which part broke down first? Why?</li> <li>Describe a specific moment where using an AI tool as a collaborator (like GitHub Copilot or a process prompt) significantly changed your team's direction or saved you a huge amount of time.</li> <li>What was the most challenging aspect of managing an AI project compared to a traditional software project?</li> </ul> <p>On Technical &amp; Conceptual Understanding</p> <ul> <li>What is one major misconception you had about Generative AI at the start of the course that has now changed?</li> <li>If you had to explain the concept of Retrieval-Augmented Generation (RAG) to a non-technical manager in 30 seconds, how would you do it now?</li> <li>Beyond your own project, what do you now understand about the limitations of current LLMs that you didn't appreciate before?</li> </ul> <p>On Critical Thinking &amp; Future Outlook</p> <ul> <li>Where did your team find the line between using AI as an effective tool and letting it hinder your own problem-solving or critical thinking? Can you give a specific example?</li> <li>After building your own AI application, what are you most optimistic about regarding AI's future in business? What are you most concerned about?</li> <li>What's one piece of advice you would give to a new team just starting this course and project?</li> </ul> <p>The Personal Journal \ud83d\udcd3</p> <p>The best interviews draw on specific, personal experiences. To prepare for this reflective conversation, I strongly encourage you to keep a simple, informal personal journal throughout the semester. This is not a graded assignment, but a tool for your own benefit.</p> <p>After a challenging \"builder session\" or an insightful \"learner session,\" take just five minutes to jot down a few thoughts. A few sentences each week will provide you with a rich set of personal reflections to draw upon, leading to a much more insightful final conversation.</p> <p>Some prompts to consider for your journal entries:</p> <ul> <li>What was the most confusing concept this week, and what finally made it \"click\"?</li> <li>Describe a specific prompt that failed spectacularly. What did you learn from the failure?</li> <li>Did using an AI tool today feel like a superpower or a crutch? Why?</li> <li>What was a moment of success or friction within your team this week? How did you handle it?</li> <li>What's one thing you believe about AI now that you didn't believe last week?</li> </ul>"},{"location":"syllabus/ai-project/mvp/","title":"The Minimum Viable Product (MVP) &amp; AI Collaboration Log","text":""},{"location":"syllabus/ai-project/mvp/#objective","title":"Objective \ud83c\udfaf","text":"<p>This deliverable assesses not only your working MVP code but also the intentionality of your process. You will demonstrate how your team is executing its governance plan and strategically collaborating with AI tools and prompts to build your application.</p>"},{"location":"syllabus/ai-project/mvp/#due-date-evaluation","title":"Due Date &amp; Evaluation","text":"<ul> <li>Due: See Canvas.</li> <li>Evaluation: This deliverable is a major component of your Group Project grade. The evaluation is structured to reward both solid, methodical improvement and the implementation of advanced capabilities.</li> </ul>"},{"location":"syllabus/ai-project/mvp/#components-of-the-mvp-deliverable","title":"Components of the MVP Deliverable","text":"<p>Your MVP submission consists of three parts, all of which must be present in your team's GitHub repository.</p>"},{"location":"syllabus/ai-project/mvp/#the-functional-mvp-code","title":"The Functional MVP Code","text":"<ul> <li>Requirement: Your core application code must be functional and demonstrate the primary user stories from your charter.</li> </ul>"},{"location":"syllabus/ai-project/mvp/#the-mvp-retrospective","title":"The MVP Retrospective","text":"<p><code>MVP_RETROSPECTIVE.md</code></p> <ul> <li>Requirement: Create a new file named <code>MVP_RETROSPECTIVE.md</code>. In it, reflect on your process by answering:<ol> <li>Governance in Action: How well did our team follow the collaboration protocol in our <code>GOVERNANCE.md</code>? What worked? What will we change?</li> <li>Challenges &amp; Learnings: What was the biggest technical or conceptual challenge we faced in this phase, and how did we overcome it?</li> </ol> </li> </ul>"},{"location":"syllabus/ai-project/mvp/#the-ai-collaboration-log","title":"The AI Collaboration Log","text":"<p><code>AI_COLLABORATION_LOG.md</code></p> <ul> <li>Requirement: Create a new file named <code>AI_COLLABORATION_LOG.md</code> to document your team's interaction with AI. This log must contain three sections:<ul> <li>Section 1: Tool Manifest: List the 2-3 primary AI tools your team used (e.g., GitHub Copilot, Gemini in AI Studio, Gemini CLI). For each tool, briefly describe why you used it and for what types of tasks.</li> <li>Section 2: Application Prompts: Document the 2 most critical prompts that are used inside your application. For each, provide the full prompt text and a brief rationale for its design.</li> <li>Section 3: Process Prompts: Provide 1-2 examples of high-impact prompts you used to help with the process of building your project (e.g., a prompt for brainstorming, summarizing research, or generating user stories).</li> </ul> </li> </ul>"},{"location":"syllabus/ai-project/peer-review/","title":"Peer Review Guide: The PoC Launch Kit","text":""},{"location":"syllabus/ai-project/peer-review/#objective","title":"Objective \ud83c\udfaf","text":"<p>The goal of this peer review is to practice the essential professional skill of giving and receiving constructive feedback. As you prepare to enter the business world, you will constantly be asked to evaluate the work of your colleagues and, in turn, have your work evaluated by them.</p> <p>This exercise provides a structured opportunity to:</p> <ol> <li>Refine your critical eye by analyzing how other teams approached the same challenge.</li> <li>Learn from your peers by seeing their unique solutions and communication styles.</li> <li>Practice delivering feedback that is specific, actionable, and professional.</li> </ol>"},{"location":"syllabus/ai-project/peer-review/#timeline-logistics-grading","title":"Timeline, Logistics, &amp; Grading","text":"<ul> <li>Due Date: See Canvas.</li> <li>Process: During the last week of class, each student will be randomly assigned two other teams' projects to review. Relevant links will be provided in Canvas.</li> <li>Submission: You will submit your feedback for each team via Canvas. The feedback will be shared with the reviewed team after grades are finalized.</li> <li>Grading: This is an individual task, but you are encouraged to discuss it as a team. Each student who submits two thoughtful reviews will receive 5 points (on a Complete/Incomplete basis) toward their final course grade.</li> </ul>"},{"location":"syllabus/ai-project/peer-review/#the-review-framework","title":"The Review Framework","text":"<p>For each of the two teams you are assigned, you must review their \"Launch Kit\" (their video and their quick-start guide) and answer the following questions in the submission form.</p> <p>Instructions: Use this rubric to evaluate each of the two \"Launch Kits\" assigned to you. The goal is to provide specific, constructive feedback that will help the other team understand what they did well and how they can improve. Fill out one of these for each team you review and submit it via the provided form.</p>"},{"location":"syllabus/ai-project/peer-review/#the-video-pitch","title":"The Video Pitch","text":"<p>10 Points Total</p> Criteria Excellent Good Needs Improvement Clarity of Purpose(4 pts) 4 ptsThe business problem and the AI solution were explained with exceptional clarity. The video's core message was immediately understandable. 3 ptsThe problem and solution were clear, but could have been explained more concisely or directly. 1-2 ptsThe purpose of the product was confusing or not well-defined in the video. Persuasiveness(4 pts) 4 ptsThe video was highly compelling and effectively demonstrated the product's value. It made a strong case for user adoption or sponsorship. 3 ptsThe video showed the product's value, but the \"wow\" factor or call to action could have been stronger. 1-2 ptsThe value proposition was weak or unclear. It was not persuasive. Professionalism(2 pts) 2 ptsThe video was polished, with clear audio, good pacing, and professional-looking visuals and editing. 1 ptThe video was generally well-produced but had minor technical issues (e.g., inconsistent audio, awkward edits). 0 ptsThe video had significant technical issues that were distracting and made it hard to watch."},{"location":"syllabus/ai-project/peer-review/#written-feedback-for-the-video","title":"Written Feedback for the Video","text":"<ol> <li> <p>What was the single strongest element of their video pitch? (Provide a specific, positive comment.)</p> </li> <li> <p>What is one specific, actionable suggestion you have for how they could make their video even more effective? (Provide a constructive suggestion for improvement.)</p> </li> </ol>"},{"location":"syllabus/ai-project/peer-review/#the-quick-start-guide","title":"The Quick-Start Guide","text":"<p><code>QUICK_START.md</code>: 10 Points Total</p> Criteria Excellent Good Needs Improvement Clarity for User(5 pts) 5 ptsThe guide was exceptionally easy to follow. The language was simple, clear, and perfectly suited for a non-technical first-time user. 3-4 ptsThe guide was mostly clear, but some steps might be slightly confusing or contained minor jargon. 1-2 ptsThe guide was difficult to understand, with missing steps, confusing language, or technical jargon. Effectiveness(5 pts) 5 ptsAfter reading the guide, I feel very confident I could use the app's key features successfully and without frustration. 3-4 ptsAfter reading the guide, I am fairly confident I could use the app, but I might still have a few questions. 1-2 ptsAfter reading the guide, I am still unsure how to perform the core tasks described."},{"location":"syllabus/ai-project/peer-review/#written-feedback-for-the-quick-start-guide","title":"Written Feedback for the Quick-Start Guide","text":"<ol> <li> <p>What was the most helpful instruction or feature in their guide? (Provide a specific, positive comment.)</p> </li> <li> <p>What one part of the guide was the most confusing or could be simplified for a new user? (Provide a constructive suggestion for improvement.)</p> </li> </ol>"},{"location":"syllabus/ai-project/peer-review/#rubric","title":"Rubric","text":"<p>What \"Complete\" Means (5/5 Points)</p> <p>To receive a \"Complete\" grade for this assignment, your submission must meet these three criteria:</p> <ul> <li>\u2705 Timeliness: You submitted reviews for both of your assigned teams by the deadline.</li> <li>\u2705 Completeness: You answered all of the scaled and open-ended questions in the review framework for each team.</li> <li>\u2705 Thoughtfulness: Your open-ended feedback is constructive and specific. A response like \"it was good\" is insufficient. Your feedback for each artifact must include at least one specific point of praise and one actionable suggestion for improvement.</li> </ul>"},{"location":"syllabus/ai-project/refactor/","title":"Experimentation Report","text":""},{"location":"syllabus/ai-project/refactor/#the-experimentation-refactoring-report","title":"The Experimentation &amp; Refactoring Report","text":""},{"location":"syllabus/ai-project/refactor/#objective","title":"Objective \ud83c\udfaf","text":"<p>The goal of this project phase is to move your application from a \"working prototype\" to a \"quality prototype.\" This is not about adding a long list of new features. Instead, the focus is on experimentation, evaluation, and refactoring.</p> <p>Your mission is to systematically test and improve your MVP. You will experiment with different prompt design strategies, evaluate the quality of your chatbot's responses, and refactor your code to make it more efficient and maintainable. This iterative process of improvement is a core part of professional AI development.</p>"},{"location":"syllabus/ai-project/refactor/#due-date-evaluation","title":"Due Date &amp; Evaluation","text":"<ul> <li>Due: See Canvas.</li> <li> <p>Evaluation: This deliverable is a major component of your Group Project grade. The evaluation is structured to reward both solid, methodical improvement and the implementation of advanced capabilities.</p> <ul> <li>Core Requirements (Proficient / \"B\" Grade): To pass this stage, all teams must submit a high-quality <code>EXPERIMENT_LOG.md</code> that demonstrates a thoughtful process of evaluation and refactoring of their MVP.</li> <li>Advanced Path (Exemplary / \"A\" Grade): To earn the highest marks, teams must complete the core requirements and successfully implement one of the optional advanced features (e.g., RAG, Context Caching, or a basic Agent with tools).</li> </ul> </li> </ul>"},{"location":"syllabus/ai-project/refactor/#components-of-the-deliverable","title":"Components of the Deliverable","text":""},{"location":"syllabus/ai-project/refactor/#1-the-refactored-application-code","title":"1. The Refactored Application Code","text":"<ul> <li>Requirement: Your team's GitHub repository should contain the improved version of your application code. The code should be cleaner, better organized, and more robust than your initial MVP.</li> </ul>"},{"location":"syllabus/ai-project/refactor/#2-the-experiment-log","title":"2. The Experiment Log","text":"<p><code>EXPERIMENT_LOG.md</code> </p> <p>This is the central document for this phase. Create a new file named <code>EXPERIMENT_LOG.md</code>. In it, your team must document at least three distinct experiments you ran to improve your application.</p>"},{"location":"syllabus/ai-project/refactor/#the-branching-workflow","title":"The Branching Workflow","text":"<p>For each experiment, you are required to use a separate Git branch to isolate your work. This is standard professional practice and allows you to experiment safely without breaking your main application.</p> <p>Branching Workflow</p> <ol> <li>Create a New Branch: Before you start, create and switch to a new branch with a descriptive name. <pre><code>git checkout -b experiment/new-prompt-design\n</code></pre></li> <li>Run Your Experiment: Make all your code changes and commits on this new branch.</li> <li>Evaluate &amp; Decide: Test your changes. If the experiment is successful and you want to keep it, merge it back into your <code>main</code> branch. <pre><code>git checkout main\ngit merge experiment/new-prompt-design\n</code></pre> If the experiment is unsuccessful, you can simply abandon the branch by switching back to <code>main</code>.</li> </ol>"},{"location":"syllabus/ai-project/refactor/#log-template","title":"Log Template","text":"<p>Use the following template in your <code>EXPERIMENT_LOG.md</code> for each of the three experiments:</p> <pre><code>---\n### Experiment #[1]: [A short, descriptive title for the experiment]\n\n* **Git Branch:** `[Name of the branch used for this experiment, e.g., experiment/new-prompt-design]`\n\n* **Hypothesis:** What were you trying to improve, and what change did you think would work? (e.g., \"We hypothesized that using a Chain-of-Thought prompt would improve the accuracy of our chatbot's math calculations.\")\n\n* **Method:** What specific change did you make on this branch? (e.g., \"We replaced our simple question prompt with a new prompt that included the phrase 'Let's think step by step'.\")\n\n* **Evaluation:** How did you measure the result? Was the new output better, worse, or the same? Provide examples of the \"before\" and \"after\" outputs.\n\n* **Decision:** Did you merge this branch into `main`? Why or why not?\n---\n</code></pre>"},{"location":"syllabus/ai-project/refactor/#3-optional-for-a-grade-advanced-feature-implementation","title":"3. (Optional for \"A\" Grade) Advanced Feature Implementation","text":"<ul> <li>Requirement: If your team is pursuing the \"Advanced Path,\" your application code must include a functional implementation of one of the following:<ul> <li>Retrieval-Augmented Generation (RAG): The ability to answer questions from a specific document.</li> <li>Context Caching: A mechanism to intelligently store and reuse previous results to improve speed or cost.</li> <li>Basic Agent: The ability for your application to use a simple external tool (e.g., a calculator or a simple API).</li> </ul> </li> <li>Documentation: Your <code>EXPERIMENT_LOG.md</code> should include the implementation of this feature as one of your documented \"experiments.\"</li> </ul>"}]}